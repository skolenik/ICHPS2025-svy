@manual{aapor:2011:standard:definitions,
  author =       {{AAPOR}},
  title =        {Standard Definitions: Final Dispositions of Case Codes and Outcome Rates for Surveys},
  year =         {2011},
  organization = {The American Association for Public Opinion Research},
  edition =      {7th}
}

@manual{aapor:2015:standard:definitions,
  author =       {{AAPOR}},
  title =        {Standard Definitions: Final Dispositions of Case Codes and Outcome Rates for Surveys},
  year =         {2015},
  organization = {The American Association for Public Opinion Research},
  edition =      {8th}
}

@manual{aapor:2016:standard:definitions,
  author =       {{AAPOR}},
  title =        {Standard Definitions: Final Dispositions of Case Codes and Outcome Rates for Surveys},
  year =         {2016},
  organization = {The American Association for Public Opinion Research},
  edition =      {9th}
}

@MANUAL{aapor:2014:ti:terms,
  title =        {{AAPOR} Terms and Conditions for Transparency Certification},
  author =       {{AAPOR}},
  organization = {The American Association for Public Opinion Research},
  year =         {2014},
  note =         {Available at http://www.aapor.org/AAPOR{\_}Main/media/MainSiteFiles/TI-Terms-and-Conditions-10-4-17.pdf},
}

@techreport{aapor:2013:nonprob,
  title =        {Report of the {AAPOR} Task Force on Non-Probability Sampling},
  author =       {Reg Baker and Brick, J. Michael and Bates, Nancy A., and Battaglia, M. P. and 
                  Couper, M. P. and Dever, Jill A. and Gile, Krista J. and Roger Tourangeau},
  organization = {{AAPOR}},
  year =         2013,
  note =         {Available from 
      https://aapor.org/wp-content/uploads/2022/11/NPS_TF_Report_Final_7_revised_FNL_6_22_13-2.pdf}
}

@Book{abadir:magnus:2005,
  author    = {Karim M. Abadir and Jan R. Magnus},
  year      = {2005},
  title     = {Matrix Algebra},
  publisher = {Cambridge University Press},
  address   = {New York}
}

@Book{abra:steg:special1964,
  author    = "M. Abramovitz and I. A. Stegun",
  year      = 1964,
  title     = "Handbook of Mathematical Functions",
  publisher = "National Bureau of Standards",
  address   = "Washington, D.C."
}

@TECHREPORT{acs:2009,
  AUTHOR =       {{U.S. Census Bureau}},
  TITLE =        {American Community Survey: Design and Methodology},
  INSTITUTION =  {U.S. Census Bureau},
  YEAR =         {2009},
  address =      {U.S. Government Printing Office, Washington, DC},
}

@TECHREPORT{adelman:2004,
  AUTHOR =       {Adelman, C.},
  TITLE =        {Principal indicators of student academic histories in postsecondary education, 1972-2000},
  INSTITUTION =  {Institute of Education Sciences},
  YEAR =         {2004},
  note =         {Available from http://purl.access.gpo.gov/GPO/LPS91584},
  source =       {Isabella Zaniletti},
}

@article{adam:gull:ukou:eldr:chin:camp:2004,
    author = {Adams, Geoffrey and Gulliford, Martin C. and Ukoumunne, Obioha C.
        and Eldridge, Sandra and Chinn, Susan and Campbell, Michael J.},
    doi = {10.1016/j.jclinepi.2003.12.013},
    journal = {Journal of Clinical Epidemiology},
    number = {8},
    pages = {785--794},
    title = {Patterns of intra-cluster correlation from primary care research to inform study design and analysis},
    volume = {57},
    year = {2004}
}

@book{agresti:2012,
    author = {Agresti, Alan},
    edition = {3},
    isbn = {0470463635},
    publisher = {Wiley},
    title = {Categorical Data Analysis (Wiley Series in Probability and Statistics)},
    year = {2012}
}

@Book{ait:brown:lognormal:1963,
  author    = "J. Aitchison and J.~A.~C.~Brown",
  year      = 1963,
  title     = "The Lognormal Distribution",
  publisher = "Cambridge University Press"
}

@Article{aitch:silvey:1957,
  author    = {Aitchison, J. and Silvey, S. D. },
  year      = {1957},
  title     = {The Generalization of Probit Analysis to the Case of
          Multiple Responses},
  journal   = {Biometrika},
  volume    = {44},
  number    = {1/2},
  pages     = {131--140},
  doi       = {10.2307/2333245}
}

@Article{aitk:1999,
  author    = {Aitkin, Murray},
  year      = 1999,
  title     = {A General Maximum Likelihood Analysis of Variance
          Components in Generalized Linear Models},
  journal   = {Biometrics},
  volume    = 55,
  pages     = {117--128}
}

@TechReport{aiv:kol:eerc:2001,
  author    = "Serguei A. Aivazian and Stanislav Kolenikov",
  year      = 2001,
  title     = "Poverty and expenditure inequality in Russia",
  institution   = "EERC",
  note      = "Working Paper No. E01-01",
  address   = "Moscow, Russia"
}

@Book{aiva:enuk:mesh:1985,
  author    = "?.?. ??????? and ?.C. ?????? and ?.?. ???????? ",
  year      = 1985,
  title     = "?????????? ??????????. ???????????? ????????????",
  publisher = "??????? ? ??????????",
  volume    = 2,
  address   = "??????"
}

@InProceedings{akaike:1973,
  author    = {H. Akaike},
  editor    = {B. N. Petrov and F. Csaki},
  year      = {1973},
  title     = {Information theory and an extension of the maximum
          likelihood principle},
  booktitle = {Second International Symposium on Information Theory},
  pages     = {267--281},
  address   = {Budapest, Hungary},
  organization  = {Akademiai Kiado},
  volume    = {}
}

@Article{albert:follmann:2004,
  author    = {Albert, Paul S. and Follmann, Dean A.},
  year      = {2000},
  title     = {Modeling Repeated Count Data Subject to Informative
          Dropout},
  abstract  = {In certain diseases, outcome is the number of morbid
          events over the course of follow-up. In epilepsy, e.g.,
          daily seizure counts are often used to reflect disease
          severity. Follow-up of patients in clinical trials of such
          diseases is often subject to censoring due to patients
          dying or dropping out. If the sicker patients tend to be
          censored in such trials, estimates of the treatment effect
          that do not incorporate the censoring process may be
          misleading. We extend the shared random effects approach of
          Wu and Carroll (1988, Biometrics44, 1752013188) to the
          setting of repeated counts of events. Three strategies are
          developed. The first is a likelihood-based approach for
          jointly modeling the count and censoring processes. A
          shared random effect is incorporated to introduce
          dependence between the two processes. The second is a
          likelihood-based approach that conditions on the dropout
          times in adjusting for informative dropout. The third is a
          generalized estimating equations (GEE) approach, which also
          conditions on the dropout times but makes fewer assumptions
          about the distribution of the count process. Estimation
          procedures for each of the approaches are discussed, and
          the approaches are applied to data from an epilepsy
          clinical trial. A simulation study is also conducted to
          compare the various approaches. Through analyses and
          simulations, we demonstrate the flexibility of the
          likelihood-based conditional model for analyzing data from
          the epilepsy trial.},
  address   = {Biometric Research Branch, National Cancer Institute, EPN
          Room 739, Bethesda, Maryland 20892-7438, U.S.A.
          albertp@ctep.nci.nih.gov; Office of Biostatistics Research,
          National Heart, Lung, and Blood Institute, Bethesda,
          Maryland 20892-7938, U.S.A.},
  doi       = {10.1111/j.0006-341X.2000.00667.x},
  journal   = {Biometrics},
  number    = {3},
  pages     = {667--677},
  volume    = {56}
}

@Book{allison:2001,
  author    = {Paul D. Allison},
  year      = {2001},
  title     = {Missing Data},
  publisher = {SAGE},
  series    = {Quantitative Applications in the Social Sciences},
  address   = {Thousand Oaks, CA}
}

@Book{amemiya:1985,
  author    = {Takeshi Amemiya},
  year      = {1985},
  title     = {Advanced Econometrics},
  publisher = {Harvard University Press},
  address   = {Cambridge, MA, USA}
}

@Article{ande:gerb:1984,
  author    = {James C. Anderson and David Gerbing},
  year      = {1984},
  title     = {The Effect of Sampling Error on Convergence, Improper
          Solutions, and Goodness-of-Fit Indices for Maximum
          Likelihood Confirmatory Factor Analysis},
  journal   = {Psychometrika},
  volume    = {49},
  pages     = {155--173}
}

@Article{anderson:1963,
  author    = {Anderson, T. W.},
  year      = {1963},
  title     = {Asymptotic Theory for Principal Component Analysis},
  journal   = {Annals of Mathematical Statistics},
  number    = {1},
  pages     = {122--148},
  volume    = {34}
}

@Article{anderson:1984,
  author    = {Anderson, J. A. },
  year      = {1984},
  title     = {Regression and Ordered Categorical Variables},
  doi       = {10.2307/2345457},
  journal   = {Journal of the Royal Statistical Society. Series B
          (Methodological)},
  number    = {1},
  pages     = {1--30},
  volume    = {46}
}

@Book{anderson:2003,
  author    = {T. W. Anderson},
  year      = {2003},
  title     = {An Introduction to Multivariate Statistical Analysis},
  publisher = {John Wiley and Sons},
  series    = {Wiley Series in Probability and Statistics},
  edition   = {3rd}
}

@Article{anderson:amemiya:1988,
  author    = {Anderson, T. W. and Amemiya, Yasuo},
  year      = {1988},
  title     = {The Asymptotic Normal Distribution of Estimators in Factor
          Analysis under General Conditions},
  journal   = {The Annals of Statistics},
  number    = {2},
  pages     = {759--771},
  volume    = {16},
  abstract  = {Asymptotic properties of estimators for the confirmatory
          factor analysis model are discussed. The model is
          identified by restrictions on the elements of the factor
          loading matrix; the number of restrictions may exceed that
          required for identification. It is shown that a particular
          centering of the maximum likelihood estimator derived under
          assumed normality of observations yields an asymptotic
          normal distribution that is common to a wide class of
          distributions of the factor vectors and error vectors. In
          particular, the asymptotic covariance matrix of the factor
          loading estimator derived under the normal assumption is
          shown to be valid for the factor vectors containing a fixed
          part and a random part with any distribution having finite
          second moments and for the error vectors consisting of
          independent components with any distributions having finite
          second moments. Thus the asymptotic standard errors of the
          factor loading estimators computed by standard computer
          packages are valid for virtually any type of nonnormal
          factor analysis. The results are extended to certain
          structural equation models.}
}

@Article{anderson:gerbing:1984,
  author    = {Anderson, James C. and Gerbing, David W. },
  year      = {1984},
  title     = {The effect of sampling error on convergence, improper
          solutions, and goodness-of-fit indices for maximum
          likelihood confirmatory factor analysis},
  doi       = {10.1007/BF02294170},
  journal   = {Psychometrika},
  number    = {2},
  pages     = {155--173},
  volume    = {49}
}

@InProceedings{anderson:rubin:1956,
  author    = {Anderson, T. W. and Rubin, H. },
  year      = {1956},
  title     = {Statistical Inference in Factor Analysis},
  booktitle = {Proceedings of the Third Berkeley Symposium on
          Mathematical Statistics and Probability},
  pages     = {111--150},
  comment   = {http://www.projecteuclid.org/DPubS?verb=Display\&\#38;version=1.0\&\#38;service=UI\&\#38;handle=euclid.bsmsp/1200511860\&\#38;page=record},
  volume    = {5}
}

@Article{andrews:1999estimation,
  author    = {Andrews, Donald W. K.},
  year      = {1999},
  title     = {Estimation When a Parameter is on a Boundary},
  abstract  = {This paper establishes the asymptotic distribution of an
          extremum estimator when the true parameter lies on the
          boundary of the parameter space. The boundary may be
          linear, curved, and/or kinked. Typically the asymptotic
          distribution is a function of a multivariate normal
          distribution in models without stochastic trends and a
          function of a multivariate Brownian motion in models with
          stochastic trends. The results apply to a wide variety of
          estimators and models. Examples treated in the paper are:
          (i) quasi-ML estimation of a random coefficients regression
          model with some coefficient variances equal to zero and
          (ii) LS estimation of an augmented Dickey-Fuller regression
          with unit root and time trend parameters on the boundary of
          the parameter space.},
  doi       = {doi:10.1111/1468-0262.00082},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1341--1383},
  volume    = {67}
}


@Article{andrews:moreira:stock:2006,
  author    = {Andrews, Donald W. and Moreira, Marcelo J. and Stock,
          James H.},
  year      = {2006},
  title     = {Optimal Two-Sided Invariant Similar Tests for Instrumental
          Variables Regression},
  abstract  = {This paper considers tests of the parameter on an
          endogenous variable in an instrumental variables regression
          model. The focus is on determining tests that have some
          optimal power properties. We start by considering a model
          with normally distributed errors and known error covariance
          matrix. We consider tests that are similar and satisfy a
          natural rotational invariance condition. We determine a
          two-sided power envelope for invariant similar tests. This
          allows us to assess and compare the power properties of
          tests such as the conditional likelihood ratio (CLR), the
          Lagrange multiplier, and the Anderson-Rubin tests. We find
          that the CLR test is quite close to being uniformly most
          powerful invariant among a class of two-sided tests. The
          finite-sample results of the paper are extended to the case
          of unknown error covariance matrix and possibly nonnormal
          errors via weak instrument asymptotics. Strong instrument
          asymptotic results also are provided because we seek tests
          that perform well under both weak and strong instruments.},
  doi       = {10.1111/j.1468-0262.2006.00680.x},
  journal   = {Econometrica},
  number    = {3},
  pages     = {715--752},
  volume    = {74}
}

@TechReport{andrews:1997,
  author    = "Donald W. K. Andrews",
  year      = 1997,
  title     = "Estimation When a Parameter is on a Boundary: Theory and
          Applications",
  institution   = "Cowles Foundation for Research in Economics, Yale
          University",
  type      = "Discussion Paper",
  number    = 988
}

@Article{andrews:1989:power,
  author={Andrews, Donald W K},
  title={Power in Econometric Applications},
  journal={Econometrica},
  year=1989,
  volume={57},
  number={5},
  pages={1059--1090},
}

@TechReport{andrews:1999:cowlesdp,
  author    = "Donald W. K. Andrews",
  year      = 1999,
  title     = "Testing When a Parameter Is on the Boundary of the
          Maintained Hypothesis",
  institution   = "Cowles Foundation, Yale University",
  type      = "Discussion Paper",
  number    = {1229}
}

@Article{andrews:1999:ecna,
  author    = "Donald W. K. Andrews",
  year      = 1999,
  title     = "Estimation When a Parameter is on a Boundary",
  journal   = "Econometrica",
  volume    = 67,
  number    = 6,
  pages     = "1341--1383"
}

@Article{andrews:2000:ecna,
  author    = "Donald W. K. Andrews",
  year      = {2000},
  title     = "Inconsistency of the Bootstrap when a Parameter is on the
          Boundary of the Parameter Space",
  journal   = "Econometrica",
  volume    = {68},
  number    = {2},
  pages     = {399-405},
  note      = {doi:10.1111/1468-0262.00114}
}

@Article{andrews:2001,
  author    = "Donald W. K. Andrews",
  year      = {2001},
  title     = "Testing When a Parameter is on the Boundary of the
          Maintained Hypothesis",
  journal   = "Econometrica",
  volume    = {69},
  number    = {3},
  pages     = {683-734},
  note      = {doi:10.1111/1468-0262.00210}
}

@Article{andrews:2007,
  author    = {David F. Andrews},
  year      = {2007},
  title     = {Robust likelihood inference for public policy},
  journal   = {The Canadian Journal of Statistics},
  number    = {3},
  pages     = {341--350},
  comment   = {http://www.ingentaconnect.com/content/ssc/cjs/2007/00000035/00000003/art00001}
          ,
  volume    = {35}
}

@Article{andrews:plob:1994,
  author    = {Andrews, Donald W. K. and Ploberger, Werner },
  year      = {1994},
  title     = {Optimal Tests when a Nuisance Parameter is Present Only
          Under the Alternative},
  doi       = {10.2307/2951753},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1383--1414},
  volume    = {62},
  abstract  = {This paper derives asymptotically optimal tests for
          testing problems in which a nuisance parameter exists under
          the alternative hypothesis but not under the null. For
          example, the results apply to tests of one-time structural
          change with unknown change-point. Several other examples
          are discussed in the paper. The results of the paper are of
          interest, because the testing problem considered is
          nonstandard and the classical asymptotic optimality results
          for the Lagrange multiplier (LM), Wald, and likelihood
          ratio (LR) tests do not apply. A weighted average power
          criterion is used here to generate optimal tests. This
          criterion is similar to that used by Wald (1943) to obtain
          the classical asymptotic optimality properties of Wald
          tests in "regular" testing problems. In fact, the optimal
          tests introduced here reduce to the standard LM, Wald, and
          LR tests when standard regularity conditions hold.
          Nevertheless, in the nonstandard cases of main interest,
          new optimal tests are obtained and the LR test is not found
          to be an optimal test.}
}

@Article{andrews:plob:1995,
  author    = {Andrews, Donald W. K. and Ploberger, Werner },
  year      = {1995},
  title     = {Admissibility of the Likelihood Ratio Test When a Nuisance
          Parameter is Present Only Under the Alternative},
  doi       = {10.2307/2242538},
  journal   = {The Annals of Statistics},
  number    = {5},
  pages     = {1609--1629},
  volume    = {23},
  abstract  = {This paper establishes the asymptotic admissibility of the
          likelihood ratio (LR) test for a general class of testing
          problems in which a nuisance parameter is present only
          under the alternative hypothesis. The paper also
          establishes the finite sample admissibility of the LR test
          for testing problems of this sort that arise in Gaussian
          linear regression models with known variance.}
}

@Book{andrews:stock:2005,
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  abstract  = {This volume contains the papers presented in honor of the
          lifelong achievements of Thomas J. Rothenberg on the
          occasion of his retirement. The authors of the chapters
          include many of the leading econometricians of our day, and
          the chapters address topics of current research
          significance in econometric theory. The chapters cover four
          themes: identification and efficient estimation in
          econometrics, asymptotic approximations to the
          distributions of econometric estimators and tests,
          inference involving potentially nonstationary time series,
          such as processes that might have a unit autoregressive
          root, and nonparametric and semiparametric inference.
          Several of the chapters provide overviews and treatments of
          basic conceptual issues, while others advance our
          understanding of the properties of existing econometric
          procedures and/or propose new ones. Specific topics include
          identification in nonlinear models, inference with weak
          instruments, tests for nonstationary in time series and
          panel data, generalized empirical likelihood estimation,
          and the bootstrap.},
  howpublished  = {Hardcover},
  isbn      = {052184441X},
  publisher = {Cambridge University Press}
}

@article{andridge:little:2010,
    abstract = {
        Hot deck imputation is a method for handling missing data in which
        each missing value is replaced with an observed response from a
        "similar" unit. Despite being used extensively in practice, the
        theory is not as well developed as that of other imputation methods.
        We have found that no consensus exists as to the best way to apply
        the hot deck and obtain inferences from the completed data set. Here
        we review different forms of the hot deck and existing research on
        its statistical properties. We describe applications of the hot deck
        currently in use, including the {U.S}. Census Bureau's hot deck for
        the Current Population Survey ({CPS}). We also provide an extended
        example of variations of the hot deck applied to the third National
        Health and Nutrition Examination Survey ({NHANES} {III}). Some
        potential areas for future research are highlighted.
    },
    author = {Andridge, Rebecca R. and Little, Roderick J.},
    journal = {International statistical review},
    number = {1},
    pages = {40--64},
    pmid = {21743766},
    title = {A Review of Hot Deck Imputation for Survey Non-response},
    volume = {78},
    year = {2010}
}

@Article{andrienko:guriev:2004,
  author    = {Andrienko, Yuri and Guriev, Sergei},
  year      = {2004},
  title     = {Determinants of interregional mobility in Russia},
  abstract  = {Abstract The paper studies the determinants of internal
          migration in Russia. Using panel data on gross
          region-to-region migration flows in 1992-99, we estimate
          the effect of economic, political and social factors.
          Although overall migration is rather low, it turns out that
          its intensity does depend on economic factors even
          controlling for fixed effects for each origin-destination
          pair. People move from poorer and job scarce regions with
          worse public good provision to those which are richer and
          prospering better both in terms of employment prospects and
          public goods. Migration is, however, constrained by the
          lack of liquidity; for the poorest regions, an increase in
          income raises rather than decreases outmigration. Our
          estimates imply that up to a third of Russian regions are
          locked in poverty traps.},
  doi       = {10.1111/j.0967-0750.2004.00170.x},
  journal   = {The Economics of Transition},
  number    = {1},
  pages     = {1--27},
  volume    = {12}
}

@Article{angeles:guilkey:mroz:1998,
  author    = "Gustavo Angeles and David K. Guilkey and Thomas A. Mroz",
  year      = "1998",
  title     = "Purposive Program Placement and the Estimation of Family
          Planning Program Effects in {T}anzania",
  journal   = "Journal of the American Statistical Association",
  volume    = "93",
  number    = "443",
  pages     = "884--899"
}

@TechReport{angeles:you:2007,
  author    = {Angeles, Gustavo and You, Yuan},
  year      = {2007},
  title     = {Availability of Data for Estimating {SES} Indices using
          Household Surveys},
  institution   = {Carolina Population Center},
  type      = {Working Paper},
  number    = {},
  address   = {Chapel Hill, NC, USA}
}

@Article{angr:imbe:rubi:1996,
  author    = {Joshua Angrist and Guido W. Imbens and Donald Rubin},
  year      = {1996},
  title     = {Identification of Causal Effects Using Instrumental
          Variables},
  journal   = {Journal of the American Statistical Association},
  volume    = {91},
  number    = {434},
  pages     = {444--472}
}

@Article{angrist:krueger:1991,
  author    = {Angrist, Joshua D. and Krueger, Alan B. },
  year      = {1991},
  title     = {Does Compulsory School Attendance Affect Schooling and
          Earnings?},
  journal   = {The Quarterly Journal of Economics},
  number    = {4},
  pages     = {979--1014},
  volume    = {106},
  abstract  = {We establish that season of birth is related to
          educational attainment because of school start age policy
          and compulsory school attendance laws. Individuals born in
          the beginning of the year start school at an older age, and
          can therefore drop out after completing less schooling than
          individuals born near the end of the year. Roughly 25
          percent of potential dropouts remain in school because of
          compulsory schooling laws. We estimate the impact of
          compulsory schooling on earnings by using quarter of birth
          as an instrument for education. The instrumental variables
          estimate of the return to education is close to the
          ordinary least squares estimate, suggesting that there is
          little bias in conventional estimates.}
}

@Article{angrist:krueger:1992,
  author    = {Angrist, Joshua D. and Krueger, Alan B. },
  year      = {1992},
  title     = {The Effect of Age at School Entry on Educational
          Attainment: An Application of Instrumental Variables with
          Moments from Two Samples},
  journal   = {Journal of the American Statistical Association},
  number    = {418},
  pages     = {328--336},
  volume    = {87},
  abstract  = {We present a model in which compulsory school attendance
          laws, which typically require school attendance until a
          specified birthday, induce a relationship between years of
          schooling and age at school entry. Variation in school
          starting age created by children's dates of birth provides
          a natural experiment for estimating the effect of age at
          school entry. Because no large data set contains
          information on both age at school entry and educational
          attainment, we use an instrumental variables (IV) estimator
          with data derived from the 1960 and 1980 Censuses to
          estimate and test the age-at-entry/compulsory schooling
          model. In most IV applications, the two covariance matrices
          that form the estimator are constructed from the same
          sample. We use a method-of-moments framework to discuss IV
          estimators that combine moments from different data sets.
          In our application, quarter of birth dummies are the
          instrumental variables used to link the 1960 Census, from
          which age at school entry can be derived for one cohort of
          students, to the 1980 Census, which contains educational
          attainment for the same cohort of students. The results
          suggest that compulsory attendance laws constrain roughly
          10% of students to stay in school.}
}

@Article{antoine:bonnal:renault:2006,
  author    = {Antoine, Bertille and Bonnal, H\'{e}l\`{e}ne and Renault,
          Eric },
  year      = {2007},
  title     = {On the efficient use of the informational content of
          estimating equations: Implied probabilities and Euclidean
          empirical likelihood},
  abstract  = {A number of information-theoretic alternatives to GMM have
          recently been proposed in the literature. For practical use
          and general interpretation, the main drawback of these
          alternatives, particularly in the case of conditional
          moment restrictions, is that they give up the computational
          and interpretational simplicity of quadratic optimization.
          The main contribution of this paper is to analyze the
          informational content of estimating equations within the
          unified framework of Chi-square distance. Improved
          inference by control variables, closed form formulae for
          implied probabilities and information-theoretic
          interpretations of continuously updated GMM are discussed
          in the two cases of unconditional and conditional moment
          restrictions.},
  booktitle = {'Information and Entropy Econometrics' - A Volume in Honor
          of Arnold Zellner},
  doi       = {10.1016/j.jeconom.2006.05.005},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {461--487},
  volume    = {138}
}

@Book{apa:style:2001,
  author    = "APA",
  year      = 2001,
  title     = "Publication Manual of the American Psychological
          Association",
  publisher = "American Psychological Association",
  edition   = "5th"
}

@Manual{arbuckle:1997,
  author    = {Arbuckle, James},
  year      = {1997},
  title     = {AMOS Users? Guide: Version 3.6},
  organization  = {SmallWaters},
  address   = {Chicago}
}

@TechReport{araar:duclos:2006,
  author={Abdelkrim Araar and Jean-Yves Duclos},
  title={DAD: a Software for Poverty and Distributive Analysis},
  year=2006,
  institution={Poverty and Economic Policy---Poverty Monitoring, Measurement and Analysis Program},
  comment={http://ideas.repec.org/p/lvl/pmmacr/2006-10.html},
  number={2006-10},
  abstract=
     {DAD is designed to facilitate the analysis and the comparisons
     of social welfare, inequality, poverty and equity across
     distributions of living standards and using disaggregated data.
     It is freely distributed. DAD's features include the estimation
     of a large number of indices and curves that are useful for
     distributive comparisons as well as the provision of various
     statistical tools to enable statistical inference. Many of the
     features are useful for estimating the impact of programs (and
     reforms to these programs) on poverty and equity.},
}
@Article{arellano:2002,
  author    = {Arellano, Manuel},
  year      = {2002},
  title     = {Sargan's Instrumental Variables Estimation and the
          Generalized Method of Moments},
  doi       = {10.2307/1392415},
  issn      = {07350015},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {4},
  pages     = {450--459},
  volume    = {20},
  abstract  = {This article surveys J. D. Sargan's work on instrumental
          variables (IV) estimation and its connections with the
          generalized method of moments (GMM). First the modeling
          context in which Sargan motivated IV estimation is
          presented. Then the theory of IV estimation as developed by
          Sargan is discussed. His approach to efficiency, his
          minimax estimator, tests of overidentification and
          underidentification, and his later work on the
          finite-sample properties of IV estimators are reviewed.
          Next, his approach to modeling IV equations with serial
          correlation is discussed and compared with the GMM
          approach. Finally, Sargan's results for
          nonlinear-in-parameters IV models are described.}
}

@ARTICLE{archer:lemeshow:2006,
  AUTHOR =       {Archer, K. J. and S. Lemeshow},
  TITLE =        {Goodness-of-fit test for a logistic regression model fitted using survey sample data},
  JOURNAL =      {Stata Journal},
  YEAR =         {2006},
  volume =       {6},
  number =       {1},
  pages =        {97--105},
}




@Article{armi:s?ho:1989,
  author    = {Gerhard Arminger and Ronald J. Schoenberg},
  year      = {1989},
  title     = {Pseudo Maximum Likelihood Estimation and a Test for
          Misspecification in Mean and Covariance Structure Models},
  journal   = {Psychometrika},
  volume    = {54},
  pages     = {409--426}
}

@Article{arminger:muthen:1998,
  author    = {Arminger, Gerhard and Muth{\'e}n, Bengt },
  year      = {1998},
  title     = {A {B}ayesian approach to nonlinear latent variable models
          using the {G}ibbs sampler and the {M}etropolis-{H}astings
          algorithm},
  doi       = {10.1007/BF02294856},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {271--300},
  volume    = {63},
  abstract  = {Nonlinear latent variable models are specified that
          include quadratic forms and interactions of latent
          regressor variables as special cases. To estimate the
          parameters, the models are put in a Bayesian framework with
          conjugate priors for the parameters. The posterior
          distributions of the parameters and the latent variables
          are estimated using Markov chain Monte Carlo methods such
          as the Gibbs sampler and the Metropolis-Hastings algorithm.
          The proposed estimation methods are illustrated by two
          simulation studies and by the estimation of a non-linear
          model for the dependence of performance on task complexity
          and goal specificity using empirical data.}
}




@INPROCEEDINGS{ash:2011,
  AUTHOR =       {Stephen Ash},
  TITLE =        {Using Successive Difference Replication for Estimating Variances},
  BOOKTITLE =    {Proceedings of the Survey Research Methods Section},
  YEAR =         {2011},
  address =      {Alexandria, VA},
  organization = {The American Statistical Association},
  note =         {available at http://www.amstat.org/sections/srms/proceedings/y2011/Files/302108{\_}67867.pdf},
}


@Article{aspar:2005,
  author    = {Tihomir Asparouhov},
  year      = {2005},
  title     = {Sampling Weights in Latent Variable Modeling},
  journal   = {Structural Equation Modeling},
  volume    = {12},
  number    = {3},
  pages     = {411--434}
}

@ARTICLE{atanassov:2004,
  AUTHOR =       {Emmanuel Atanassov},
  TITLE =        {On the discrepancy of the Halton sequences},
  JOURNAL =      {Mathematica Balkanica},
  YEAR =         {2004},
  volume =       {18},
  number =       {1--2},
  pages =        {15--32},
}


@Book{atki:bour:2000,
  editor    = "Anthony B. Atkinson and Francois Bourguignon",
  year      = 2000,
  title     = "Handbook of Income Distribution",
  publisher = "Elsevier",
  number    = 16,
  series    = "Handbooks in Economics"
}

@Book{atki:rian:2000,
  author    = "Atkinson, A. C. and Riani, Marco",
  year      = {2000},
  title     = "Robust Diagnostic Regression Analysis",
  pages     = {327},
  publisher = {Springer-Verlag Inc}
}

@Book{atki:rian:ceri:2003,
  author    = "Atkinson, A. C. and Riani, Marco and Andrea Cerioli",
  year      = {2003},
  title     = "Exploring Multivariate Data with Forward Search",
  publisher = {Springer-Verlag Inc},
  address   = {New York}
}

@Article{atkinson:1970,
  author    = {Atkinson, Anthony B.},
  year      = 1970,
  title     = {On the measurement of inequality},
  journal   = {Journal of Economic Theory},
  volume    = {2},
  number    = {3},
  pages     = {244--263}
}

@Book{atkinson:riani:cerioli:2004,
  author    = {Atkinson, Anthony C. and Riani, Marco and Cerioli,
          Andrea},
  year      = {2004},
  title     = {Exploring Multivariate Data with the Forward Search
          (Springer Series in Statistics)},
  abstract  = {This book is concerned with data in which the observations
          are independent and in which the response is multivariate.
          Anthony Atkinson has been Professor of Statistics at the
          London School of Economics since 1989. Before that he was a
          Professor at Imperial College, London. He is the author of
          Plots, Transformations, and Regression, co-author of
          Optimum Experimental Designs, and joint editor of The
          Fascination of Statistics, a volume celebrating the
          centenary of the International Statistical Institute.
          Professor Atkinson has served as editor of The Journal of
          the Royal Statistical Society, Series B and as associate
          editor of Biometrika and Technometrics. He has published
          well over 100 articles in these and other journals
          including The Annals of Statistics, Biometrics, The Journal
          of the American Statistical Association, and Statistics and
          Computing. Marco Riani, after receiving his Ph.D. in
          Statistics in 1995 from the University of Florence, joined
          the Faculty of Economics at Parma University as
          postdoctoral fellow. In 1997 he won the prize for the best
          Italian Ph.D. thesis in Statistics. He is currently
          Associate Professor of Statistics in the University of
          Parma. He has published in Technometrics, The Journal of
          Computational and Graphical Statistics, The Journal of
          Business and Economic Statistics, The Journal of
          Forecasting, Environmetrics, Computational Statistics and
          Data Analysis, Metron, and other journals.},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {0387408525},
  publisher = {Springer}
}

@Article{aust:cald:1996,
  author    = {J.T. Austin and R.F. Calderon},
  year      = {1996},
  title     = {Theoretical and technical contributions to structural
          equation modeling: An updated annotated bibliography},
  journal   = {Structural Modeling},
  volume    = {3},
  pages     = {105--175}
}

@article{austin:jembere:chiu:2018,
  author = {Peter C Austin and Nathaniel Jembere and Maria Chiu},
  title ={Propensity score matching and complex surveys},
  journal = {Statistical Methods in Medical Research},
  volume = {27},
  number = {4},
  pages = {1240-1257},
  year = {2018},
  doi = {10.1177/0962280216658920},
  note ={PMID: 27460539},
  URL = {https://doi.org/10.1177/0962280216658920},
  abstract = 
      { Researchers are increasingly using complex population-based sample surveys
      to estimate the effects of treatments, exposures and interventions. In such
      analyses, statistical methods are essential to minimize the effect of
      confounding due to measured covariates, as treated subjects frequently differ
      from control subjects. Methods based on the propensity score are increasingly
      popular. Minimal research has been conducted on how to implement propensity
      score matching when using data from complex sample surveys. We used Monte
      Carlo simulations to examine two critical issues when implementing propensity
      score matching with such data. First, we examined how the propensity score
      model should be formulated. We considered three different formulations
      depending on whether or not a weighted regression model was used to estimate
      the propensity score and whether or not the survey weights were included in
      the propensity score model as an additional covariate. Second, we examined
      whether matched control subjects should retain their natural survey weight or
      whether they should inherit the survey weight of the treated subject to which
      they were matched. Our results were inconclusive with respect to which method
      of estimating the propensity score model was preferable. In general, greater
      balance in measured baseline covariates and decreased bias was observed when
      natural retained weights were used compared to when inherited weights were
      used. We also demonstrated that bootstrap-based methods performed well for
      estimating the variance of treatment effects when outcomes are binary. We
      illustrated the application of our methods by using the Canadian Community
      Health Survey to estimate the effect of educational attainment on lifetime
      prevalence of mood or anxiety disorders. }
}



@Article{azza:capi:2003,
  author    = {Azzalini, Adelchi and Capitanio, Antonella},
  year      = {2003},
  title     = {Distributions Generated by Perturbation of Symmetry with
          Emphasis on a Multivariate Skew t-Distribution},
  abstract  = {A fairly general procedure is studied to perturb a
          multivariate density satisfying a weak form of multivariate
          symmetry, and to generate a whole set of non-symmetric
          densities. The approach is sufficiently general to
          encompass some recent proposals in the literature,
          variously related to the skew normal distribution. The
          special case of skew elliptical densities is examined in
          detail, establishing connections with existing similar
          work. The final part of the paper specializes further to a
          form of multivariate skew t-density. Likelihood inference
          for this distribution is examined, and it is illustrated
          with numerical examples.},
  doi       = {10.2307/3647510},
  issn      = {13697412},
  journal   = {Journal of the Royal Statistical Society. Series B
          (Statistical Methodology)},
  number    = {2},
  pages     = {367--389},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  volume    = {65}
}

@Article{azzalini:capitanio:2002,
  author    = {Azzalini, A. and Capitanio, A.},
  year      = {1999},
  title     = {Statistical applications of the multivariate skew normal
          distribution},
  abstract  = {Azzalini and Dalla Valle have recently discussed the
          multivariate skew normal distribution which extends the
          class of normal distributions by the addition of a shape
          parameter. The first part of the present paper examines
          further probabilistic properties of the distribution, with
          special emphasis on aspects of statistical relevance.
          Inferential and other statistical issues are discussed in
          the following part, with applications to some multivariate
          statistics problems, illustrated by numerical examples.
          Finally, a further extension is described which introduces
          a skewing factor of an elliptical density.},
  address   = {University of Padua, Italy, ; University of Bologna,
          Italy},
  doi       = {10.1111/1467-9868.00194},
  journal   = {Journal of the Royal Statistical Society: Series B:
          Statistical Methodology},
  number    = {3},
  pages     = {579--602},
  volume    = {61}
}

@Article{azzalini:capitanio:2003,
  author    = {Azzalini, Adelchi and Capitanio, Antonella},
  year      = {2003},
  title     = {Distributions generated by perturbation of symmetry with
          emphasis on a multivariate skew t-distribution},
  abstract  = {Summary. A fairly general procedure is studied to perturb
          a multivariate density satisfying a weak form of
          multivariate symmetry, and to generate a whole set of
          non-symmetric densities. The approach is sufficiently
          general to encompass some recent proposals in the
          literature, variously related to the skew normal
          distribution. The special case of skew elliptical densities
          is examined in detail, establishing connections with
          existing similar work. The final part of the paper
          specializes further to a form of multivariate skew
          t-density. Likelihood inference for this distribution is
          examined, and it is illustrated with numerical examples.},
  doi       = {10.1111/1467-9868.00391},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {2},
  pages     = {367--389},
  volume    = {65}
}

@Article{azzalini:valle:1996,
  author    = {Azzalini, A. and Valle, Dalla A.},
  year      = {1996},
  title     = {The multivariate skew-normal distribution},
  abstract  = {The paper extends earlier work on the so-called
          skew-normal distribution, a family of distributions
          including the normal, but with an extra parameter to
          regulate skewness. The present work introduces a
          multivariate parametric family such that the marginal
          densities are scalar skew-normal, and studies its
          properties, with special emphasis on the bivariate case.
          10.1093/biomet/83.4.715},
  doi       = {10.1093/biomet/83.4.715},
  journal   = {Biometrika},
  number    = {4},
  pages     = {715--726},
  volume    = {83}
}

@Article{azzoni:2001,
  author    = "Carlos R. Azzoni",
  year      = 2001,
  title     = "Economic Growth and Regional Income Inequality in
          {Brazil}",
  journal   = "The Annals of Regional Science",
  volume    = 35,
  number    = 1,
  pages     = "133--152"
}

@Article{baba:ferg:joer:1987,
  author    = "Emin Babakus and Carl E. Jr. Ferguson and Karl G.
          J{\"o}reskog",
  year      = 1987,
  title     = "The Sensitivity of Confirmatory Maximum Likelihood Factor
          Analysis to Violations of Measurement Scale and
          Distributional Assumptions",
  journal   = "Journal of Marketing Research",
  volume    = 24,
  pages     = "222--228"
}

@Article{baddeley:turner:moller:hazelton:2005,
  author    = {Baddeley, A. and Turner, R. and Moller, J. and Hazelton,
          M.},
  year      = {2005},
  title     = {Residual analysis for spatial point processes (with
          discussion)},
  abstract  = {We define residuals for point process models fitted to
          spatial point pattern data, and we propose diagnostic plots
          based on them. The residuals apply to any point process
          model that has a conditional intensity; the model may
          exhibit spatial heterogeneity, interpoint interaction and
          dependence on spatial covariates. Some existing ad hoc
          methods for model checking (quadrat counts, scan statistic,
          kernel smoothed intensity and Berman's diagnostic) are
          recovered as special cases. Diagnostic tools are developed
          systematically, by using an analogy between our spatial
          residuals and the usual residuals for (non-spatial)
          generalized linear models. The conditional intensity ?
          plays the role of the mean response. This makes it possible
          to adapt existing knowledge about model validation for
          generalized linear models to the spatial point process
          context, giving recommendations for diagnostic plots. A
          plot of smoothed residuals against spatial location, or
          against a spatial covariate, is effective in diagnosing
          spatial trend or co-variate effects. Q–Q-plots of the
          residuals are effective in diagnosing interpoint
          interaction.},
  doi       = {10.1111/j.1467-9868.2005.00519.x},
  issn      = {1369-7412},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {5},
  pages     = {617--666},
  publisher = {Blackwell Publishing},
  volume    = {67}
}

@Article{baggerly:1998,
  author    = {Baggerly, Keith A.},
  year      = {1998},
  title     = {Empirical likelihood as a goodness-of-fit measure},
  abstract  = {The method of empirical likelihood can be viewed as one of
          allocating probabilities to an n-cell contingency table so
          as to minimise a goodness-of-fit criterion. It is shown
          that, when the Cressie-Read power-divergence statistic is
          used as the criterion, confidence regions enjoying the same
          convergence rates as those found for empirical likelihood
          can be obtained for the entire range of values of the
          Cressie-Read parameter {lambda}, including -1, maximum
          entropy, 0, empirical likelihood, and 1, Pearson's {chi}2.
          It is noted that, in the power-divergence family, empirical
          likelihood is the only member which is Bartlett-correctable.
          However, simulation results suggest that, for the mean,
          using a, scaled F distribution yields more accurate
          coverage levels for moderate sample sizes.
          10.1093/biomet/85.3.535},
  comment   = {Empirical likelihood is applied to the contingency tables
          analysis; no other chi-square goodness-of-fit measures are
          discussed, although the generalizations might be available.
          Relation to Cressie-Read is made, but most econometric
          papers have better discussions of it. No econometrics
          papers are mentioned whatsoever.

          Interesting qualitative shifts are observed for specific
          values of Cressie-Read parameter. First, at \$\labmda=0\$
          (empirical likelihood), the behavior changes with respect
          to the true value being in the convex hull or not: for
          \$\lambda \ge 0\$, the CI is always within the convex hull.
          Second, at \$\lambda=-1\$ (maximum entropy, exponential
          tilting), there is some argument about optimal allocation
          that I did not get. Third, at \$\lambda=-2\$
          (Neyman-modified \$\chi^2\$, Euclidean likelihood, or
          Hotelling \$T^2\$ for the normal mean), the shape of the
          confidence region changes: for \$\lambda>-2\$, they
          protrude towards outliers, and for \$\lambda<-2\$, tilted
          away from them, which is hardly reasonable.

          The analytical expression for the second order
          \$O(n^{-1})\$ term in the ELRT statistic is derived for the
          case of \$N(0,I)\$ mean.

          The simulations compared performance of the \$\chi^2\$,
          Bartlett-corrected \$\chi^2\$, and scaled \$F\$
          distributions for a range of \$\lambda\$ values, sample
          sizes, and distributions. It appears that Bartlett
          correction is performing well in the center of the
          distribution (i.e., for the confidence regions of about
          50\% coverage), but \$F\$ distributions appear better at
          the tails. Normal and \$t\_5\$ distributions worked best
          for \$\lambda\$ between -2 and 0 (Euclidean to empirical
          likelihood), while \$\chi^2\$ distribution worked better
          with \$\labmda\$ around 2 (Pearson's \$\chi^2\$).

          Finally, of all Cressie-Read test statistics, only the
          empirical likelihood was found to be Bartlett-correctable.

          Overall, a paper with many important technical results, but not terribly well written.},
  doi       = {10.1093/biomet/85.3.535},
  journal   = {Biometrika},
  number    = {3},
  pages     = {535--547},
  volume    = {85}
}

@Article{bai:2003,
  author    = "Jushan Bai",
  year      = 1993,
  title     = "Inferential Theory for Factor Models of Large Dimensions",
  journal   = "Econometrica",
  volume    = 71,
  pages     = "135--171"
}

@Article{bai:jiang:yao:zheng:2009,
  author    = {Bai, Zhidong and Jiang, Dandan and Yao, Jian-feng and
          Zheng, Shurong},
  year      = {2009},
  title     = {Corrections to LRT on Large Dimensional Covariance Matrix
          by RMT},
  abstract  = {In this paper, we give an explanation to the failure of
          two likelihood ratio procedures for testing about
          covariance matrices from Gaussian populations when the
          dimension is large compared to the sample size. Next, using
          recent central limit theorems for linear spectral
          statistics of sample covariance matrices and of random
          F-matrices, we propose necessary corrections for these LR
          tests to cope with high-dimensional effects. The asymptotic
          distributions of these corrected tests under the null are
          given. Simulations demonstrate that the corrected LR tests
          yield a realized size close to nominal level for both
          moderate p (around 20) and high dimension, while the
          traditional LR tests with chi-square approximation fails.
          Another contribution from the paper is that for testing the
          equality between two covariance matrices, the proposed
          correction applies equally for non-Gaussian populations
          yielding a valid pseudo-likelihood ratio test.},
  archiveprefix = {arXiv},
  eprint    = {0902.0552}
}

@Book{baker:kim:2004,
  author    = {Baker, Frank B. and Kim, Seock-Ho },
  year      = {2004},
  title     = {Item Response Theory: Parameter Estimation Techniques},
  publisher = {Chapman \& Hall/CRC},
  series    = {Statistics, a Series of Textbooks and Monographs},
  edition   = {2nd}
}

@Article{baltagi2000unequally,
  author    = {Baltagi, Badi H. and Wu, Ping X.},
  year      = {2000},
  title     = {UNEQUALLY SPACED PANEL DATA REGRESSIONS WITH AR(1)
          DISTURBANCES},
  doi       = {10.1017/S0266466699156020},
  journal   = {Econometric Theory},
  number    = {06},
  pages     = {814--823},
  volume    = {15}
}

@Article{bancroft:han:1977,
  author    = {T. A. Bancroft and C.-P. Han},
  year      = {1977},
  title     = {Inference Based on Conditional Specification: A Note and a
          Bibliography},
  journal   = {International Statistical Review},
  volume    = {45},
  pages     = {117--127}
}

@Article{barrett:donald:2009,
  author    = {Barrett, Garry F. and Donald, Stephen G.},
  year      = {2009},
  title     = {Statistical Inference with Generalized {G}ini Indices of
          Inequality, Poverty, and Welfare},
  abstract  = {This article considers statistical inference for
          consistent estimators of generalized Gini indices of
          inequality, poverty, and welfare. Our method does not
          require grouping the population into a fixed number of
          quantiles. The empirical indices are shown to be
          asymptotically normally distributed using functional limit
          theory. Easily computed asymptotic variance expressions are
          obtained using influence functions. Inference based on
          first-order asymptotics is then compared with the grouped
          method and various bootstrap methods in simulations and
          with U.S. income data. The bootstrap-t method based on our
          asymptotic theory is found to have superior size and power
          properties in small samples.},
  journal   = {Journal of Business and Economic Statistics},
  number    = {1},
  pages     = {1--17},
  volume    = {27}
}

@Article{bartho:1961,
  author    = "Bartholomew, D. J.",
  year      = "1961",
  title     = "Ordered Tests in the Analysis of Variance",
  journal   = "Biometrika",
  volume    = "48",
  pages     = "325--332"
}

@Book{bartho:knott:1999,
  author    = {David J. Bartholomew and Martin Knott},
  year      = {1999},
  title     = {Latent Variable Models and Factor Analysis},
  publisher = {Arnold Publishers},
  volume    = {7},
  series    = {Kendall's Library of Statistics},
  edition   = {2nd},
  address   = {London}
}

@Book{bartho:steele:mpus:galb:2002,
  author    = {David J. Bartholomew and Fiona Steele and Irini Moustaki
          and Jane I. Galbraith},
  year      = {2002},
  title     = {The Analysis and Interpretation of Multivariate Data for
          Social Scientists},
  publisher = {Chapman and Hall/CRC},
  series    = {Texts in Statistical Science},
  address   = {Boca Raton, FL}
}

@Article{bartho:1961,
  author    = "Bartholomew, D. J.",
  year      = "1961",
  title     = "Ordered Tests in the Analysis of Variance",
  journal   = "Biometrika",
  volume    = "48",
  pages     = "325--332"
}


@article{bartlett:1937,
    author = {Bartlett, M. S.},
    doi = {10.2307/96803},
    journal = {Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences},
    number = {901},
    pages = {268--282},
    publisher = {The Royal Society},
    title = {Properties of Sufficiency and Statistical Tests},
    volume = {160},
    year = {1937}
}

@TechReport{bartolo:2000,
  author    = "Annamaria Di Bartolo",
  year      = "2000",
  title     = "Human Capital Estimation through Structural Equation
          Models with some Categorical Observed Variables",
  institution   = "IRISS at CEPS/INSTEAD",
  type      = "Working Paper",
  note      = "RePEc handle: RePEc:irs:iriswp:2000-02"
}

@Article{basm:1957,
  author    = {R.L. Basmann},
  year      = {1957},
  title     = {A Generalized Classical Method of Linear Estimation of
          Coefficients in a Structural Equation},
  journal   = {Econometrica},
  volume    = {25},
  pages     = {77--83}
}

@Article{basmann:1960,
  author    = {Basmann, R. L. },
  year      = {1960},
  title     = {On Finite Sample Distributions of Generalized Classical
          Linear Identifiability Test Statistics},
  doi       = {10.2307/2281588},
  journal   = {Journal of the American Statistical Association},
  number    = {292},
  volume    = {55},
  abstract  = {In the estimation of econometric simultaneous equations
          models, hypothesized necessary conditions for the
          identifiability of a single equation usually specify the
          exclusion of a number of variables from the structural
          equation in question. If the pre-determined variables are
          completely exogenous, if the disturbances in the equations
          are jointly normally distributed, and if a moderately high
          degree of precision can be obtained in reduced-form
          estimation, then the exact finite sample distribution of
          the generalized classical linear identifiability test
          statistic can be closely approximated by Snedecor's F with
          appropriate degrees of freedom.}
}

@Article{basmann:1961,
  author    = {Basmann, R. L. },
  year      = {1961},
  title     = {A Note on the Exact Finite Sample Frequency Functions of
          Generalized Classical Linear Estimators in Two Leading
          Over-Identified Cases},
  doi       = {10.2307/2282084},
  journal   = {Journal of the American Statistical Association},
  number    = {295},
  pages     = {619--636},
  volume    = {56}
}

@Article{bassett1978asymptotic,
  author    = {Bassett, Gilbert and Koenker, Roger},
  year      = {1978},
  title     = {Asymptotic Theory of Least Absolute Error Regression},
  journal   = {Journal of the American Statistical Association},
  number    = {363},
  pages     = {618--622},
  volume    = {73}
}

@Article{bassmann:1961,
  author    = {Basmann, R. L.},
  year      = {1961},
  title     = {A Note on the Exact Finite Sample Frequency Functions of
          Generalized Classical Linear Estimators in Two Leading
          Over-Identified Cases},
  doi       = {10.2307/2282084},
  journal   = {Journal of the American Statistical Association},
  number    = {295},
  pages     = {619--636},
  publisher = {American Statistical Association},
  volume    = {56}
}

@misc{batt:izra:hoag:fran:2009,
  author =       {M. P. Battaglia and D. Izrael and D. C. Hoaglin  and M. R. Frankel},
  title =        {Practical considerations in raking survey data},
  howpublished = {{\it Survey Practice}, http://surveypractice.wordpress.com/2009/06/29/raking-survey-data/},
  year =         {2009},
}




@INPROCEEDINGS{batt:eise:imme:kont:2010,
  AUTHOR =       {Michael P. Battaglia and Donna Eisenhower and Stephen Immerwahr and Kevin Konty},
  TITLE =        {Dual-Frame Weighting Of {RDD} And Cell Phone Interviews At The Local Level},
  BOOKTITLE =    {Proceedings of Survey Research Methods Section},
  YEAR =         {2010},
  address =      {Alexandria, VA},
  publisher =    {The American Statistical Association},
}


@Article{bauer:2003,
  author    = {Bauer, Daniel J.},
  year      = {2003},
  title     = {Estimating Multilevel Linear Models as Structural Equation
          Models},
  abstract  = {Multilevel linear models (MLMs) provide a powerful
          framework for analyzing data collected at nested or
          non-nested levels, such as students within classrooms. The
          current article draws on recent analytical and software
          advances to demonstrate that a broad class of MLMs may be
          estimated as structural equation models (SEMs). Moreover,
          within the SEM approach it is possible to include
          measurement models for predictors or outcomes, and to
          estimate the mediational pathways among predictors
          explicitly, tasks which are currently difficult with the
          conventional approach to multilevel modeling. The
          equivalency of the SEM approach with conventional methods
          for estimating MLMs is illustrated using empirical
          examples, including an example involving both multiple
          indicator latent factors for the outcomes and a causal
          chain for the predictors. The limitations of this approach
          for estimating MLMs are discussed and alternative
          approaches are considered. 10.3102/10769986028002135},
  doi       = {10.3102/10769986028002135},
  journal   = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS},
  number    = {2},
  pages     = {135--167},
  volume    = {28}
}

@Article{bauer:2005,
  author    = {Daniel J Bauer},
  year      = {2005},
  title     = {A semiparametric approach to modeling nonlinear relations
          among latent variables},
  journal   = {Structural Equation Modeling},
  volume    = {4},
  pages     = {513--535}
}

@Article{bauer:2007:gmm,
  author    = {Bauer, Daniel J.},
  year      = {2007},
  title     = {Observations on the Use of Growth Mixture Models in
          Psychological Research},
  abstract  = {Psychologists are applying growth mixture models at an
          increasing rate. This article argues that most of these
          applications are unlikely to reproduce the underlying
          taxonic structure of the population. At a more fundamental
          level, in many cases there is probably no taxonic structure
          to be found. Latent growth classes then categorically
          approximate the true continuum of individual differences in
          change. This approximation, although in some cases
          potentially useful, can also be problematic. The utility of
          growth mixture models for psychological science thus
          remains in doubt. Some ways in which these models might be
          more profitably used are suggested.},
  doi       = {10.1080/00273170701710338},
  journal   = {Multivariate Behavioral Research},
  number    = {4},
  pages     = {757--786},
  publisher = {Psychology Press},
  volume    = {42}
}

@Article{bauer:curran:2004,
  author    = {Daniel J. Bauer and Patrick J. Curran},
  year      = {2004},
  title     = {The Integration of Continuous and Discrete Latent Variable
          Models: Potential Problems and Promising Opportunities},
  journal   = {Psychological Methods},
  volume    = {9},
  number    = {1},
  pages     = {3--29},
  doi       = {10.1037/1082-989X.9.1.3}
}

@Article{baum:2004:stata,
  author    = {Baum, Christopher F.},
  year      = 2004,
  title     = {A review of Stata 8.1 and its time series capabilities},
  journal   = {International Journal of Forecasting},
  volume    = {20},
  number    = {1},
  pages     = {151-161}
}

@Article{baum:schaffer:stillman:2003,
  author    = {Christopher F. Baum and Mark E. Schaffer and Steven
          Stillman},
  year      = {2003},
  title     = {Instrumental variables and {GMM}: Estimation and testing},
  journal   = {The Stata Journal},
  volume    = {3},
  number    = {1},
  pages     = {1--31},
  abstract  = {We discuss instrumental variables (IV) estimation in the
          broader context of the generalized method of moments (GMM),
          and describe an extended IV estimation routine that
          provides GMM estimates as well as additional diagnostic
          tests. Stand-alone test procedures for heteroskedasticity,
          overidentification, and endogeneity in the IV context are
          also described.}
}

@book{baum:2009,
    author = {Baum, Christopher F.},
    isbn = {9781597180450},
    publisher = {Stata Press},
    title = {An Introduction to Stata Programming},
    year = {2009}
}

@Article{baumg:hombu:1996,
  author    = {Baumgartner, H. and Homburg, C.},
  year      = {1996},
  title     = {Applications of structural equation modeling in marketing
          and consumer research: A review},
  doi       = {10.1016/0167-8116(95)00038-0},
  issn      = {01678116},
  number    = {2},
  pages     = {139--161},
  journal   = {International Journal of Research in Marketing},
  volume    = {13}
}

@ARTICLE{bell:mccaffrey:2002,
  author =       {Robert M. Bell and Daniel F. Mc{C}affrey},
  title =        {Bias Reduction in Standard Errors for Linear Regression with Multi-Stage Samples},
  journal =      {Survey Methodology},
  year =         {2002},
  volume =       {28},
  number =       {2},
  pages =        {169--179},
}


@Article{bellhouse2002analysis,
  author    = {Bellhouse, D. R. and Rao, J. N. K.},
  year      = {2002},
  title     = {Analysis of domain means in complex surveys},
  abstract  = {Under simple random sampling, standard methods of analysis
          of data apply, but data are often collected from complex
          surveys involving clustering, stratification and unequal
          probabilities of selection. As a result, application of
          standard methods to these data without some adjustment for
          survey design can lead to erroneous inferences. Here the
          analysis of domain means under complex sampling designs is
          examined. Rao-Scott-type corrections that take account of
          the design effects are obtained. The case of one- and
          two-way analyses of variance of domain means is first
          studied and then extended to include generalized linear
          models. The special case of a Poisson model is studied in
          detail. Finally, data analysis is carried out on domain
          estimates from two complex surveys to illustrate the
          proposed methods.},
  doi       = {10.1016/S0378-3758(01)00167-7},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {1},
  pages     = {47--58},
  volume    = {102}
}

@Article{bellhouse:rao:2002,
  author    = {Bellhouse, D. R. and Rao, J. N. K.},
  year      = {2002},
  title     = {Analysis of domain means in complex surveys},
  journal   = {Journal of Statistical Planning and Inference},
  volume    = {102},
  pages     = {47--58}
}

@Book{bels:kuh:welsch:1980,
  author    = "David A. Belsley and Edwin Kuh and Roy E. Welsch",
  year      = 1980,
  title     = "Regression Diagnostics : Identifying Influential Data and
          Sources of Collinearity",
  publisher = "Wiley-Interscience",
  address   = "New York"
}

@incollection{bethlehem:2002,
    address = {New York},
    author = {Bethlehem, Jelke},
    booktitle = {Survey Nonresponse},
    editor = {Groves, Robert M. and Dillman, Don A. and Eltinge, John L. and Little, Roderick J. A.},
    pages = {375--288},
    publisher = {Wiley},
    title = {Weighting Nonresponse Adjustments Based on Auxiliary Information},
    year = {2002}
}

@Article{benjamini:hochberg:1995,
  author    = {Benjamini, Yoav and Hochberg, Yosef},
  year      = {1995},
  title     = {Controlling the False Discovery Rate: A Practical and
          Powerful Approach to Multiple Testing},
  abstract  = {The common approach to the multiplicity problem calls for
          controlling the familywise error rate (FWER). This
          approach, though, has faults, and we point out a few. A
          different approach to problems of multiple significance
          testing is presented. It calls for controlling the expected
          proportion of falsely rejected hypotheses-the false
          discovery rate. This error rate is equivalent to the FWER
          when all hypotheses are true but is smaller otherwise.
          Therefore, in problems where the control of the false
          discovery rate rather than that of the FWER is desired,
          there is potential for a gain in power. A simple sequential
          Bonferroni-type procedure is proved to control the false
          discovery rate for independent test statistics, and a
          simulation study shows that the gain in power is
          substantial. The use of the new procedure and the
          appropriateness of the criterion are illustrated with
          examples.},
  doi       = {10.2307/2346101},
  issn      = {00359246},
  journal   = {Journal of the Royal Statistical Society. Series B
          (Methodological)},
  number    = {1},
  pages     = {289--300},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  volume    = {57}
}

@Article{bent:stei:1992,
  author    = {P.M. Bentler and J.A. Stein},
  year      = {1992},
  title     = {Structural equation models in medical research},
  journal   = {Statistical Methods in Medical Research},
  volume    = {1},
  pages     = {159--181}
}

@Article{bent:week:1980,
  author    = {Bentler, P. M. and Weeks, D. G. },
  year      = {1980},
  title     = {Linear structural equations with latent variables},
  journal   = {Psychometrika},
  pages     = {289--308},
  volume    = {45}
}

@Article{bentler1990comparative,
  author    = {Bentler, P. M.},
  year      = {1990},
  title     = {Comparative fit indexes in structural models.},
  abstract  = {Normed and nonnormed fit indexes are frequently used as
          adjuncts to chi-square statistics for evaluating the fit of
          a structural model. A drawback of existing indexes is that
          they estimate no known population parameters. A new
          coefficient is proposed to summarize the relative reduction
          in the noncentrality parameters of two nested models. Two
          estimators of the coefficient yield new normed (CFI) and
          nonnormed (FI) fit indexes. CFI avoids the underestimation
          of fit often noted in small samples for Bentler and
          Bonett's (1980) normed fit index (NFI). FI is a linear
          function of Bentler and Bonett's non-normed fit index
          (NNFI) that avoids the extreme underestimation and
          overestimation often found in NNFI. Asymptotically, CFI,
          FI, NFI, and a new index developed by Bollen are equivalent
          measures of comparative fit, whereas NNFI measures relative
          fit by comparing noncentrality per degree of freedom. All
          of the indexes are generalized to permit use of Wald and
          Lagrange multiplier statistics. An example illustrates the
          behavior of these indexes under conditions of correct
          specification and misspecification. The new fit indexes
          perform very well at all sample sizes.},
  address   = {Department of Psychology, University of California, Los
          Angeles 90024-1563.},
  issn      = {0033-2909},
  journal   = {Psychol Bull},
  number    = {2},
  pages     = {238--246},
  volume    = {107}
}

@Article{bentler:1983,
  author    = {Bentler, P. M. },
  year      = {1983},
  title     = {Simultaneous equation systems as moment structure models},
  journal   = {Journal of Econometrics},
  number    = {1-2},
  pages     = {13--42},
  comment   = {http://dx.doi.org/10.1016/0304-4076(83)90092-1},
  volume    = {22},
  abstract  = {An introduction to latent variable covariance structure
          models is provided. Estimation and testing of such models
          with normal and non-normal variables is reviewed, and a
          relatively inexpensive asymptotically distribution-free
          estimator is introduced. The traditional econometric
          simultaneous equation system is reconceptualized as a
          random vector structural equation model. Extensions of the
          model to deal with latent variables and a wider variety of
          structural phenomena are made via the
          J\"{o}reskog-Keesling-Wiley LISREL approach and a simpler
          approach due to Bentler and Weeks. These representation
          systems are extended via first and second moments about the
          origin of measured variables, explained in terms of the
          structural coefficients and the moments of explanatory
          variables. Citations to further literature are provided.}
}

@Article{bentler:1990:cfi,
  author    = {Bentler, P. M.},
  year      = {1990},
  title     = {Fit Indexes, Lagrange Multipliers, Constraint Changes and
          Incomplete Data in Structural Models},
  abstract  = {Certain aspects of model modification and evaluation are
          discussed, with an emphasis on some points of view that
          expand upon or may differ from Kaplan (1990). The
          usefulness of BentlerBonett indexes is reiterated. When
          degree of misspecification can be measured by the size of
          the noncentrality parameter of a x[SUP2] distribution, the
          comparative fit index provides a useful general index of
          model adequacy that does not require knowledge of sourees
          of misspecification. The dependence of the Lagrange
          Multiplier X[SUP2] statistic on both the estimated
          multiplier parameter and estimated constraint or parameter
          change is discussed. A sensitivity theorem that shows the
          effects of unit change in constraints on model fit is
          developed for model modification in structural models.
          Recent incomplete data methods, such as those developed by
          Kaplan and his collaborators, are extended to be applicable
          in a wider range of situations.},
  doi       = {10.1207/s15327906mbr2502\_3},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {163--172},
  publisher = {Psychology Press},
  volume    = {25}
}

@Article{bentler:1990:mbr,
  author    = {Bentler, P. M. },
  year      = {1990},
  title     = {Fit Indexes, Lagrange Multipliers, Constraint Changes and
          Incomplete Data in Structural Models},
  doi       = {http://dx.doi.org/10.1207/s15327906mbr2502\_3},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {163--172},
  volume    = {25},
  abstract  = {Certain aspects of model modification and evaluation are
          discussed, with an emphasis on some points of view that
          expand upon or may differ from Kaplan (1990). The
          usefulness of Bentler-Bonett indexes is reiterated. When
          degree of misspecification can be measured by the size of
          the noncentrality parameter of a $\chi^2$ distribution, the
          comparative fit index provides a useful general index of
          model adequacy that does not require knowledge of sourees
          of misspecification. The dependence of the Lagrange
          Multiplier $\chi^2$ statistic on both the estimated
          multiplier parameter and estimated constraint or parameter
          change is discussed. A sensitivity theorem that shows the
          effects of unit change in constraints on model fit is
          developed for model modification in structural models.
          Recent incomplete data methods, such as those developed by
          Kaplan and his collaborators, are extended to be applicable
          in a wider range of situations.}
}

@Article{bentler:1990:pb,
  author    = {Bentler, P. M. },
  year      = {1990},
  title     = {Comparative fit indexes in structural models.},
  journal   = {Psychological Bulletin},
  number    = {2},
  pages     = {238--246},
  comment   = {http://view.ncbi.nlm.nih.gov/pubmed/2320703},
  volume    = {107}
}


@TECHREPORT{bentler:2004,
  AUTHOR =       {Peter M. Bentler},
  TITLE =        {Maximal Reliability for Unit-weighted Composites},
  INSTITUTION =  {University of California, Los Angeles},
  YEAR =         {2004},
  type =         {UCLA Statistics Preprint },
  number =       {405},
}


@Article{bentler:2009:psmka:alpha,
  author    = {Bentler, Peter M.},
  year      = {2009},
  title     = {Alpha, Dimension-Free, and Model-Based Internal
          Consistency Reliability},
  abstract  = {As pointed out by Sijtsma (2009), coefficient alpha is
          inappropriate as a single summary of the internal
          consistency of a composite score. Better estimators of
          internal consistency are available. In addition to those
          mentioned by Sijtsma, an old dimension-free coefficient and
          structural equation model based coefficients are proposed
          to improve the routine reporting of psychometric internal
          consistency. The various ways to measure internal
          consistency are also shown to be appropriate to binary and
          polytomous items.},
  comment   = {Bentler (2009) discusses several alternative measures of
          internal consistency of a composite score: "greatest lower
          bound", Benter's lower bound, Kronbach's alpha, 1-factor
          reliability. He shows relations between them under various
          assumptions: (i) whether the factor model does or does not
          hold; (ii) whether or not Heywood cases are allowed; (iii)
          whether or not the error are allowed to covary; (iv)
          whether the degrees of freedom of the model are positive
          (confirmatory analysis) or negative (exploratory analysis).

          He also briefly discusses the issue of sampling variation
          as a source of bias for some of the measures, although
          that's about two paragraphs.

          Bentler kind of expects the reader to see right through the
          argument to the key point. I am personally not able to do
          it so quickly, even though I do know Kronbach's alpha
          sucks. The paper does not provide a clear cut set of
          scenarios under which each of the discussed internal
          consistency measures is meaningful -- for each of the
          measures, the ideas are thrown out as to when it
          outperforms alpha, and sometimes how it compares with
          another randomly chosen measure. If you want to see a big
          picture, all that information needs to be hand-picked and
          presented as a table or something, so expect going over
          this paper with an extra sheet of paper and a pencil.},
  doi       = {10.1007/s11336-008-9100-1},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {137--143},
  volume    = {74}
}

@InBook{bentler:dijkstra:1985,
  author    = {Bentler, P.M., and Dijkstra, T.},
  editor    = {P. R. Kirshnaiah},
  year      = {1985},
  title     = {Efficient estimation via linearization in structural
          models},
  booktitle = {Multivariate analysis VI},
  chapter   = {1},
  pages     = {8--42},
  publisher = {North Holland},
  address   = {Amsterdam}
}

@Misc{bentler:eqs,
  author    = {P. M. Bentler and E. J. C. Wu},
  year      = {1995},
  title     = {{EQS} Program Manual},
  publisher = {Multivariate Software Inc.}
}

@Article{bentler:stein:1992,
  author    = {Bentler, P. M. and Stein, J. A.},
  year      = {1992},
  title     = {Structural equation models in medical research},
  abstract  = {Structural equation modelling (SEM) is a modern
          statistical method that allows one to evaluate causal
          hypotheses on a set of intercorrelated nonexperimental
          data. The sample variances and covariances, and possibly
          the means, are compared to those predicted by a
          theory-based hypothetical model after optimal estimation of
          the parameters of the model. The goodness-of-fit of the
          empirical data to the hypothesized model is evaluated
          statistically. This review describes the underlying
          statistical theory and rationale of SEM. Both confirmatory
          factor analysis and latent variable path models are
          discussed. The applicability of SEM to assessment of
          reliability and validity is noted. A detailed example is
          provided, and several examples from the medical literature
          are briefly reviewed. Cautions regarding the possible
          misuse or misinterpretation of the technique are also
          mentioned. Possible future directions for the use of SEM in
          medical research are suggested. Two appendices provide more
          technical details. 10.1177/096228029200100203},
  doi       = {10.1177/096228029200100203},
  journal   = {Stat Methods Med Res},
  number    = {2},
  pages     = {159--181},
  volume    = {1}
}

@Article{bentler:yuan:1999,
  author    = {Bentler, Peter M. and Yuan, Ke-Hai},
  year      = {1999},
  title     = {Structural Equation Modeling with Small Samples: Test
          Statistics},
  doi       = {10.1207/S15327906Mb340203},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {181--197},
  volume    = {34},
  abstract  = {Structural equation modeling is a well-known technique for
          studying relationships among multivariate data. In
          practice, high dimensional nonnormal data with small to
          medium sample sizes are very common, and large sample
          theory, on which almost all modeling statistics are based,
          cannot be invoked for model evaluation with test
          statistics. The most natural method for nonnormal data, the
          asymptotically distribution free procedure, is not defined
          when the sample size is less than the number of
          nonduplicated elements in the sample covariance. Since
          normal theory maximum likelihood estimation remains defined
          for intermediate to small sample size, it may be invoked
          but with the probable consequence of distorted performance
          in model evaluation. This article studies the small sample
          behavior of several test statistics that are based on
          maximum likelihood estimator, but are designed to perform
          better with nonnormal data. We aim to identify statistics
          that work reasonably well for a range of small sample sizes
          and distribution conditions. Monte Carlo results indicate
          that Yuan and Bentler's recently proposed F-statistic
          performs satisfactorily.}
}


@INCOLLECTION{bentler:savalei:2010,
  AUTHOR =       {Peter M. Bentler and Victoria Savalei},
  TITLE =        {Analysis of Correlation Structures: Current Status and Open Problems},
  BOOKTITLE =    {Statistics in the Social Sciences: Current Methodological Developments},
  PUBLISHER =    {Wiley},
  YEAR =         {2010},
  editor =       {Stanislav Kolenikov and Douglas Steinley and Lori Thombs},
}


@Article{bera:2002,
  author    = {Bera, Anil K. and Suprayitno, Totok and Premaratne,
          Gamini},
  year      = {2002},
  title     = {On some heteroskedasticity-robust estimators of
          variance-covariance matrix of the least-squares
          estimators},
  abstract  = {Chesher and Jewitt (Econometrica 55 (1987) 1217)
          demonstrated that the Eicker (Ann. Math. Statist. 34 (1963)
          447) and White (Econometrica 48 (1980) 817) consistent
          estimator of the variance-covariance matrix in
          heteroskedastic models could be severely biased if the
          design matrix is highly unbalanced. In this paper we,
          therefore, reconsider Rao's (J. Amer. Statist. Assoc. 65
          (1970) 161) minimum norm quadratic unbiased estimator
          (MINQUE). We derive the analytical expressions for the mean
          squared errors (MSE) of the Eicker-White, one of MacKinnon
          and White's (J. Econometrics 29 (1985) 305) and MINQUE
          estimators, and perform a numerical comparison. Our
          analysis shows that although MINQUE is unbiased by
          construction, it has very large variance particularly for
          the highly unbalanced design matrices. Since the variance
          is the dominant factor in our MSE computation, MINQUE is
          not the preferred estimator in terms of MSE comparison. We
          also studied the finite sample behavior of the confidence
          interval of regression coefficients in terms of coverage
          probabilities based on different variance-covariance matrix
          estimators. Our results indicate that although MINQUE
          generally has the largest MSE, it performs relatively well
          in terms of coverage probabilities.},
  doi       = {10.1016/S0378-3758(02)00274-4},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {1-2},
  pages     = {121--136},
  volume    = {108}
}

@InBook{bera:2000,
  author    = {Bera, A. K.},
  editor    = {Rao, C. R. and Szekely, G. J.},
  year      = {2000},
  title     = {Hypothesis testing in the 20th century with a special
          reference to testing with misspecified models},
  abstract  = {A selection of articles presented at the Eighth Lukacs
          Symposium held at the Bowling Green State University, Ohio.
          They discuss consistency and accuracy of the sequential
          bootstrap, hypothesis testing, geometry in multivariate
          analysis, the classical extreme value model, the analysis
          of cross-classified data, diffusion models for neural
          activity, estimation with quadratic loss, econometrics,
          higher order asymptotics, pre- and post-limit theorems, and
          more.},
  booktitle = {Statistics for the 21st Century: Methodologies for
          Applications of the Future},
  chapter   = {3},
  comment   = {A large variety of tests are reviewed in this paper. Bera
          (2000) proceeds in historical order from the classic
          Pearson's goodness-of-fit test to Neyman-Pearson LR and MP
          tests framework (the issue of power maximization by
          maximizing the scores, and relation to the Rao's score/LM
          test is highlighted in that section), to Neyman's smooth
          test, and to Wald, Rao score and Neyman's C(alpha) tests.
          Relations of many classical econometric and statistical
          tests to special cases of the above are given. Additional
          considerations such as one-sided testing or testing when
          nuisance parmaters are not identified under the null, are
          given treatment.

          Bera (2000) then moves on to discuss the issues of
          misspecifications. The earliest discussion of specification
          issue is that between Fisher, E. Pearson and Student about
          underlying normality of the ANOVA-type tests, and fragile
          assumptions required for those to work. A detailed
          analytical treatment is given for tests with locally
          misspecified alternatives: even though the parametric
          constraints imposed by the null might be correct, ignoring
          the nuisance parameters makes the score test behave as
          non-central chi^2, thus having the wrong size.

          Further on, distributional misspecification is considered
          that leads to point quasi-MLE estimators maximizing
          information wrt true distribution, and sandwich-type
          estimators of variance. A review of the literature
          heteroskedasticity and autocorrelation-consistent variance
          estimators in econometrics is given, and corrections to
          Wald and score tests outlined. The information matrix (IM)
          test conludes the paper, and is presented as a version of
          the score test.

          Important philosophical and foundational remarks are spread
          throughout. For instance, in Sec. 3.2 Haavelmo's arguments
          about test construction and fine-tuning with respect to
          possible alternatives are given, and a characterization of
          possible misspecification (type III error) is provided. The
          following broad types are suggested: complete
          misspecification when the DGP is not even in the set of
          assumed alternatives; under-testing when the class of
          assumed alternatives is not rich enough to cover the true
          DGP (testing fewer directions of alternatives than really
          neded); and over-testing with over-specification, when the
          power is diluted in an overparameterized alternative model.
          Another characterizatoin of misspecifications might run
          along the lines of parametric, distributional and
          higher-order moments misspecifications. },
  howpublished  = {Hardcover},
  pages     = {33--90},
  publisher = {CRC}
}

@Article{bera:bilias:2002,
  author    = {Bera, Anil K. and Bilias, Yannis },
  year      = {2002},
  title     = {The {MM, ME, ML, EL, EF and GMM} approaches to estimation:
          a synthesis},
  abstract  = {The 20th century began on an auspicious statistical note
          with the publication of Karl Pearson's (Philos. Mag. Ser.
          50 (1900) 157) goodness-of-fit test, which is regarded as
          one of the most important scientific breakthroughs. The
          basic motivation behind this test was to see whether an
          assumed probability model adequately described the data at
          hand. Pearson (Philos. Trans. Roy. Soc. London Ser. A 185
          (1894) 71) also introduced a formal approach to statistical
          estimation through his method of moments (MM) estimation.
          Ronald A. Fisher, while he was a third year undergraduate
          at the Gonville and Caius College, Cambridge, suggested the
          maximum likelihood estimation (MLE) procedure as an
          alternative to Pearson's MM approach. In 1922 Fisher
          published a monumental paper that introduced such basic
          concepts as consistency, efficiency, sufficiency--and even
          the term "parameter" with its present meaning. Fisher
          (Philos. Trans. Roy. Soc. London Ser. A 222 (1922) 309)
          provided the analytical foundation of MLE and studied its
          efficiency relative to the MM estimator. Fisher (J. Roy.
          Statist. Soc. 87 (1924a) 442) established the asymptotic
          equivalence of minimum [chi]2 and ML estimators and wrote
          in favor of using minimum [chi]2 method rather than
          Pearson's MM approach. Recently, econometricians have found
          working under assumed likelihood functions restrictive, and
          have suggested using a generalized version of Pearson's MM
          approach, commonly known as the GMM estimation procedure as
          advocated in Hansen (Econometrica 50 (1982) 1029). Earlier,
          Godambe (Ann. Math. Statist. 31 (1960) 1208) and Durbin (J.
          Roy. Statist. Soc. Ser. B 22 (1960) 139) developed the
          estimating function (EF) approach to estimation that has
          been proven very useful for many statistical models. A
          fundamental result is that score is the optimum EF.
          Ferguson (Ann. Math. Statist. 29 (1958) 1046) considered an
          approach very similar to GMM and showed that estimation
          based on the Pearson [chi]2 statistic is equivalent to
          efficient GMM. Golan et al. (Maximum Entropy Econometrics:
          Robust Estimation with Limited Data. Wiley, New York, 1996)
          developed entropy-based formulation that allowed them to
          solve a wide range of estimation and inference problems in
          econometrics. More recently, Imbens et al. (Econometrica 66
          (1998) 333), Kitamura and Stutzer (Econometrica 65 (1997)
          861) and Mittelhammer et al. (Econometric Foundations.
          Cambridge University Press, Cambridge, 2000) put GMM within
          the framework of empirical likelihood (EL) and maximum
          entropy (ME) estimation. It can be shown that many of these
          estimation techniques can be obtained as special cases of
          minimizing Cressie and Read (J. Roy. Statist. Soc. Ser. B
          46 (1984) 440) power divergence criterion that comes
          directly from the Pearson (1900) [chi]2 statistic. In this
          way we are able to assimilate a number of seemingly
          unrelated estimation techniques into a unified framework.},
  doi       = {10.1016/S0304-4076(01)00113-0},
  journal   = {Journal of Econometrics},
  number    = {1-2},
  pages     = {51--86},
  volume    = {107}
}

@Article{bera:supr:prem:2002,
  author    = "Anil K. Bera and Totok Suprayitno and Gamini Premaratne",
  year      = 2002,
  title     = "On some hetero\-ske\-dast\-icity-robust estimators of
          variance{-}covariance matrix of the least-squares
          estimators",
  journal   = "Journal of Statistical Planning and Inference",
  volume    = 108,
  number    = "1--2",
  pages     = "121--136"
}

@Article{beran:1988,
  author    = {Beran, Rudolf },
  year      = {1988},
  title     = {Prepivoting Test Statistics: A Bootstrap View of
          Asymptotic Refinements},
  doi       = {10.2307/2289292},
  journal   = {Journal of the American Statistical Association},
  number    = {403},
  pages     = {687--697},
  volume    = {83}
}

@Article{beran:srivastava:1985,
  author    = {Beran, Rudolf and Srivastava, Muni S. },
  year      = {1985},
  title     = {Bootstrap Tests and Confidence Regions for Functions of a
          Covariance Matrix},
  doi       = {http://dx.doi.org/10.2307/2241146},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {95--115},
  publisher = {Institute of Mathematical Statistics},
  volume    = {13},
  abstract  = {Bootstrap tests and confidence regions for functions of
          the population covariance matrix have the desired
          asymptotic levels, provided model restrictions, such as
          multiple eigenvalues in the covariance matrix, are taken
          into account in designing the bootstrap algorithm.}
}

@Misc{bergmann:2011,
  author={Michael Bergmann},
  title={IPFWEIGHT: Stata module to create adjustment weights for surveys},
  year=2011,
  howpublished={Statistical Software Components, Boston College Department of Economics},
  abstract=
    {ipfweight performs a stepwise adjustment (known as iterative
    proportional fitting or raking) of survey sampling weights to achieve
    known population margins. The iterative process is repeated until the
    difference between the sample margins and the known population margins is
    smaller than a specified tolerance value or the specified maximum number
    of iterations is obtained. Additionally, thresholds for maximum and
    minimum weighting factors can be specified as well as a simple
    replacement of missing values.},
  note={RePEc:boc:bocode:s457353},
}


@Article{berk1995statistical,
  author    = {Berk, Richard A. and Western, Bruce and Weiss, Robert E.},
  year      = {1995},
  title     = {Statistical Inference for Apparent Populations},
  abstract  = {In this paper we consider statistical inference for
          datasets that are not replicable. We call these datasets,
          which are common in sociology, apparent populations. We
          review how such data are usually analyzed by sociologists
          and then suggest that perhaps a Bayesian approach has merit
          as an alternative. We illustrate our views with an
          empirical example.},
  doi       = {10.2307/271073},
  journal   = {Sociological Methodology},
  pages     = {421--458},
  volume    = {25}
}

@Article{berkhof:snijders:2001,
  author    = {Berkhof, Johannes and Snijders, Tom A.},
  year      = {2001},
  title     = {Variance Component Testing in Multilevel Models},
  abstract  = {Available variance component tests are reviewed and three
          new score tests are presented. In the first score test, the
          asymptotic normal distribution of the test statistic is
          used as a reference distribution. In the other two score
          tests, a Satterthwaite approximation is used for the null
          distribution of the test statistic. We evaluate the
          performance of the score tests and other available tests by
          means of a Monte Carlo study. The new tests are
          computationally relatively cheap and have good power
          properties. 10.3102/10769986026002133},
  doi       = {10.3102/10769986026002133},
  journal   = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS},
  number    = {2},
  pages     = {133--152},
  volume    = {26}
}

@TECHREPORT{berk:he:cata:2002,
  AUTHOR =       {Berkner, L. and He, S. and Cataldi, E. F.},
  TITLE =        {Descriptive summary of 1995--96 Beginning Postsecondary Students: Six years later},
  INSTITUTION =  {U.S. Department of Education, National Center for Education Statistics},
  YEAR =         {2002},
  number =       {2003-151},
  address =      {Washington, DC},
  source =       {Isabella Zaniletti},
}



@Article{berndt:hall:hall:hausman:1974,
  author    = {Berndt, E.K. and Hall, B.H. and Hall, R.E. and Hausman,
          J.A.},
  year      = {1974},
  title     = {Estimation and Inference in Nonlinear Structural Models},
  journal   = {Annals of Economic and Social Measurement},
  volume    = {3/4},
  pages     = {653--665}
}

@Article{bert:duflo:mull:2004,
  author    = "M. Bertrand and E. Duflo and S. Mullainathan",
  year      = "2004",
  title     = "How Much Should We Trust Difference in Differences
          Estimates?",
  journal   = "Quarterly Journal of Economics",
  volume    = "119",
  number    = "1",
  pages     = "249--275"
}

@Article{beve:1992,
  author    = "Beveridge, S",
  year      = 1992,
  title     = "Least-Squares Estimation of Missing Values in Time-Series",
  journal   = "Communications in Statistics---Theory and Methods",
  volume    = 21,
  number    = 12,
  pages     = "3479--3496"
}

@Book{bfso:1984,
  author    = {Breiman, Leo and Friedman, Jerome and Stone, Charles J.
          and Olshen, R. A.},
  year      = {1984},
  title     = {Classification and Regression Trees},
  abstract  = {{The methodology used to construct tree structured rules
          is the focus of this monograph. Unlike many other
          statistical procedures, which moved from pencil and paper
          to calculators, this text's use of trees was unthinkable
          before computers. Both the practical and theoretical sides
          have been developed in the authors' study of tree methods.
          Classification and Regression Trees reflects these two
          sides, covering the use of trees as a data analysis method,
          and in a more mathematical framework, proving some of their
          fundamental properties.}},
  howpublished  = {Paperback},
  isbn      = {0412048418},
  publisher = {{Chapman \& Hall/CRC}}
}

@Book{bhat:1996,
  author    = {Rajendra Bhatia},
  year      = {1996},
  title     = {Matrix Analysis},
  publisher = {Springer},
  series    = {Graduate Texts in Mathematics},
  address   = {New York}
}

@Article{bhat:2001,
  author    = {Bhat, C. R. },
  year      = {2001},
  title     = {Quasi-random maximum simulated likelihood estimation of
          the mixed multinomial logit model},
  journal   = {Transportation Research Part B: Methodological},
  number    = {7},
  pages     = {677--693},
  volume    = {35},
  abstract  = {This paper proposes the use of a quasi-random sequence for
          the estimation of the mixed multinomial logit model. The
          mixed multinomial structure is a flexible discrete choice
          formulation which accommodates general patterns of
          competitiveness as well as heterogeneity across individuals
          in sensitivity to exogenous variables. The estimation of
          this model has been achieved in the past using the
          pseudo-random maximum simulated likelihood method that
          evaluates the multi-dimensional integrals in the
          log-likelihood function by computing the integrand at a
          sequence of pseudo-random points and taking the average of
          the resulting integrand values. We suggest and implement an
          alternative quasi-random maximum simulated likelihood
          method which uses cleverly crafted non-random but more
          uniformly distributed sequences in place of the
          pseudo-random points in the estimation of the mixed logit
          model. Numerical experiments, in the context of intercity
          travel mode choice, indicate that the quasi-random method
          provides considerably better accuracy with much fewer draws
          and computational time than does the pseudo-random method.
          This result has the potential to dramatically influence the
          use of the mixed logit model in practice; specifically,
          given the flexibility of the mixed logit model, the use of
          the quasi-random estimation method should facilitate the
          application of behaviorally rich structures in discrete
          choice modeling.}
}

@Article{bhat:2003,
  author    = {Bhat, C. R.},
  year      = {2003},
  title     = {Simulation estimation of mixed discrete choice models
          using randomized and scrambled {H}alton sequences},
  abstract  = {The use of simulation techniques has been increasing in
          recent years in the transportation and related fields to
          accommodate flexible and behaviorally realistic structures
          for analysis of decision processes. This paper proposes a
          randomized and scrambled version of the Halton sequence for
          use in simulation estimation of discrete choice models. The
          scrambling of the Halton sequence is motivated by the rapid
          deterioration of the standard Halton sequence's coverage of
          the integration domain in high dimensions of integration.
          The randomization of the sequence is motivated from a need
          to statistically compute the simulation variance of model
          parameters. The resulting hybrid sequence combines the good
          coverage property of quasi-Monte Carlo sequences with the
          ease of estimating simulation error using traditional Monte
          Carlo methods. The paper develops an evaluation framework
          for assessing the performance of the traditional
          pseudo-random sequence, the standard Halton sequence, and
          the scrambled Halton sequence. The results of computational
          experiments indicate that the scrambled Halton sequence
          performs better than the standard Halton sequence and the
          traditional pseudo-random sequence for simulation
          estimation of models with high dimensionality of
          integration.},
  doi       = {10.1016/S0191-2615(02)00090-5},
  journal   = {Transportation Research Part B: Methodological},
  number    = {9},
  pages     = {837--855},
  volume    = {37}
}

@Article{bhat::2001,
  author    = {Bhat, C. R.},
  year      = {2001},
  title     = {Quasi-random maximum simulated likelihood estimation of
          the mixed multinomial logit model},
  abstract  = {This paper proposes the use of a quasi-random sequence for
          the estimation of the mixed multinomial logit model. The
          mixed multinomial structure is a flexible discrete choice
          formulation which accommodates general patterns of
          competitiveness as well as heterogeneity across individuals
          in sensitivity to exogenous variables. The estimation of
          this model has been achieved in the past using the
          pseudo-random maximum simulated likelihood method that
          evaluates the multi-dimensional integrals in the
          log-likelihood function by computing the integrand at a
          sequence of pseudo-random points and taking the average of
          the resulting integrand values. We suggest and implement an
          alternative quasi-random maximum simulated likelihood
          method which uses cleverly crafted non-random but more
          uniformly distributed sequences in place of the
          pseudo-random points in the estimation of the mixed logit
          model. Numerical experiments, in the context of intercity
          travel mode choice, indicate that the quasi-random method
          provides considerably better accuracy with much fewer draws
          and computational time than does the pseudo-random method.
          This result has the potential to dramatically influence the
          use of the mixed logit model in practice; specifically,
          given the flexibility of the mixed logit model, the use of
          the quasi-random estimation method should facilitate the
          application of behaviorally rich structures in discrete
          choice modeling.},
  journal   = {Transportation Research Part B: Methodological},
  number    = {7},
  pages     = {677--693},
  volume    = {35}
}

@Article{bhattacharya2005asymptotic,
  author    = {Bhattacharya, Debopam},
  year      = {2005},
  title     = {Asymptotic inference from multi-stage samples},
  abstract  = {I develop a GMM-based framework for asymptotic inference
          to analyze data from surveys whose designs involve
          stratification and clustering. I set up the estimation
          problem, derive the appropriate asymptotic distribution
          theory as the number of clusters per stratum tends to
          infinity and compute asymptotic standard errors that are
          robust to sample-design effects. The analysis is then
          extended to nonparametric regression and to semiparametric
          estimation based on U-processes. Empirical illustrations
          are provided using consumption expenditure data from the
          complexly designed Indian national household survey.},
  doi       = {10.1016/j.jeconom.2004.01.002},
  journal   = {Journal of Econometrics},
  number    = {1},
  pages     = {145--171},
  volume    = {126}
}

@Article{bickel:freedman:1981,
  author    = {Bickel, Peter J. and Freedman, David A. },
  year      = {1981},
  title     = {Some Asymptotic Theory for the Bootstrap},
  doi       = {10.2307/2240410},
  journal   = {The Annals of Statistics},
  number    = {6},
  pages     = {1196--1217},
  volume    = {9},
  abstract  = {Efron's "bootstrap" method of distribution approximation
          is shown to be asymptotically valid in a large number of
          situations, including t-statistics, the empirical and
          quantile processes, and von Mises functionals. Some
          counter-examples are also given, to show that the
          approximation does not always succeed.}
}

@Article{bickel:freedman:1984,
  author    = {Bickel, P. J. and Freedman, D. A.},
  year      = {1984},
  title     = {Asymptotic Normality and the Bootstrap in Stratified
          Sampling},
  abstract  = {This paper is about the asymptotic distribution of linear
          combinations of stratum means in stratified sampling, with
          and without replacement. Both the number of strata and
          their size is arbitrary. Lindeberg conditions are shown to
          guarantee asymptotic normality and consistency of variance
          estimators. The same conditions also guarantee the validity
          of the bootstrap approximation for the distribution of the
          t-statistic. Via a bound on the Mallows distance,
          situations will be identified in which the bootstrap
          approximation works even though the normal approximation
          fails. Without proper scaling, the naive bootstrap fails.},
  doi       = {10.2307/2241388},
  journal   = {The Annals of Statistics},
  number    = {2},
  pages     = {470--482},
  publisher = {Institute of Mathematical Statistics},
  volume    = {12}
}

@Article{bickel:gotze:vanzwet:1997,
  author    = {Bickel, P. J. and G\"{o}tze, F. and van Zwet, W. R. },
  year      = {1997},
  title     = {Resampling Fewer Than n Observations: Gains, Losses, and
          Remedies for Losses},
  journal   = {Statistica Sinica},
  pages     = {1--31},
  volume    = {7}
}

@Book{biemer:lyberg:2003,
  author    = {Biemer, Paul P. and Lyberg, Lars E.},
  year      = {2003},
  title     = {Introduction to Survey Quality},
  abstract  = {{Peruse the history of survey research and the essential
          concepts for data quality. With an emphasis on total survey
          error, the authors review principles and concepts in the
          field and examine important unresolved issues in survey
          methods. Spanning a range of topics dealing with the
          quality of data collected through the survey process, they
          focus on such key issues as: <p> <ul> <li>Major sources of
          survey error, examining the origins of each error source
          most successful methods for reducing errors from those
          sources <li>Methods most often used in practice for
          evaluating the effects of the source on total survey error
          <li>Implications of improving survey quality for
          organizational management and costs </ul>}},
  howpublished  = {Hardcover},
  isbn      = {0471193755},
  publisher = {Wiley-Interscience},
  series    = {Wiley Series in Survey Methodology}
}

@article{biemer:2010,
    abstract =
        {The total survey error (TSE) paradigm provides a theoretical
        framework for optimizing surveys by maximizing data quality within
        budgetary constraints. In this article, the TSE paradigm is viewed as
        part of a much larger design strategy that seeks to optimize surveys
        by maximizing total survey quality; i.e., quality more broadly
        defined to include user-specified dimensions of quality. Survey
        methodology, viewed within this larger framework, alters our
        perspectives on the survey design, implementation, and evaluation. As
        an example, although a major objective of survey design is to
        maximize accuracy subject to costs and timeliness constraints, the
        survey budget must also accommodate additional objectives related to
        relevance, accessibility, interpretability, comparability, coherence,
        and completeness that are critical to a survey's  ? fitness for use.?
        The article considers how the total survey quality approach can be
        extended beyond survey design to include survey implementation and
        evaluation. In doing so, the  ? fitness for use? perspective is shown
        to influence decisions regarding how to reduce survey error during
        design implementation and what sources of error should be evaluated
        in order to assess the survey quality today and to prepare for the
        surveys of the future.},
    author = {Biemer, Paul P.},
    doi = {10.1093/poq/nfq058},
    journal = {Public Opinion Quarterly},
    number = {5},
    pages = {817--848},
    title = {Total Survey Error: Design, Implementation, and Evaluation},
    volume = {74},
    year = {2010}
}


@TechReport{biewen:jenkins:2003,
  author    = {Martin Biewen and Stephen P. Jenkins},
  year      = 2003,
  title     = {Estimation of Generalized Entropy and Atkinson Inequality
          Indices from Survey Data},
  institution   = {Institute for Social and Economic Research},
  type      = {ISER working papers},
  note      = {available at
          http://ideas.repec.org/p/ese/iserwp/2003-11.html},
  number    = {2003--11},
  address   = {Colchester, University of Essex, UK}
}

@Misc{biewen:jenkins:svygeiatk,
  author    = {Stephen P. Jenkins and Martin Biewen},
  year      = 2005,
  title     = {SVYGEI\_SVYATK: Stata module to derive the sampling
          variances of Generalized Entropy and Atkinson inequality
          indices when estimated from complex survey},
  howpublished  = {Statistical Software Components, Boston College Department
          of Economics},
  note      = {RePEc handle: RePEc:boc:bocode:s453601. Available at
          http://ideas.repec.org/c/boc/bocode/s453601.html}
}

@article{bilias:chen:ying:2000,
    author = {Bilias, Y. and Songnian Chen and Zhiliang Ying},
    doi = {10.1016/S0304-4076(00)00042-7},
    issn = {03044076},
    journal = {Journal of Econometrics},
    number = {2},
    pages = {373--386},
    title = {Simple resampling methods for censored regression quantiles},
    volume = {99},
    year = {2000},
    abstract =
       {Powell (Journal of Econometrics 25 (1984) 303--325; Journal
       of Econometrics 32 (1986) 143--155) considered censored
       regression quantile estimators. The asymptotic covariance
       matrices of his estimators depend on the error densities and
       are therefore difficult to estimate reliably. The difficulty
       may be avoided by applying the bootstrap method (Hahn,
       Econometric Theory 11 (1995) 105–121). Calculation of the
       estimators, however, requires solving a nonsmooth and
       nonconvex minimization problem, resulting in high
       computational costs in implementing the bootstrap. We propose
       in this paper computationally simple resampling methods by
       convexfying Powell's approach in the resampling stage. A
       major advantage of the new methods is that they can be
       implemented by efficient linear programming. Simulation
       studies show that the methods are reliable even with moderate
       sample sizes.},
}

@Book{bill:1995,
  author    = "Patrick Billingsley",
  year      = 1995,
  title     = "Probability and Measure",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@InProceedings{bind:hidi:hbook6:1988,
  author    = "D. A. Binder and M. A. Hidiroglou",
  editor    = "P. R. Krishnaiah and C. R. Rao",
  year      = "1988",
  title     = "Sampling in Time",
  booktitle = "Handbook of Statistics",
  publisher = "North Holland",
  pages     = "187--211",
  volume    = "6",
  address   = "Amsterdam"
}

@Article{binder:1983,
  author    = {David A. Binder},
  year      = {1983},
  title     = {On the Variances of Asymptotically Normal Estimators from
          Complex Surveys},
  journal   = {International Statistical Review},
  volume    = {51},
  pages     = {279--292},
  comment   = { This is an early work leading to development of PMLE
          later by Skinner (1989). In the paper, Binder (1983) shows
          how estimating equations lead to the parameter estimates in
          a general setting, and with examples of linear regression,
          GLM, logistic regression, and loglinear (multinomial)
          model. }
}

@article{binder:1992,
    abstract =
        {The proportional hazards model makes a number of assumptions which
        may not always be completely satisfied. However, fitting such models
        can have both descriptive and analytical value. We discuss the
        implications of the survey design for large-scale studies. We propose
        a weighted estimator and derive a design-based estimator for the
        variance of the estimated parameters. The derivation requires using
        methods not previously used in the context of analysis of survey
        data. We conduct a small simulation study comparing the proposed
        method with others in the literature.},
    author = {Binder, David A.},
    doi = {10.1093/biomet/79.1.139},
    journal = {Biometrika},
    number = {1},
    pages = {139--147},
    title = {Fitting {Cox}'s proportional hazards models from survey data},
    volume = {79},
    year = {1992}
}



@Article{binder:1996,
  author    = {David Binder},
  year      = {1996},
  title     = {Linearization methods for single phase and two-phase
          samples: a cookbook approach},
  journal   = {Survey Methodology},
  volume    = {22},
  number    = {1},
  pages     = {17--22}
}

@Article{binder:kovacevic:1995,
  author    = {Binder, David A. and Kovacevic, Milorad S.},
  year      = {1995},
  title     = {Estimating Some Measures of Income Inequality from Survey
          Data: An Application of the Estiamting Equation Approach},
  journal   = {Survey Methodology},
  pages     = {137--145},
  volume    = {21}
}

@Article{binder:patak:1994,
  author    = {Binder, David A. and Patak, Zdenek},
  year      = {1994},
  title     = {Use of Estimating Functions for Estimation from Complex
          Surveys},
  abstract  = {We describe a method for point and interval estimation of
          population parameters from complex surveys using estimating
          functions. The theory was originally developed for infinite
          populations and has recently been applied to finite
          populations. With estimating functions, a unifying
          framework can be given for point and interval estimation of
          both finite and infinite population parameters. We discuss
          test inversion methods to derive confidence intervals for
          one-dimensional parameters and propose a method for
          eliminating nuisance parameters in the multidimensional
          setting. We show that special cases of our proposal result
          in conditional and orthogonal methods proposed in the
          literature. We describe a simulation study using real data
          to compare the coverage probabilities of confidence
          intervals obtained under various approaches.},
  journal   = {Journal of the American Statistical Association},
  number    = {427},
  pages     = {1035--1043},
  volume    = {89}
}

@InCollection{binder:roberts:2003,
  author    = {David A. Binder and Georgia R. Roberts},
  editor    = {R. L. Chambers and C. J. Skinner},
  year      = {2003},
  title     = {Design-based and Model-based Methods for Estimating Model
          Parameters},
  chapter   = {3},
  booktitle = {Analysis of Survey Data},
  publisher = {John Wiley \& Sons},
  address   = {New York}
}

@InCollection{binder:roberts:2009,
  author    = {David A. Binder and Georgia R. Roberts},
  editor    = {D. Pfeffermann and C. R. Rao},
  year      = {2009},
  title     = {Design- and Model-Based Inference for Model Parameters},
  chapter   = {24},
  booktitle = {Sample Surveys: Inference and Analysis},
  publisher = {Elsevier},
  address   = {Oxford, UK},
  series    = {Handbook of Statistics},
  volume    = {29B}
}

@Article{binder:theberge:1988,
  author    = {David A. Binder and A. Th{\'e}berge},
  year      = {1988},
  title     = {Estimating the Variance of Raking-Ratio Estimators},
  journal   = {Canadian Journal of Statistics},
  volume    = {16},
  number    = {supplement},
  pages     = {47--55}
}

@Book{bkrw:1998,
  author    = {Bickel, Peter J. and Klaassen, Chris A. J. and Ritov,
          Ya'acov and Wellner, Jon A. },
  year      = {1998},
  title     = {Efficient and Adaptive Estimation for Semiparametric
          Models},
  isbn      = {0387984739},
  publisher = {Springer},
  abstract  = {This book is about estimation in situations where we
          believe we have enough knowledge to model some features of
          the data parametrically, but are unwilling to assume
          anything for other features. Such models have arisen in a
          wide variety of contexts in recent years, particularly in
          economics, epidemiology, and astronomy. The complicated
          structure of these models typically requires us to consider
          nonlinear estimation procedures which often can only be
          implemented algorithmically. The theory of these procedures
          is necessarily based on asymptotic approximations.}
}

@Article{blalock:1961,
  author    = {Hubert M. Blalock},
  year      = {1961},
  title     = {Correlation and Causality: The Multivariate Case},
  abstract  = {Simon's method for making causal inferences from patterns
          of intercorrelations is applied to a five variable
          sociological problem. The technique is used in exploratory
          fashion to develop successive causal models with
          progressively better prediction equations. It is argued
          that Simon's method yields more theoretical insights than
          does partial correlation.},
  doi       = {10.2307/2573216},
  issn      = {00377732},
  journal   = {Social Forces},
  number    = {3},
  pages     = {246--251},
  publisher = {University of North Carolina Press},
  volume    = {39}
}

@techreport{blum:luke:davi:dave:yu:sode:2009,
    author = {Blumberg, Stephen J. and  Luke, Julian V. and Gestur Davidson and Michael E. Davern and Tzy-Chyi Yu and Karen Soderberg},
    institution = {National Center for Health Statistics},
    series = {National Health Statistics Report},
    title = {Wireless Substitution: State-level Estimates From the {N}ational {H}ealth {I}nterview {S}urvey, January-December 2007},
    number = {14},
    year = {2009}
}

@techreport{blum:gane:luke:gonz:2013,
    author = {Blumberg, Stephen J. and Ganesh, Nadarajasundaram and Luke, Julian V. and Gonzales, Gilbert},
    institution = {National Center for Health Statistics},
    series = {National Health Statistics Report},
    title = {Wireless Substitution: State-level Estimates From the {N}ational {H}ealth {I}nterview {S}urvey, 2012},
    number = {70},
    year = {2013}
}

@Article{bock:bargmann:1966,
  author    = {Bock, R. and Bargmann, Rolf },
  year      = {1966},
  title     = {Analysis of covariance structures},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {507--534},
  comment   = {http://dx.doi.org/10.1007/BF02289521},
  volume    = {31},
  abstract  = {A general method is presented for estimating variance
          components when the experimental design has one random way
          of classification and a possibly unbalanced fixed
          classification. The procedure operates on a sample
          covariance matrix in which the fixed classes play the role
          of variables and the random classes correspond to
          observations. Cases are considered which assume (i)
          homogeneous and (ii) nonhomogeneous error variance, and
          (iii) arbitrary scale factors in the measurements and
          homogeneous error variance. The results include
          maximum-likelihood estimations of the variance components
          and scale factors, likelihood-ratio tests of the
          goodness-of-fit of the model assumed for the design, and
          large-sample variances and covariances of the estimates.
          Applications to mental test data are presented. In these
          applications the subjects constitute the random dimension
          of the design, and a classification of the mental tests
          according to objective features of format or content
          constitute the fixed dimensions.}
}

@Article{boll:1987,
  author    = {Kenneth A. Bollen},
  year      = {1987},
  title     = {Outliers and Improper Solutions: A Confirmatory Factor
          Analysis Example},
  journal   = {Sociological Methods and Research},
  volume    = {15},
  pages     = {375--384}
}

@Article{boll:1990,
  author    = {Kenneth A. Bollen},
  year      = {1990},
  title     = {Outlier Screening and a Distribution-Free Test for
          Vanishing Tetrads},
  journal   = {Sociological Methods and Research},
  volume    = {19},
  pages     = {80--92}
}

@InCollection{boll:2001,
  author    = {Kenneth A. Bollen},
  editor    = {Robert Cudeck and Stephen Du Toit and Dag S{\"o}rbom},
  year      = {2001},
  title     = {Two-stage Least Squares and Latent Variable Models:
          Simultaneous Estimation and Robustness to
          Misspecifications},
  booktitle = {Structural Equation Modeling: Present and Future, A
          Festschrift in Honor of Karl J{\"o}reskog},
  pages     = {119--138},
  publisher = {Scientific Software}
}

@Article{boll:armi:1991,
  author    = {Kenneth A. Bollen and G. Arminger},
  year      = {1991},
  title     = {Observational Residuals in Factor Analysis and Structural
          Equation Models},
  journal   = {Sociological Methodology},
  volume    = {21},
  pages     = {235--262}
}

@Book{boll:curr:2006,
  author    = {Kenneth A. Bollen and Patrick M. Curran},
  year      = {2006},
  title     = {Latent Curve Models: A Structural Equation Perspective},
  publisher = {Wiley},
  address   = {New York, USA},
  pages     = {285}
}

@Article{boll:gran:1981,
  author    = {Kenneth A. Bollen and B. Grandjean},
  year      = {1981},
  title     = {The Dimension(s) of Democracy: Further Issues in the
          Measurement and Effects of Political Democracy},
  journal   = {American Sociological Review},
  volume    = {46},
  pages     = {232--239}
}

@TechReport{boll:kirb:curr:paxt:chen:2005,
  author    = {Kenneth A. Bollen and James B. Kirby and Patrick J. Curran
          and Pamela M. Paxton and Feinian Chen},
  year      = {2005},
  title     = {Latent Variable Models under Misspecification: Two Stage
          Least Squares ({2SLS}) and Maximum Likelihood ({ML})
          Estimators},
  institution   = {University of North Carolina},
  address   = {Chapel Hill}
}

@Article{boll:stin:1990,
  author    = {Kenneth A. Bollen and Robert Stine},
  year      = {1990},
  title     = {Direct and Indirect Effects: Classical and Bootstrap
          Estimates of Variability},
  journal   = {Sociological Methodology},
  volume    = {20},
  pages     = {115--140}
}

@Article{boll:stin:1992,
  author    = {Bollen, Kenneth A. and R. Stine},
  year      = {1992},
  title     = {Bootstrapping Goodness of Fit Measures in Structural
          Equation Models},
  journal   = {Sociological Methods and Research},
  volume    = {21},
  pages     = {205--229}
}

@Article{boll:ting:1993,
  author    = {Kenneth A. Bollen and Kwok-Fai Ting},
  year      = {1993},
  title     = {Confirmatory Tetrad Analysis},
  journal   = {Sociological Methodology},
  volume    = {23},
  pages     = {147--175}
}

@Article{boll:ting:1998,
  author    = {Kenneth A. Bollen and Kwok-Fai Ting},
  year      = {1998},
  title     = {Bootstrapping a Test Statistic for Vanishing Tetrads},
  journal   = {Sociological Methods and Research},
  volume    = {27},
  pages     = {77--102}
}

@Article{bollen:paxton:1998,
  author    = {Bollen, Kenneth A. and Paxton, Pamela},
  year      = {1998},
  title     = {Detection and Determinants of Bias in Subjective
          Measures},
  abstract  = {Many concepts in sociology are difficult or impossible to
          objectively measure. This limitation forces a reliance on
          subjective measures that typically contain both systematic
          and random measurement errors. Systematic errors, or
          \&quot;biases,\&quot; are the focus of this paper. Campbell
          and Fiske's (1959) multitrait-multimethod (MTMM) research
          design is the best known social scientific procedure for
          uncovering systematic errors, but the data requirements for
          classical MTMM designs are too demanding for many areas of
          sociology in which secondary data are the norm. We show
          that the benefits of the MTMM design are available under
          more relaxed conditions. In addition, we illustrate how
          researchers can examine the determinants of systematic
          errors and gain insights into the potential for confounding
          or spurious effects caused by systematic errors. We
          demonstrate the usefulness of these methods using the
          subjective measures of liberal democracy used in several
          recent ASR papers and provide additional examples,
          including measures of the reputational quality of graduate
          programs and job evaluations for comparable-worth
          investigations. We conclude that sociologists can do far
          more to understand the systematic error present in their
          subjective variables.},
  journal   = {American Sociological Review},
  number    = {3},
  pages     = {465--478},
  volume    = {63}
}

@Article{bollen:1984,
  author    = {Bollen, Kenneth A. },
  year      = {1984},
  title     = {Multiple indicators: Internal consistency or no necessary
          relationship?},
  doi       = {10.1007/BF00227593},
  journal   = {Quality and Quantity},
  number    = {4},
  pages     = {377--385},
  abstract  = {The preceding discussion demonstrates the importance of
          having an explicit measurement model before analyzing
          measures. It is not valid to make any blanket statement on
          whether or not indicators should correlate until we know
          what type of indicators they are. If they are
          effect-indicators that have ?well-behaved? errors and are
          positive measures of a single latent variable, then the
          internal-consistency view is appropriate and positive
          correlations of the indicators should occur. If
          cause-indicators are used then the NNR view is correct;
          indicator intercorrelations may be positive, negative, or
          zero. Finally, in general MIMIC models, cause-indicators
          have NNR while effect-indicators should be positively
          related under the assumptions of the model. In general, a
          cause- or an effect-indicator may have any type of
          relation.},
  volume    = {18}
}

@Article{bollen:1993,
  author    = {Bollen, Kenneth A.},
  year      = {1993},
  title     = {Liberal Democracy: Validity and Method Factors in
          Cross-National Measures},
  doi       = {10.2307/2111550},
  journal   = {American Journal of Political Science},
  number    = {4},
  pages     = {1207--1230},
  volume    = {37},
  abstract  = {This paper examines the definition and measurement of
          liberal democracy. Specifically, my purposes are (1) to
          propose a working definition of liberal democracy; (2) to
          outline a theory of ``method factors'' in
          subjective measures of liberal democracy; (3) to provide
          the first estimates of the proportion of variance due to
          systematic error, validity, and random error in commonly
          used measures; (4) to replicate these results across
          several years; and (5) to estimate the degree of liberal
          democracy in more than 150 countries. All but one measure
          contain systematic error, and in some cases the bias
          component is large. Furthermore, a new liberal democracy
          index has a .96 squared correlation with the liberal
          democracy latent variable and has negligible correlations
          with the method factors that are present in the individual
          indicators. The results suggest that the current practice
          of treating unadjusted democracy indicators as error free
          can be misleading.}
}

@Article{bollen:1996,
  author    = {Kenneth A. Bollen},
  year      = {1996},
  title     = {An Alternative Two Stage Least Squares ({2SLS}) Estimator
          for Latent Variable Models},
  journal   = {Psychometrika},
  volume    = {61},
  number    = {1},
  pages     = {109--121}
}

@InCollection{bollen:1996:hetero,
  author    = {Kenneth A. Bollen},
  editor    = {George Marcoulides and Randy Schumaker},
  year      = {1996},
  title     = {A Limited-Information Estimator for {LISREL} Models With
          and Without Heteroscedastic Errors},
  booktitle = {Advanced Structural Equation Modeling Techniques},
  pages     = {227--241},
  publisher = {Erlbaum},
  address   = {Mahwah, NJ}
}

@Article{bollen:2002,
  author    = {Bollen, Kenneth A. },
  year      = {2002},
  title     = {Latent Variables in Psychology and the Social Sciences},
  doi       = {10.1146/annurev.psych.53.100901.135239},
  journal   = {Annual Review of Psychology},
  number    = {1},
  pages     = {605--634},
  volume    = {53}
}

@Article{bollen:barb:1981,
  author    = "Kenneth A. Bollen and Kenney H. Barb",
  year      = 1981,
  title     = "Pearson's {R} and Coarsely Categorized Measures",
  journal   = "American Sociological Review",
  volume    = 46,
  pages     = "232--239"
}

@Article{bollen:bauer:2004,
  author    = {Bollen, Kenneth A. and Bauer, Daniel J.},
  year      = {2004},
  title     = {Automating the Selection of Model-Implied Instrumental
          Variables},
  doi       = {10.1177/0049124103260341},
  journal   = {Sociological Methods Research},
  number    = {4},
  pages     = {425--452},
  volume    = {32},
  abstract  = {Recently, interest has grown in the use of instrumental
          variables (IVs) in estimating factor analysis and latent
          variable models such as structural equations models. Bollen
          (1996) suggested a two-stage least squares (2SLS) technique
          that makes use of model-implied IVs in estimating the
          measurement and latent variable models. Model-implied
          instrumental variables are the observed variables in the
          model that can serve as instrumental variables in a given
          equation. One difficulty inhibiting the practical use of
          the 2SLS estimator is identifying the model-implied IVs.
          The authors provide a simple procedure that identifies the
          model-implied IVs and a computer algorithm that can easily
          be implemented to automate the selection of IVs for
          simultaneous equations, factor analysis, and latent
          variable models. 10.1177/0049124103260341}
}

@InCollection{bollen:bauer:christ:edwards:2010,
  author    = {Kenneth A. Bollen and Daniel J. Bauer and Sharon L. Christ
          and Michael C. Edwards},
  editor    = {Stanislav Kolenikov and Douglas Steinley and Lori Thombs},
  year      = {2010},
  title     = {An Overview of Structural Equation Models and Recent
          Extensions},
  booktitle = {Statistics in the Social Sciences: Current Methodological
          Developments},
  publisher = {Wiley},
  address   = {New York}
}

@Article{bollen:davis:2009a,
  author    = {Bollen, Kenneth A. and Davis, Walter R.},
  year      = {2009},
  title     = {Causal Indicator Models: Identification, Estimation, and
          Testing},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {3},
  pages     = {498--522},
  publisher = {Psychology Press},
  volume    = {16}
}

@Article{bollen:davis:2009b,
  author    = {Bollen, Kenneth A. and Davis, Walter R.},
  year      = {2009},
  title     = {Causal Indicator Models: Identification, Estimation, and
          Testing},
  doi       = {10.1080/10705510903008253},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {3},
  pages     = {498--522},
  publisher = {Psychology Press},
  volume    = {16}
}

@TechReport{bollen:glan:steck:2006,
  author    = "Kenneth A. Bollen and Jennifer L. Glanville and Guy
          Stecklov",
  year      = 2006,
  title     = "Socioeconomic Status, Permanent Income, and Fertility: A
          Latent Variable Approach",
  institution   = "MEASURE Evaluation Project at Carolina Population Center",
  address   = "Chapel Hill",
  type      = "Working Paper",
  number    = "WP-06-90"
}

@Article{bollen:glan:steck:ars2001,
  author    = "Kenneth A. Bollen and Jennifer L. Glanville and Guy
          Stecklov",
  year      = 2001,
  title     = "Socioeconomic Status and Class in Studies of Fertility and
          Health in Developing Countries",
  journal   = "Annual Review of Sociology",
  volume    = 27,
  pages     = "153--185"
}

@TechReport{bollen:glan:steck:cpcwp:2001,
  author    = "Kenneth A. Bollen and Jennifer L. Glanville and Guy
          Stecklov",
  year      = 2001,
  title     = "Economic Status Proxies in Studies of Fertility in
          Developing Countries: Does the Measure Matter?",
  institution   = "MEASURE Evaluation Project at Carolina Population Center",
  address   = "Chapel Hill",
  type      = "Working Paper",
  number    = "WP-01-38"
}

@TechReport{bollen:glan:steck:cpcwp:2002,
  author    = "Kenneth A. Bollen and Jennifer L. Glanville and Guy
          Stecklov",
  year      = 2002,
  title     = "Socioeconomic Status, Permanent Income, and Fertility: A
          Latent Variable Approach",
  institution   = "MEASURE Evaluation Project at Carolina Population Center",
  address   = "Chapel Hill",
  type      = "Working Paper",
  number    = "WP-02-62"
}

@Article{bollen:glan:steck:pops2002,
  author    = "Kenneth A. Bollen and Jennifer L. Glanville and Guy
          Stecklov",
  year      = 2002,
  title     = "Economic Status Proxies in Studies of Fertility in
          Developing Countries: Does the Measure Matter?",
  journal   = "Population Studies",
  volume    = "56",
  issue     = 1,
  pages     = "81--96",
  note      = "DOI: 10.1080/00324720213796"
}

@article{bollen:glanville:stecklov:2007:pops:latentvar,
    abstract =
        {This paper examines how permanent income and other components of socio-economic status ({SES}) are related to fertility in less developed countries. Because permanent income cannot be measured directly, we employ a latent-variable method. We compare our results with those of the more common proxy-variable method and investigate the consequences of not accounting for measurement error. Using data from Ghana and Peru, we find that permanent income has a large, negative influence on fertility and that research must take the latent nature of permanent income into account to uncover its influence. Controlling for measurement error in the proxies for permanent income can also lead to substantial changes in the estimated effects of control variables. Finally, we examine which of the common proxies for permanent income most closely capture the concept. The results have implications beyond this specific dependent variable, providing evidence on the sensitivity of microanalyses to the treatment of long-term economic status.},
    author = {Bollen, Kenneth A. and Glanville, Jennifer L. and Stecklov, Guy},
    doi = {10.1080/00324720601103866},
    journal = {Population Studies},
    number = {1},
    pages = {15--34},
    title = {Socio-economic status, permanent income, and fertility: A latent-variable approach},
    doi = {10.1080/00324720601103866},
    volume = {61},
    year = {2007}
}

@Article{bollen:kirby:curran:paxton:chen:2007,
  author    = {Bollen, K. A. and Kirby, J. B. and Curran, P. J. and
          Paxton, P. M. and Chen, F. },
  year      = {2007},
  title     = {Latent variable models under misspecification: two-stage
          least squares ({2SLS}) and Maximum Likelihood ({ML})
          estimators},
  doi       = {10.1177/0049124107301947},
  journal   = {Sociological Methods and Research},
  number    = {1},
  pages     = {48--86},
  volume    = {36},
  abstract  = {This article compares maximum likelihood (ML) estimation
          to three variants of two-stage least squares (2SLS)
          estimation in structural equation models. The authors use
          models that are both correctly and incorrectly specified.
          Simulated data are used to assess bias, efficiency, and
          accuracy of hypothesis tests. Generally, 2SLS with reduced
          sets of instrumental variables performs similarly to ML
          when models are correctly specified. Under correct
          specification, both estimators have little bias except at
          the smallest sample sizes and are approximately equally
          efficient. As predicted, when models are incorrectly
          specified, 2SLS generally performs better, with less bias
          and more accurate hypothesis tests. Unless a researcher has
          tremendous confidence in the correctness of his or her
          model, these results suggest that a 2SLS estimator should
          be considered. {\copyright} 2007 Sage Publications.}
}

@Article{bollen:lennox:1991,
  author    = {Kenneth Bollen and Richard Lennox},
  year      = {1991},
  title     = {Conventional Wisdom on Measurement: A Structural Equation
          Perspective},
  journal   = {Psychological Bulletin},
  volume    = {110},
  number    = {2},
  pages     = {305--314}
}

@Book{bollen:long:1993,
  editor    = "Kenneth A. Bollen and J. Scott Long",
  year      = "1993",
  title     = "Testing structural equation models",
  publisher = "Sage",
  address   = "Thousand Oaks, CA"
}

@Article{bollen:maydeu:olivares:2007,
  author    = {Bollen, Kenneth and Maydeu-Olivares, Albert},
  year      = {2007},
  title     = {A Polychoric Instrumental Variable (PIV) Estimator for
          Structural Equation Models with Categorical Variables},
  abstract  = {Abstract\&nbsp;\&nbsp; This paper presents a new
          polychoric instrumental variable (PIV) estimator to use in
          structural equation models (SEMs) with categorical observed
          variables. The PIV estimator is a generalization of
          Bollen's (Psychometrika 61:109–121, 1996) 2SLS/IV
          estimator for continuous variables to categorical
          endogenous variables. We derive the PIV estimator and its
          asymptotic standard errors for the regression coefficients
          in the latent variable and measurement models. We also
          provide an estimator of the variance and covariance
          parameters of the model, asymptotic standard errors for
          these, and test statistics of overall model fit. We examine
          this estimator via an empirical study and also via a small
          simulation study. Our results illustrate the greater
          robustness of the PIV estimator to structural
          misspecifications than the system-wide estimators that are
          commonly applied in SEMs.},
  doi       = {10.1007/s11336-007-9006-3},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {309--326},
  volume    = {72}
}

@Article{bollen:paxton:1998:asr,
  author    = {Bollen, Kenneth A. and Paxton, Pamela },
  year      = {1998},
  title     = {Detection and Determinants of Bias in Subjective
          Measures},
  abstract  = {Many concepts in sociology are difficult or impossible to
          objectively measure. This limitation forces a reliance on
          subjective measures that typically contain both systematic
          and random measurement errors. Systematic errors, or
          \&quot;biases,\&quot; are the focus of this paper. Campbell
          and Fiske's (1959) multitrait-multimethod (MTMM) research
          design is the best known social scientific procedure for
          uncovering systematic errors, but the data requirements for
          classical MTMM designs are too demanding for many areas of
          sociology in which secondary data are the norm. We show
          that the benefits of the MTMM design are available under
          more relaxed conditions. In addition, we illustrate how
          researchers can examine the determinants of systematic
          errors and gain insights into the potential for confounding
          or spurious effects caused by systematic errors. We
          demonstrate the usefulness of these methods using the
          subjective measures of liberal democracy used in several
          recent ASR papers and provide additional examples,
          including measures of the reputational quality of graduate
          programs and job evaluations for comparable-worth
          investigations. We conclude that sociologists can do far
          more to understand the systematic error present in their
          subjective variables.},
  journal   = {American Sociological Review},
  number    = {3},
  pages     = {465--478},
  volume    = {63}
}

@InCollection{bollen:paxton:1998:book,
  author    = {Kenneth Bollen and Pamela Paxton},
  editor    = {Randall E. Schumacker and George A. Marcoulides},
  year      = {1998},
  title     = {Two-Stage Least Squares Estimation of Interaction
          Effects},
  booktitle = {Interaction and Nonlinear Effects in Structural Equation
          Modeling},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, New Jersey}
}

@Article{bollen:paxton:1998:sem,
  author    = {Bollen, Kenneth A. and Paxton, Pamela },
  year      = {1998},
  title     = {Interactions of Latent Variables in Structural Equation
          Models},
  journal   = {Structural Equation Modeling},
  number    = {3},
  pages     = {267--293},
  volume    = {5},
  abstract  = {Provides a discussion of an alternative two-stage least
          squares (2SLS) technique to include interactions of latent
          variables in structural equation models. The method
          requires selection of instrumental variables, and rules for
          selection are presented. An empirical example and
          Statistical Analysis System programs are presented.}
}

@Book{bollen:selv:1989,
  author    = {Kenneth A. Bollen},
  year      = 1989,
  title     = "Structural Equations with Latent Variables",
  publisher = "Wiley",
  address   = "New York"
}

@Article{bollen:ting:2000,
  author    = {Bollen, Kenneth A. and Ting, Kwok-Fai},
  year      = {2000},
  title     = {A tetrad test for causal indicators},
  journal   = {Psychological Methods},
  number    = {1},
  pages     = {3--22},
  volume    = {5},
  abstract  = {The authors propose a confirmatory tetrad analysis test to
          distinguish causal from effect indicators in structural
          equation models. The test uses "nested" vanishing tetrads
          that are often implied when comparing causal and effect
          indicator models. The authors present typical models that
          researchers can use to determine the vanishing tetrads for
          4 or more variables. They also provide the vanishing
          tetrads for mixtures of causal and effect indicators, for
          models with fewer than 4 indicators per latent variable, or
          for cases with correlated errors. The authors illustrate
          the test results for several simulation and empirical
          examples and emphasize that their technique is a
          theory-testing rather than a model-generating approach.
          They also review limitations of the procedure including the
          indistinguishable tetrad equivalent models, the largely
          unknown finite sample behavior of the test statistic, and
          the inability of any procedure to fully validate a model
          specification.}
}

@article{bollen:biem:karr:tuel:berz:2016,
    author = {Bollen, Kenneth A. and Biemer, Paul P. and Karr, Alan F. and Tueller, Stephen and Berzofsky, Marcus E.},
    doi = {10.1146/annurev-statistics-011516-012958},
    journal = {Annual Review of Statistics and Its Application},
    pages = {375--392},
    title = {Are Survey Weights Needed? A Review of Diagnostic Tests in Regression Analysis},
    volume = {3},
    year = {2016}
}

@Book{boom:1983,
  author    = {Anne Boomsma},
  year      = {1983},
  title     = {On the Robustness of {LISREL} (Maximum Likelihood
          Estimation) Against Small Sample Size and Nonnormality},
  publisher = {Sociometric Research Foundation},
  address   = {Amsterdam, the Netherlands}
}

@Book{boom:mari:snij:2000,
  editor    = {Anne Boomsma and Marijtje van Duijn and Tom Snijders},
  year      = {2000},
  title     = {Essays on Item Response Theory},
  publisher = {Springer},
  series    = {Lecture Notes in Statistics},
  address   = {New York}
}

@Misc{boomsma:hoogland:2001,
  author    = {Anne Boomsma and Jeffrey J. Hoogland},
  year      = {2001},
  title     = {The Robustness of {LISREL} Modeling Revisited},
  howpublished  = {Structural Equation Modeling: Present and Future}
}

@Article{booth:butler:hall:1994,
  author    = {Booth, James G. and Butler, Ronald W. and Hall, Peter},
  year      = {1994},
  title     = {Bootstrap Methods for Finite Populations},
  doi       = {10.2307/2290991},
  journal   = {Journal of the American Statistical Association},
  number    = {428},
  pages     = {1282--1289},
  volume    = {89},
  abstract  = {We show that the familiar bootstrap plug-in rule of Efron
          has a natural analog in finite population settings. In our
          method a characteristic of the population is estimated by
          the average value of the characteristic over a class of
          empirical populations constructed from the sample. Our
          method extends that of Gross to situations in which the
          stratum sizes are not integer multiples of their respective
          sample sizes. Moreover, we show that our method can be used
          to generate second-order correct confidence intervals for
          smooth functions of population means, a property that has
          not been established for other resampling methods suggested
          in the literature. A second resampling method is proposed
          that also leads to second-order correct confidence
          intervals and is less computationally intensive than our
          bootstrap. But a simulation study reveals that the second
          method can be quite unstable in some situations, whereas
          our bootstrap performs very well.}
}

@Book{boro:1999,
  author    = "A. A. Borovkov and K. Borovkov and O. Borovkova",
  year      = 1999,
  title     = "Probability Theory",
  publisher = "T\&F STM"
}

@ARTICLE{boudreau:lawless:2006,
  AUTHOR =       {Christian Boudreau and Jerald F. Lawless},
  TITLE =        {Survival analysis based on the proportional hazards model and survey data},
  JOURNAL =      {The Canadian Journal of Statistics},
  YEAR =         {2006},
  volume =       {34},
  number =       {2},
  pages =        {203--216},
}


@Article{bouis:1994,
  author    = {Bouis, H.E.},
  year      = {1994},
  title     = {The Effect of Income on Demand for Food in Poor Countries:
          Are Our Food Consumption Databases Giving Us Reliable
          Estimates?},
  journal   = {Journal of Development Economics},
  volume    = {44},
  number    = {1},
  pages     = {199--226}
}

@Article{bound1995problems,
  author    = {Bound, John and Jaeger, David A. and Baker, Regina M.},
  year      = {1995},
  title     = {Problems with Instrumental Variables Estimation When the
          Correlation Between the Instruments and the Endogeneous
          Explanatory Variable is Weak},
  doi       = {10.2307/2291055},
  journal   = {Journal of the American Statistical Association},
  number    = {430},
  pages     = {443--450},
  volume    = {90}
}

@Article{bound:jaeger:baker:1995,
  author    = {Bound, John and Jaeger, David A. and Baker, Regina M. },
  year      = {1995},
  title     = {Problems with Instrumental Variables Estimation When the
          Correlation Between the Instruments and the Endogeneous
          Explanatory Variable is Weak},
  doi       = {10.2307/2291055},
  journal   = {Journal of the American Statistical Association},
  number    = {430},
  pages     = {443--450},
  volume    = {90}
}

@TechReport{bour:pere:stern:2002,
  author    = "Bourguignon, F. and L. Pereira da Silva and N. Stern",
  year      = "2002",
  title     = "Evaluating the Poverty Impact of Economic Policies: Some
          Analytical Challenges",
  institution   = "The World Bank",
  address   = "Washington, DC"
}

@Article{box:cox:1964,
  author    = "G. E. P. Box and D. R. Cox",
  year      = 1964,
  title     = "An analysis of transformations",
  journal   = "Journal of the Royal Statistical Society",
  series    = "B",
  volume    = 26,
  issue     = 2,
  pages     = "211--252"
}

@Article{box:cox:1982,
  author    = "G. E. P. Box and D. R. Cox",
  title     = "An analysis of transformations revisited, rebutted",
  journal   = "Journal of the American Statistical Association",
  volume    = 77,
  pages     = "209--210"
}

@Book{box:draper:1987,
  author    = {Box, George E. P. and Norman R. Draper},
  year      = {1987},
  title     = {Empirical Model-Building and Response Surfaces},
  publisher = {Wiley},
  address   = {New York}
}

@article{braaten:weller:1979,
    author = {Braaten, E. and Weller, G.},
    doi = {10.1016/0021-9991(79)90019-6},
    journal = {Journal of Computational Physics},
    number = {2},
    pages = {249--258},
    title = {An improved low-discrepancy sequence for multidimensional
             quasi-{M}onte {C}arlo integration},
    volume = {33},
    year = {1979},
    abstract =
       {We present an improved method of generating vectors for
       Monte Carlo integration, which produces a significant
       improvement in rate of convergence over previous methods for
       problems in more than eight dimensions.},
}

@Book{braz:davi:reid:2007,
  author    = {Brazzale, A. R. and Davison, A. C. and Reid, N. },
  year      = {2007},
  title     = {Applied Asymptotics: Case Studies in Small-Sample
          Statistics},
  isbn      = {0521847036},
  publisher = {Cambridge University Press},
  address   = {New York},
  series    = {Cambridge Series in Statistical and Probabilistic
          Mathematics},
  abstract  = {In fields such as biology, medical sciences, sociology,
          and economics researchers often face the situation where
          the number of available observations, or the amount of
          available information, is sufficiently small that
          approximations based on the normal distribution may be
          unreliable. Theoretical work over the last quarter-century
          has led to new likelihood-based methods that lead to very
          accurate approximations in finite samples, but this work
          has had limited impact on statistical practice. This book
          illustrates by means of realistic examples and case studies
          how to use the new theory, and investigates how and when it
          makes a difference to the resulting inference. The
          treatment is oriented towards practice and comes with code
          in the R language (available from the web) which enables
          the methods to be applied in a range of situations of
          interest to practitioners. The analysis includes some
          comparisons of higher order likelihood inference with
          bootstrap or Bayesian methods. Author resource page:
          http://www.isib.cnr.it/~brazzale/AA/}
}

@article{breidt:opsomer:2008,
    author = {Breidt, F. Jay and Opsomer, Jean D.},
    doi = {10.1214/009053607000000703},
    journal = {Annals of Statistics},
    number = {1},
    pages = {403--427},
    title = {Endogenous post-stratification in surveys: classifying with a sample-fitted model},
    volume = {36},
    year = {2008},
    abstract =
       {Post-stratification is frequently used to improve the
       precision of survey estimators when categorical auxiliary
       information is available from sources outside the survey. In
       natural resource surveys, such information is often obtained
       from remote sensing data, classified into categories and
       displayed as pixel-based maps. These maps may be constructed
       based on classification models fitted to the sample data.
       Post-stratification of the sample data based on categories
       derived from the sample data (``endogenous
       post-stratification'') violates the standard
       post-stratification assumptions that observations are
       classified without error into post-strata, and post-stratum
       population counts are known. Properties of the endogenous
       post-stratification estimator are derived for the case of a
       sample-fitted generalized linear model, from which the
       post-strata are constructed by dividing the range of the
       model predictions into predetermined intervals. Design
       consistency of the endogenous post-stratification estimator
       is established under mild conditions. Under a superpopulation
       model, consistency and asymptotic normality of the endogenous
       post-stratification estimator are established, showing that
       it has the same asymptotic variance as the traditional
       post-stratified estimator with fixed strata. Simulation
       experiments demonstrate that the practical effect of first
       fitting a model to the survey data before post-stratifying is
       small, even for relatively small sample sizes.},
}

@article{breidt:opsomer:2017,
  title={Model-assisted survey estimation with modern prediction techniques},
  author={Breidt, F Jay and Opsomer, Jean D},
  journal={Statistical Science},
  volume={32},
  number={2},
  pages={190--205},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@Article{breusch:qian:schmidt:wyhowski:1999,
  author    = {Trevor Breusch and Hailong Qian and Peter Schmidt and
          Donald Wyhowski},
  year      = {1999},
  title     = {Redundancy of moment conditions},
  journal   = {Journal of Econometrics},
  volume    = {91},
  pages     = {89--111},
  abstract  = {It is well known that adding moment conditions cannot
          decrease asymptotic efficiency. However, sometimes
          additional moment conditions are redundant, in the sense
          that they do not increase the asymptotic efficiency of
          estimation for some or all of the parameters of interest.
          This paper gives a general treatment of redundancy of
          moment conditions. It provides necessary and sufficient
          conditions for redundancy, in several equivalent forms. The
          paper also provides interesting results for the case that
          there are three (or more) sets of moment conditions.}
}

@article{brewer:1963,
  author = {Brewer, K. R. W.},
  title = {Ratio estimation and finite populations: 
      Some results deductible from the assumption of an underlying stochastic process},
  journal = {Australian Journal of Statistics},
  volume = {5},
  number = {3},
  pages = {93-105},
  doi = {https://doi.org/10.1111/j.1467-842X.1963.tb00288.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-842X.1963.tb00288.x},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-842X.1963.tb00288.x},
  year = {1963}
}



@Book{brewer:hanif:1983,
  author    = {Brewer, Ken and M Hanif},
  year      = {1983},
  title     = {Sampling with Unequal Probabilities},
  publisher = {Springer-Verlag},
  address   = {New York}
}

@Book{brewer:2002,
  author    = {K. Brewer},
  year      = {2002},
  title     = {Combined Survey Sampling Inference},
  publisher = {Arnold/Oxford University Press}
}

@Article{brewer:donadio:2003,
  author    = {Brewer, K.R.W. and Donadio, Martin E.},
  year      = {2003},
  title     = {The high entropy variance of the {H}orvitz-{T}hompson
          estimator},
  journal   = {Survey Methodology},
  volume    = {29},
  number    = {2},
  pages     = {189--196},
  abstract  = {Using both purely design-based and model-assisted
          arguments, it is shown that, under conditions of high
          entropy, the variance of the Horvitz-Thompson (HT)
          estimator depends almost entirely on first order inclusion
          probabilities. Approximate expressions and estimators are
          derived for this ?high entropy? variance of the HT
          estimator. Monte Carlo simulation studies are conducted to
          examine the statistical properties of the proposed variance
          estimators.},
  note      = {Available at
          http://www.statcan.ca/english/ads/12-001-XIE/12-001-XIE20030026778.pdf}

}

@TechReport{brick:morg:vall:2000,
  author    = {J. Michael Brick and David Morganstein and Richard
          Valliant},
  year      = {2000},
  title     = {Analysis of Complex Sample Data Using Replication},
  institution   = {Westat},
  type      = {technical report},
  note      = {http://www.westat.com/wesvar/techpapers/ACS-Replication.pdf}

}

@Book{brown:1986,
  author    = {Brown, L. D.},
  year      = {1986},
  title     = {Fundamentals of statistical exponential families: with
          applications in statistical decision theory},
  address   = {California},
  isbn      = {0940600102},
  publisher = {Institute of Mathematical Statistics}
}

@Book{brown:1987,
  author    = {Brown, L. D. },
  year      = {1987},
  title     = {Fundamentals of statistical exponential families: with
          applications in statistical decision theory},
  address   = {California},
  isbn      = {0940600102},
  publisher = {Institute of Mathematical Statistics},
  series    = {IMS Lecture Notes},
  volume    = 9,
  comment   = {http://portal.acm.org/citation.cfm?id=41464}
}

@Book{brown:2006:cfa,
  author    = {Brown, Timothy A. },
  year      = {2006},
  title     = {Confirmatory Factor Analysis for Applied Research},
  isbn      = {1593852754},
  publisher = {The Guilford Press},
  series    = {Methodology In The Social Sciences}
}

@Article{brown:newey:2002,
  author    = {Brown, B. W. and Newey, W. K. },
  year      = {2002},
  title     = {Generalized Method of Moments, Efficient Bootstrapping,
          and Improved Inference},
  issue     = {4},
  volume    = {20},
  journal   = {Journal of Business and Economic Statistics},
  pages     = {507--517},
  abstract  = {Generalized method of moments (GMM) has been an important
          innovation in econometrics. Its usefulness has motivated a
          search for good inference procedures based on GMM. This
          article presents a novel method of bootstrapping for GMM
          based on resampling from the empirical likelihood
          distribution that imposes the moment restrictions. We show
          that this approach yields a large-sample improvement and is
          efficient, and give examples. We also discuss the
          development of GMM and other recent work on improved
          inference.}
}

@Article{browne:1974,
  author    = {Browne, Michael W. },
  year      = {1974},
  title     = {Generalized least squares estimators in the analysis of
          covariances structures},
  journal   = {South African Statistical Journal},
  pages     = {1--24},
  volume    = {8}
}

@InCollection{browne:1982,
  author    = {Browne, Michael W.},
  editor    = {D. M. Hawkins},
  year      = {1982},
  title     = {Covariance structures},
  booktitle = {Topics in Multivariate Analysis},
  publisher = {Cambridge University Press},
  pages     = {72--141}
}

@Article{browne:1984,
  author    = "Michael W. Browne",
  year      = 1984,
  title     = "Asymptotically distribution-free methods for the analysis
          of the covariance structures",
  journal   = "British Journal of Mathematical and Statistical
          Psychology",
  volume    = 37,
  pages     = "62--83"
}

@Article{browne:1987,
  author    = {Browne, Michael W. },
  year      = {1987},
  title     = {Robustness of statistical inference in factor analysis and
          related models},
  doi       = {http://dx.doi.org/10.1093/biomet/74.2.375},
  journal   = {Biometrika},
  pages     = {375--384},
  volume    = {74},
  abstract  = {A class of latent variable models which includes the
          unrestricted factor analysis model is considered. It is
          shown that minimum discrepancy test statistics and
          estimators derived under normality assumptions retain their
          asymptotic properties when the common factors are not
          normally distributed but the unique factors do have a
          multivariate normal distribution. The minimum discrepancy
          test statistics and estimators considered include the usual
          likelihood ratio test statistic and maximum likelihood
          estimators. 10.1093/biomet/74.2.375}
}

@Article{browne:adf1984,
  author    = "M. W. Browne",
  year      = 1984,
  title     = "Asymptotically distribution-free methods for the analysis
          of the covariance structures",
  journal   = "British Journal of Mathematical and Statistical
          Psychology",
  volume    = 37,
  pages     = "62--83"
}

@Article{browne:adf:1984,
  author    = "M. W. Browne",
  year      = 1984,
  title     = "Asymptotically Distribution-Free Methods for the Analysis
          of Covariance Structures",
  journal   = "British Journal of Mathematical and Statistical
          Psychology",
  volume    = 37,
  pages     = "62--83"
}

@InCollection{browne:cudeck:1993,
  author    = {Michael W. Browne and Robert Cudeck},
  editor    = {Kenneth A Bollen and J Scott Long},
  year      = {1993},
  title     = {Alternative Ways of Assessing Model Fit},
  booktitle = {Testing Sructural Equation Models},
  publisher = {SAGE Publishers},
  chapter   = {6},
  pages     = {136--162},
  address   = {Newbury Park, CA, USA}
}

@Article{browne:maccallum:kim:anderson:glaser:2002,
  author    = {Browne, Michael W. and Mac{C}allum, Robert C. and Kim,
          Cheong-Tag T. and Andersen, Barbara L. and Glaser, Ronald},
  year      = {2002},
  title     = {When fit indices and residuals are incompatible.},
  abstract  = {Standard chi-square-based fit indices for factor analysis
          and related models have a little known property: They are
          more sensitive to misfit when unique variances are small
          than when they are large. Consequently, very small
          correlation residuals indicating excellent fit can be
          accompanied by indications of bad fit by the fit indices
          when unique variances are small. An empirical example of
          this incompatibility between residuals and fit indices is
          provided. For illustrative purposes, an artificial example
          is provided that yields exactly the same correlation
          residuals as the empirical example but has larger unique
          variances. For this example, the fit indices indicate
          excellent fit. A theoretical explanation for this
          phenomenon is provided using relationships between unique
          variances and eigenvalues of the fitted correlation
          matrix.},
  issn      = {1082-989X},
  journal   = {Psychological methods},
  number    = {4},
  pages     = {403--421},
  volume    = {7}
}

@Article{brualdi:mell:1994,
  author    = "Richard A. Brualdi and Stephen Mellendorf",
  year      = 1994,
  title     = "Regions in the Complex Plane Containing the Eigenvalues of
          a Matrix",
  journal   = "The American Mathematical Monthly",
  volume    = 101,
  issue     = 10,
  pages     = "975--985"
}

@Article{buch:1998,
  author    = "Moshe Buchinsky",
  year      = 1998,
  title     = "Recent Advances in Quantile Regression Models: A Practical
          Guideline for Empirical Research",
  journal   = "The Journal of Human Resources",
  volume    = 33,
  number    = 1,
  pages     = "88--126"
}

@ARTICLE{buchmann:2009,
  AUTHOR =       {Buchmann, C.},
  TITLE =        {Gender inequalities in the transition to college},
  JOURNAL =      {Teachers College Record},
  YEAR =         {2009},
  volume =       {111},
  number =       {10},
  pages =        {2320},
  source =       {Isabella Zaniletti},
}


@Article{buhmann:rain:schmaus:smee:1988,
  author    = "Buhmann, B. and Rainwater, L. and Schmaus, G. and
          Smeeding, T.",
  year      = "1988",
  title     = "Equivalence Scales, Well-Being, Inequality and Poverty:
          Sensitive Estimates Across Ten Countries Using the
          {Luxembourg Income Study (LIS)} database",
  journal   = "The Review of Income and Wealth",
  volume    = "34",
  pages     = "115--142"
}

@Article{buhmann:rainwater:schmaus:smeeding:1988,
  author    = {Buhmann, Brigitte and Rainwater, Lee and Schmaus, Guenther
          and Smeeding, Timothy M.},
  year      = {1988},
  title     = {Equivalence Scales, Well-Being, Inequality, and Poverty:
          Sensitivity Estimates Across Ten Countries Using the
          {L}uxembourg {I}ncome {S}tudy ({LIS}) Database},
  abstract  = {The Luxembourg Income Study (LIS) database on which this
          article is based offers researchers exciting new
          possibilities for international comparisons based on
          household income microdata. Among the choices the LIS
          microdata allows a researcher, e.g. income definition,
          income accounting unit, etc., is the choice of family
          equivalence scale, a method for estimating economic
          well-being by adjusting income for measurable differences
          in need. The range of potential equivalence scales that can
          and are being used in the ten LIS countries and elsewhere
          to adjust incomes for size and related differences in need
          span a wide spectrum. The purpose of this paper is to
          review the available equivalence scales and to test the
          sensitivity of various income inequality and poverty
          measures to choice of equivalence scale using the LIS
          database. The results of our analysis indicate that choice
          of equivalence scale can sometimes systematically affect
          absolute and relative levels of poverty; and inequality and
          therefore rankings of countries (or population subgroups
          within countries). Because of these sensitivities, one must
          carefully consider summary statements and policy
          implications derived from cross-national comparisons of
          poverty and/or inequality.},
  address   = {Technical Director, LIS; Harvard University and Research
          Director, LIS; Technical Director, CEPS and
          Director-at-Large, LIS; Vanderbilt University and Overall
          Project Director, LIS},
  doi       = {10.1111/j.1475-4991.1988.tb00564.x},
  issn      = {1475-4991},
  journal   = {Review of Income and Wealth},
  number    = {2},
  pages     = {115--142},
  volume    = {34}
}

@Article{burkam:lee:1998,
  author    = {Burkam, David T. and Lee, Valerie E.},
  year      = {1998},
  title     = {Effects of Monotone and Nonmonotone Attrition on Parameter
          Estimates in Regression Models with Educational Data:
          Demographic Effects on Achievement, Aspirations, and
          Attitudes},
  abstract  = {Using the High School and Beyond longitudinal study, we
          investigate the participation patterns across four waves of
          data. Because nonrespondents from one wave are recontacted
          at subsequent waves, both monotone and nonmonotone
          attrition patterns arise. We discuss correlates of these
          two types of attrition in an attempt to describe
          individuals who may be at-risk of attrition. Gender and
          incomplete participation in the base-year (respondents who
          exhibit item nonresponse on key variables) are important
          predictors of later attrition. Estimated effects of
          monotone and nonmonotone attrition on parameter estimates
          in regression models suggest that certain demographic
          effects will be biased due to sample attrition. The
          evidence for bias is neither pervasive nor consistent, but
          suggests a systematic inflation of the Black-White
          achievement disparity.},
  doi       = {10.2307/146441},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {555--574},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Article{buse:1982,
  author    = {Buse, A. },
  year      = {1982},
  title     = {The Likelihood Ratio, {Wald}, and {Lagrange} Multiplier
          Tests: An Expository Note},
  journal   = {The American Statistician},
  number    = {3},
  pages     = {153--157},
  volume    = {36}
}

@Article{buskirk:lohr:2005,
  author    = {Buskirk, T.},
  year      = {2005},
  title     = {Asymptotic properties of kernel density estimation with
          complex survey data},
  abstract  = {Kernel density estimation has been used with great success
          with data that may be assumed to be generated from
          independent and identically distributed (iid) random
          variables. The methods and theoretical results for iid
          data, however, do not directly apply to data from
          stratified multistage samples. We present finite-sample and
          asymptotic properties of a modified density estimator
          introduced in Buskirk (Proceedings of the Survey Research
          Methods Section, American Statistical Association (1998),
          pp. 799–801) and Bellhouse and Stafford (Statist. Sin. 9
          (1999) 407–424); this estimator incorporates both the
          sampling weights and the kernel weights. We present
          regularity conditions which lead the sample estimator to be
          consistent and asymptotically normal under various modes of
          inference used with sample survey data. We also introduce a
          superpopulation structure for model-based inference that
          allows the population model to reflect naturally occurring
          clustering. The estimator, and confidence bands derived
          from the sampling design, are illustrated using data from
          the US National Crime Victimization Survey and the US
          National Health and Nutrition Examination Survey.},
  doi       = {10.1016/j.jspi.2003.09.036},
  issn      = {03783758},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {1},
  pages     = {165--190},
  volume    = {128}
}

@article{buskirk:kolenikov:2015,
  title={Finding respondents in the forest: A comparison of logistic regression and random forest models for response propensity weighting and stratification},
  author={Buskirk, Trent D and Kolenikov, Stanislav},
  journal={Survey Methods: Insights from the Field},
  pages={1--17},
  year={2015},
  publisher={DEU},
  note={Retrieved from https://surveyinsights.org/?p=5108}
}

@Article{butl:1986,
  author    = {Butler, Ronald W.},
  year      = 1986,
  title     = {Predictive Likelihood Inference With Applications (C/R:
          P23-38)},
  journal   = {Journal of the Royal Statistical Society, Series B,
          Methodological},
  volume    = 48,
  pages     = {1--23}
}

@Article{byrne:goffin:1993,
  author    = {Byrne, Barbara M. and Goffin, Richard D. },
  year      = {1993},
  title     = {Modeling {MTMM} Data from Additive and Multiplicative
          Covariance Structures: An Audit of Construct Validity
          Concordance},
  abstract  = {The purpose of this study was to determine the extent to
          which findings derived from four approaches to MTMM
          analyses were consistent in providing evidence of construct
          validity related to the measurement of four dimensions of
          perceived competence (social, academic, English,
          mathematics) across four maximally dissimilar rating
          methods (self, teacher, parent, peer). MTMM methodological
          approaches included that of Campbell-Fiske (1959), the
          general confirmatory factor analytic (CFA) model (Joreskog,
          1969), the CFA (Correlated Uniqueness) model (Marsh, 1988),
          and the Composite Direct Product model (Browne, 1984).
          Procedures were applied to data from 158 grade 11 high
          school adolescents. Advantages, disadvantages, findings,
          and caveats related to each procedure are discussed.},
  doi       = {http://dx.doi.org/10.1207/s15327906mbr2801\_5},
  journal   = {Multivariate Behavioral Research},
  number    = {1},
  pages     = {67--96},
  publisher = {Psychology Press},
  volume    = {28}
}

@Article{cadi:1995,
  author    = {Noel G. Cadigan},
  year      = {1995},
  title     = {Local Influence in Structural Equation Models},
  journal   = {Structural Equation Modeling},
  volume    = {2},
  pages     = {13--30}
}

@article{cadw:thom:boyl:bark:2010,
    author = {Cadwell, B. L. and Thompson, T. J. and Boyle, J. P. and Barker, L. E.},
    journal = {Journal of Data Science},
    number = {1},
    pages = {173--188},
    title = {{Bayesian} small area estimates of diabetes prevalence by {U.S}. county, 2005},
    volume = {8},
    year = {2010}
}

@ARTICLE{caison:2007,
  AUTHOR =       {Caison, A.},
  TITLE =        {Analysis of institutionally specific retention research:
                  a comparison between survey and institutional database methods},
  JOURNAL =      {Research in Higer Education},
  YEAR =         {2007},
  volume =       {48},
  number =       {4},
  pages =        {435},
  source =       {Isabella Zaniletti},
}


@Article{call:harv:1991,
  author    = {Callanan, Terrance P. and Harville, David A.},
  year      = 1991,
  title     = {Some New Algorithms for Computing Restricted Maximum
          Likelihood Estimates of Variance Components},
  journal   = {Journal of Statistical Computation and Simulation},
  volume    = 38,
  pages     = {239--259}
}

@Article{calv:1993,
  author    = {Calvin, James A.},
  year      = 1993,
  title     = {REML Estimation in Unbalanced Multivariate Variance
          Components Models Using An {E}M Algorithm},
  journal   = {Biometrics},
  volume    = 49,
  pages     = {691--701}
}

@Book{cameron:triverdi:2005,
  author    = {Cameron, A. Colin and Trivedi, Pravin K.},
  year      = {2005},
  title     = {Microeconometrics: Methods and Applications},
  abstract  = {{This book provides the most comprehensive treatment to
          date of microeconometrics, the analysis of individual-level
          data on the economic behavior of individuals or firms using
          regression methods for cross section and panel data. The
          book is oriented to the practitioner. A basic understanding
          of the linear regression model with matrix algebra is
          assumed. The text can be used for a microeconometrics
          course, typically a second-year economics PhD course; for
          data-oriented applied microeconometrics field courses; and
          as a reference work for graduate students and applied
          researchers who wish to fill in gaps in their toolkit.
          Distinguishing features of the book include emphasis on
          nonlinear models and robust inference, simulation-based
          estimation, and problems of complex survey data. The book
          makes frequent use of numerical examples based on generated
          data to illustrate the key models and methods. More
          substantially, it systematically integrates into the text
          empirical illustrations based on seven large and
          exceptionally rich data sets.}},
  howpublished  = {Hardcover},
  isbn      = {0521848059},
  publisher = {Cambridge University Press}
}

@TechReport{cameron:gelbach:miller:2006,
  author    = {Cameron, Colin A. and Gelbach, Jonah B. and Miller,
          Douglas L.},
  year      = {2006},
  title     = {Robust Inference with Multi-way Clustering},
  institution   = {National Bureau of Economic Research},
  number    = {327},
  series    = {Technical Working Paper Series},
  type      = {Working Paper}
}


@Article{cameron:gelbach:miller:2008,
  author    = {Cameron, Colin A. and Miller, Douglas and Gelbach, Jonah B.},
  year      = {2008},
  title     = {Bootstrap-Based Improvements for Inference with Clustered
          Errors},
  journal   = {The Review of Economics and Statistics},
  number    = {3},
  pages     = {414--427},
  volume    = {90},
  abstract  = {Researchers have increasingly realized the need to account
          for within-group dependence in estimating standard errors
          of regression parameter estimates. The usual solution is to
          calculate cluster-robust standard errors that permit
          heteroskedasticity and within-cluster error correlation,
          but presume that the number of clusters is large. Standard
          asymptotic tests can over-reject, however, with few (five
          to thirty) clusters. We investigate inference using cluster
          bootstrap-t procedures that provide asymptotic refinement.
          These procedures are evaluated using Monte Carlos,
          including the example of Bertrand, Duflo, and Mullainathan
          (2004). Rejection rates of 10\% using standard methods can
          be reduced to the nominal size of 5\% using our methods}
}

@article{cameron:gelbach:miller:2011,
    author = {Cameron, A. Colin and Gelbach, Jonah B. and Miller, Douglas L.},
    journal = {Journal of Business \& Economic Statistics},
    number = {2},
    pages = {238--249},
    title = {Robust Inference With Multiway Clustering},
    volume = {29},
    year = {2011}
}

@Article{cant:davi:hink:vent:2006,
  author    = {Canty, A. J. and Davison, A. C. and Hinkley, D. V. and
          Ventura, V.},
  year      = {2006},
  title     = {Bootstrap diagnostics and remedies},
  abstract  = {Bootstrap diagnostics are used to assess the reliability
          of bootstrap calculations and may suggest useful modified
          calculations when these are possible. Concern focuses on
          susceptibility to peculiarities in data, incorrectness of a
          resampling model, incorrect use of resampling simulation
          output, and inherent inaccuracy of the bootstrap approach.
          The last involves issues such as inconsistency of a
          bootstrap method, the order of correctness of a consistent
          bootstrap method, and approximate pivotality. The authors
          review here some of these problems, provide workable
          diagnostic methods where possible, and discuss fast and
          simple ways to effect the necessary computations.},
  comment   = {The authors show about a thousand different ways the
          bootstrap method can give incorrect answers to the variance
          estimation problem, and show some diagnostics that can be
          performed for some of those cases. This paper is a must for
          anybody using the bootstrap in their empirical work.},
  journal   = {The Canadian Journal of Statistics},
  number    = {1},
  pages     = {5--27},
  volume    = {34}
}

@Article{canty:davi:hink:vent:2006,
  author    = {Canty, Angelo J. and Davison, Anthony C. and Hinkley,
          David V. and Ventura, Valerie },
  year      = {2006},
  title     = {Bootstrap diagnostics and remedies},
  journal   = {The Canadian Journal of Statistics/La revue canadienne de
          statistique},
  number    = {1},
  pages     = {5--27},
  volume    = {34}
}

@Article{capp:jenk:2006,
  author    = {Lorenzo Cappellari and Stephen P. Jenkins},
  year      = 2006,
  title     = {Calculation of multivariate normal probabilities by
          simulation, with applications to maximum simulated
          likelihood estimation},
  journal   = {Stata Journal},
  volume    = {6},
  number    = {2},
  pages     = {156-189}
}

@PhDThesis{caragea:2003,
  author    = "Caragea, P.",
  year      = 2003,
  title     = "Approximate Likelihoods for Spatial Processes",
  school    = "University of North Carolina",
  address   = "Chapel Hill"
}

@ARTICLE{card:benn:nels:gros:2009,
  AUTHOR =       {Bradley J. Cardinale and Danuta M. Bennett and Craig E. Nelson and Kevin Gross},
  TITLE =        {Does productivity drive diversity or vice versa?
                  A test of the multivariate productivity-diversity hypothesis in streams},
  JOURNAL =      {Ecology},
  YEAR =         {2009},
  volume =       {90},
  number =       {5},
  pages =        {1227--1241},
}


@Article{carle:2009,
  author    = {Carle, Adam},
  year      = {2009},
  title     = {Fitting multilevel models in complex survey data with
          design weights: Recommendations},
  abstract  = {BACKGROUND:Multilevel models (MLM) offer complex survey
          data analysts a unique approach to understanding individual
          and contextual determinants of public health. However,
          little summarized guidance exists with regard to fitting
          MLM in complex survey data with design weights. Simulation
          work suggests that analysts should scale design weights
          using two methods and fit the MLM using unweighted and
          scaled-weighted data. This article examines the performance
          of scaled-weighted and unweighted analyses across a variety
          of MLM and software programs.METHODS:Using data from the
          2005-2006 National Survey of Children with Special Health
          Care Needs (NS-CSHCN: n = 40,723) that collected data from
          children clustered within states, I examine the performance
          of scaling methods across outcome type (categorical vs.
          continuous), model type (level-1, level-2, or combined),
          and software (Mplus, MLwiN, and GLLAMM).RESULTS:Scaled
          weighted estimates and standard errors differed slightly
          from unweighted analyses, agreeing more with each other
          than with unweighted analyses. However, observed
          differences were minimal and did not lead to different
          inferential conclusions. Likewise, results demonstrated
          minimal differences across software programs, increasing
          confidence in results and inferential conclusions
          independent of software choice.CONCLUSION:If including
          design weights in MLM, analysts should scale the weights
          and use software that properly includes the scaled weights
          in the estimation.},
  doi       = {10.1186/1471-2288-9-49},
  issn      = {1471-2288},
  journal   = {BMC Medical Research Methodology},
  number    = {1},
  pages     = {49+},
  volume    = {9}
}

@Book{carlin:louis:2000,
  author    = {Bradley P. Carlin and Thomas A. Louis},
  year      = {2000},
  title     = {Bayes and Empirical Bayes Methods for Data Analysis},
  publisher = {Chapman \& Hall/CRC},
  edition   = {2nd},
  address   = {Boca Raton, FL}
}

@Article{carlstein:1986,
  author    = {Carlstein, Edward },
  year      = {1986},
  title     = {The Use of Subseries Values for Estimating the Variance of
          a General Statistic from a Stationary Sequence},
  journal   = {The Annals of Statistics},
  number    = {3},
  pages     = {1171--1179},
  volume    = {14}
}

@Book{carr:scott:wass:2005,
  editor    = {Peter J. Carrington and John Scott and Stanley Wasserman
          },
  year      = {2005},
  title     = {Models and Methods in Social Network Analysis},
  publisher = {Cambridge University Press},
  series    = {Structural Analysis in the Social Sciences}
}

@Book{carroll2006measurement,
  author    = {Carroll, Raymond J. and Ruppert, David and Stefanski,
          Leonard A. and Crainiceanu, Ciprian M.},
  year      = {2006},
  title     = {Measurement Error in Nonlinear Models: A Modern
          Perspective, Second Edition},
  abstract  = {{<P>It's been over a decade since the first edition of
          Measurement Error in Nonlinear Models splashed onto the
          scene, and research in the field has certainly not cooled
          in the interim. In fact, quite the opposite has occurred.
          As a result, Measurement Error in Nonlinear Models: A
          Modern Perspective, Second Edition has been revamped and
          extensively updated to offer the most comprehensive and
          up-to-date survey of measurement error models currently
          available. What's new in the Second Edition? · Greatly
          expanded discussion and applications of Bayesian
          computation via Markov Chain Monte Carlo techniques · A
          new chapter on longitudinal data and mixed models · A
          thoroughly revised chapter on nonparametric regression and
          density estimation · A totally new chapter on
          semiparametric regression · Survival analysis expanded
          into its own separate chapter · Completely rewritten
          chapter on score functions · Many more examples and
          illustrative graphs · Unique data sets compiled and made
          available online In addition, the authors expanded the
          background material in Appendix A and integrated the
          technical material from chapter appendices into a new
          Appendix B for convenient navigation. Regardless of your
          field, if you're looking for the most extensive discussion
          and review of measurement error models, then Measurement
          Error in Nonlinear Models: A Modern Perspective, Second
          Edition is your ideal source.</P>}},
  howpublished  = {Hardcover},
  isbn      = {1584886331},
  publisher = {{Chapman \& Hall/CRC}}
}

@Book{carroll:rup:stef:1995,
  author    = "Raymond J. Carroll and David Ruppert and Leonard A.
          Stefanski",
  year      = 1995,
  title     = "Measurement Error in Nonlinear Models",
  publisher = "Chapman and Hall/CRC"
}

@Book{carroll:rup:stef:crain:2006,
  author    = "Raymond J. Carroll and David Ruppert and Leonard A.
          Stefanski and Ciprian M. Crainiceanu",
  year      = 2006,
  title     = "Measurement Error in Nonlinear Models: A Modern
          Perspective",
  publisher = "Chapman and Hall/CRC",
  edition   = "2nd"
}

@TechReport{carroll:wang:simp:strom:rupp:1998,
  author    = {R. J. Carroll and Suojin Wang and D. G. Simpson and A. J.
          Stromberg and D. Ruppert},
  year      = {1998},
  title     = {The Sandwich (Robust Covariance Matrix) Estimator},
  institution   = {Deparment of Statistics, Texas A \& M University},
  address   = {College Station, TX},
  note      = {Available at
          http://www.stat.tamu.edu/ftp/pub/rjcarroll/sandwich.pdf},
  abstract  = {The sandwich estimator, often known as the robust
          covariance matrix estimator or the empirical covariance
          matrix estimator, has achieved increasing use with the
          growing popularity of generalized estimating equations. Its
          virtue is that it provides consistent estimates of the
          covariance matrix for parameter estimates even when a
          parametric model fails to hold, or is not even specified.
          Surprisingly though, there has been little discussion of
          the properties of the sandwich method other than
          consistency. We investigate the sandwich estimator in
          quasilikelihood models asymptotically, and in the linear
          case analytically. We show that when the quasilikelihood
          model is correct, the sandwich covariance matrix estimate
          is often far more variable than the usual parametric
          variance estimate, and its coverage probabilities can be
          abysmal. The increased variance is a fixed feature of the
          method, and the price one pays to obtain consistency even
          when the parametric model fails. We make some simple
          suggestions for modifying the method which improve coverage
          probabilities.}
}

@article{casella:ghosh:gill:2010:lasso,
  title={Penalized regression, standard errors, and Bayesian lassos},
  author={Casella, George and Ghosh, Malay and Gill, Jeff and Kyung, Minjung},
  journal={Bayesian analysis},
  volume={5},
  number={2},
  pages={369--411},
  year={2010},
  publisher={International Society for Bayesian Analysis}
}

@Article{caudill:zanella:mixon:2000,
  author    = "Steven B. Caudill and Fernando C. Zanella and Franklin G.
          Mixon",
  year      = 2000,
  title     = "Is Economic Freedom One Dimension? {A} Factor Analysis of
          Some Common Measures of Economic Freedom",
  journal   = "Journal of Economic Development",
  volume    = 25,
  pages     = "17--40"
}

@TECHREPORT{cdc:2013:brfss:compare,
  author =       {{Centers for Disease Control and Prevention}},
  title =        {Comparability of Data {BRFSS} 2013},
  year =         {2013},
  address =      {Atlanta, GA},
  note =         {Available from http://www.cdc.gov/brfss/annual{\_}data/2013/pdf/compare{\_}2013.pdf}
}


@Article{chamberlain:1987,
  author    = {Chamberlain, G. },
  year      = {1987},
  title     = {Asymptotic efficiency in estimation with conditional
          moment restrictions},
  doi       = {http://dx.doi.org/10.1016/0304-4076(87)90015-7},
  issn      = {03044076},
  journal   = {Journal of Econometrics},
  number    = {3},
  pages     = {305--334},
  volume    = {34}
}

@Article{chambers:dunstan:1986,
  author    = {Chambers, R. L. and Dunstan, R.},
  year      = {1986},
  title     = {Estimating distribution functions from survey data},
  abstract  = {A simple method for estimating population distribution
          functions and associated quantiles from sample survey data
          is described and some asymptotic theory for it presented.
          The method assumes a model-based approach to survey
          estimation and allows auxiliary population information to
          be directly incorporated into the estimation process. Monte
          Carlo results comparing the proposed method with
          conventional design-based methods are given. These suggest
          that the model-based approach offers significant gains when
          the auxiliary population information is linearly related to
          the survey variables of interest. 10.1093/biomet/73.3.597},
  doi       = {10.1093/biomet/73.3.597},
  journal   = {Biometrika},
  number    = {3},
  pages     = {597--604},
  volume    = {73}
}

@Book{chambers:skinner:2003,
  editor    = {Roy L. Chambers and Chris J. Skinner},
  year      = {2003},
  title     = {Analysis of Survey Data},
  publisher = {Wiley},
  series    = {Wiley series in survey methodology},
  address   = {New York}
}

@article{chambers:tzavidis:2006,
    abstract =
        {Small area estimation techniques typically rely on regression models
        that use both covariates and random effects to explain variation
        between the areas. However, such models also depend on strong
        distributional assumptions, require a formal specification of the
        random part of the model and do not easily allow for outlier-robust
        inference. We describe a new approach to small area estimation that
        is based on modelling quantilelike parameters of the conditional
        distribution of the target variable given the covariates. This avoids
        the problems associated with specification of random effects,
        allowing inter-area differences to be characterised by area-specific
        M-quantile coefficients. The proposed approach is easily made robust
        against outlying data values and can be adapted for estimation of a
        wide range of area-specific parameters, including quantiles of the
        distribution of the target variable in the different small areas. The
        differences between M-quantile and random effects models are
        discussed and the alternative approaches to small area estimation are
        compared using both simulated and real data.},
    author = {Chambers, Ray and Tzavidis, Nikos},
    doi = {10.1093/biomet/93.2.255},
    journal = {Biometrika},
    number = {2},
    pages = {255--268},
    title = {M-quantile models for small area estimation},
    volume = {93},
    year = {2006}
}

@article{chang:kott:2008,
    author = {Chang, Ted and Kott, Phillip S.},
    doi = {10.1093/biomet/asn022},
    journal = {Biometrika},
    number = {3},
    pages = {555--571},
    title = {Using calibration weighting to adjust for nonresponse under a plausible model},
    volume = {95},
    year = {2008},
    abstract =
        {When we estimate the population total for a survey variable
        or variables, calibration forces the weighted estimates of
        certain covariates to match known or alternatively estimated
        population totals called benchmarks. Calibration can be used
        to correct for sample-survey nonresponse, or for coverage
        error resulting from frame undercoverage or unit
        duplication. The quasi-randomization theory supporting its
        use in nonresponse adjustment treats response as an
        additional phase of random sampling. The functional form of
        a quasi-random response model is assumed to be known, its
        parameter values estimated implicitly through the creation
        of calibration weights. Unfortunately, calibration depends
        upon known benchmark totals while the covariates in a
        plausible model for survey response may not be the benchmark
        covariates. Moreover, it may be prudent to keep the number
        of covariates in a response model small. We use calibration
        to adjust for nonresponse when the benchmark model and
        covariates may differ, provided the number of the former is
        at least as great as that of the latter. We discuss the
        estimation of a total for a vector of survey variables that
        do not include the benchmark covariates, but that may
        include some of the model covariates. We show how to measure
        both the additional asymptotic variance due to the
        nonresponse in a calibration-weighted estimator and the full
        asymptotic variance of the estimator itself. All variances
        are determined with respect to the randomization mechanism
        used to select the sample, the response model generating the
        subset of sample respondents, or both. Data from the U.S.
        National Agricultural Statistical Service's 2002 Census of
        Agriculture and simulations are used to illustrate
        alternative adjustments for nonresponse. The paper concludes
        with some remarks about adjustment for coverage error.},
}

@article{chang:krosnick:2009,
    abstract =
        {In a national field experiment, the same questionnaires were
        administered simultaneously by {RDD} telephone interviewing, by the
        Internet with a probability sample, and by the Internet with a
        nonprobability sample of people who volunteered to do surveys for
        money. The probability samples were more representative of the nation
        than the nonprobability sample in terms of demographics and electoral
        participation, even after weighting. The nonprobability sample was
        biased toward being highly engaged in and knowledgeable about the
        survey's topic (politics). The telephone data manifested more random
        measurement error, more survey satisficing, and more social
        desirability response bias than did the Internet data, and the
        probability Internet sample manifested more random error and
        satisficing than did the volunteer Internet sample. Practice at
        completing surveys increased reporting accuracy among the probability
        Internet sample, and deciding only to do surveys on topics of
        personal interest enhanced reporting accuracy in the nonprobability
        Internet sample. Thus, the nonprobability Internet method yielded the
        most accurate self-reports from the most biased sample, while the
        probability Internet sample manifested the optimal combination of
        sample composition accuracy and self-report accuracy. These results
        suggest that Internet data collection from a probability sample
        yields more accurate results than do telephone interviewing and
        Internet data collection from nonprobability samples.},
    author = {Chang, Linchiat and Krosnick, Jon A.},
    doi = {10.1093/poq/nfp075},
    journal = {Public Opinion Quarterly},
    number = {4},
    pages = {641--678},
    title = {National Surveys Via {RDD} Telephone Interviewing Versus the Internet},
    volume = {73},
    year = {2009}
}

@article{chang:krosnick:2010,
    abstract =
        {A previous field experiment conducted via national surveys showed
        that data collected via the Internet manifested higher concurrent and
        predictive validity and less random and systematic measurement error
        than data collected via telephone interviewing. To ascertain the
        extent to which these differences were attributable to mode per se, a
        laboratory experiment was conducted in which respondents were
        randomly assigned to answer questions either on a computer or over an
        intercom with an interviewer. Replicating findings from the national
        surveys, the laboratory experiment indicated higher concurrent
        validity, less survey satisficing, and less social desirability
        response bias in the computer mode than in the intercom mode. The
        mode difference in concurrent validity and non-differentiation was
        most pronounced among respondents with more limited cognitive skills.
        Taken together, these results suggest a potential inherent advantage
        of questionnaire self-administration on the computer over telephone
        administration.},
    author = {Chang, Linchiat and Krosnick, Jon A.},
    doi = {10.1093/poq/nfp090},
    journal = {Public Opinion Quarterly},
    number = {1},
    pages = {154--167},
    title = {Comparing Oral Interviewing with Self-Administered Computerized Questionnaires: An Experiment},
    volume = {74},
    year = {2010}
}

@article{chao:1982,
    author = {Chao, M. T.},
    doi = {10.1093/biomet/69.3.653},
    journal = {Biometrika},
    number = {3},
    pages = {653--656},
    title = {A general purpose unequal probability sampling plan},
    volume = {69},
    year = {1982},
    abstract =
        {We present a general purpose unequal probability without replacement
        sampling plan with fixed sample size. In contrast to existing such
        plans, our scheme keeps the sample size fixed and lets the population
        units enter the sample one at a time through a carefully designed
        random mechanism. Consequently, all high-order inclusion
        probabilities can be easily computed.},
}

@Book{chaud:stenger:2005,
  author    = {Arijit Chaudhuri and Horst Stenger},
  year      = {2005},
  title     = {Survey Sampling: Theory and Methods},
  publisher = {Chapman \& Hall/CRC},
  volume    = {181},
  series    = {Statistics: Textbooks and Monographs},
  address   = {Boca Raton, FL},
  edition   = {2nd},
  isbn      = {0824757548}
}

@Article{chaudhuri:1992,
  author    = {Chaudhuri, Probal},
  year      = {1992},
  title     = {Multivariate Location Estimation Using Extension of
          R-Estimates Through U-Statistics Type Approach},
  abstract  = {We consider a class of U-statistics type estimates for
          multivariate location. The estimates extend some
          R-estimates to multivariate data. In particular, the class
          of estimates includes the multivariate median considered by
          Gini and Galvani (1929) and Haldane (1948) and a
          multivariate extension of the well-known Hodges-Lehmann
          (1963) estimate. We explore large sample behavior of these
          estimates by deriving a Bahadur type representation for
          them. In the process of developing these asymptotic
          results, we observe some interesting phenomena that closely
          resemble the famous shrinkage phenomenon observed by Stein
          (1956) in high dimensions. Interestingly, the phenomena
          that we observe here occur even in dimension d = 2.},
  journal   = {The Annals of Statistics},
  number    = {2},
  pages     = {897--916},
  volume    = {20}
}

@article{chen:sitter:1999,
    author = {Chen, Jiahua and Sitter, Randy R},
    journal = {Statistica Sinica},
    pages = {385--406},
    title = {A pseudo empirical likelihood approach to the effective use of auxiliary information in complex surveys},
    volume = {9},
    year = {1999}
}


@Article{chen:asokan:abraham:2008,
  author    = "Jiahua Chen and Asokan Mulayath Variyath and Bovas Abraham",
  year      = "2008",
  title     = "Adjusted Empirical Likelihood and its Properties",
  journal   = "Journal of Computational and Graphical Statistics",
  volume    = "17",
  pages     = "426--443",
  doi       = "doi:10.1198/106186008X321068",
  abstract  = "Computing a profile empirical likelihood function, which
          involves constrained maximization, is a key step in
          applications of empirical likelihood. However, in some
          situations, the required numerical problem has no solution.
          In this case, the convention is to assign a zero value to
          the profile empirical likelihood. This strategy has at
          least two limitations. First, it is numerically difficult
          to determine that there is no solution; second, no
          information is provided on the relative plausibility of the
          parameter values where the likelihood is set to zero. In
          this article, we propose a novel adjustment to the
          empirical likelihood that retains all the optimality
          properties, and guarantees a sensible value of the
          likelihood at any parameter value. Coupled with this
          adjustment, we introduce an iterative algorithm that is
          guaranteed to converge. Our simulation indicates that the
          adjusted empirical likelihood is much faster to compute
          than the profile empirical likelihood. The confidence
          regions constructed via the adjusted empirical likelihood
          are found to have coverage probabilities closer to the
          nominal levels without employing complex procedures such as
          Bartlett correction or bootstrap calibration. The method is
          also shown to be effective in solving several practical
          problems associated with the empirical likelihood."
}

@Article{chen:boll:paxt:curr:kirb:2001,
  author    = {F. Chen and Kenneth A. Bollen and P. Paxton and P. Curran
          and J. Kirby},
  year      = {2001},
  title     = {Improper Solutions in Structural Equation Models: Causes,
          Consequences, and Strategies},
  journal   = {Sociological Methods and Research},
  volume    = {29},
  issue     = {4},
  pages     = {468--508}
}

@Article{chen:curran:bollen:kirby:paxton:2008,
  author    = {F. Chen and P. Curran and K. A. Bollen and J. Kirby and P. Paxton},
  year      = {2008},
  title     = {An Empirical Evaluation of the Use of Fixed Cutoff Points in
               {RMSEA} Test Statistic in Structural Equation Models},
  journal   = {Sociological Methods and Research},
  volume    = {36},
  pages     = {462--494}
}

@Article{chen:chen:2001,
  author    = {Chen, Hanfeng and Chen, Jiahua },
  year      = {2001},
  title     = {The Likelihood Ratio Test for Homogeneity in Finite
          Mixture Models},
  doi       = {10.2307/3316073},
  journal   = {The Canadian Journal of Statistics},
  number    = {2},
  pages     = {201--215},
  volume    = {29}
}

@Article{chen:cui:2006,
  author    = {Chen, Song X. and Cui, Hengjian},
  year      = {2006},
  title     = {On {B}artlett correction of empirical likelihood in the
          presence of nuisance parameters},
  doi       = {10.1093/biomet/93.1.215},
  issn      = {0006-3444},
  journal   = {Biometrika},
  number    = {1},
  pages     = {215--220},
  volume    = {93},
  abstract  = {Lazar \& Mykland (1999) showed that an empirical
          likelihood defined by two estimating equations with a
          nuisance parameter need not be Bartlett-correctable. This
          paper shows that Bartlett correction of empirical
          likelihood in the presence of a nuisance parameter depends
          critically on the way the nuisance parameter is removed
          when formulating the likelihood for the parameter of
          interest. We establish in the broad framework of estimating
          functions that the empirical likelihood is still
          Bartlett-correctable if the nuisance parameter is profiled
          out given the value of the parameter of interest.
          10.1093/biomet/93.1.215}
}

@Article{chen:qin:1993,
  author    = {Chen, Jiahua and Qin, Jing },
  year      = {1993},
  title     = {Empirical likelihood estimation for finite populations and
          the effective usage of auxiliary information},
  doi       = {10.1093/biomet/80.1.107},
  journal   = {Biometrika},
  number    = {1},
  pages     = {107--116},
  volume    = {80},
  abstract  = {In finite population inference problems, auxiliary
          population information is often available. We show in this
          paper that the empirical likelihood method can be naturally
          applied to such problems to make effective use of the
          auxiliary information. We prove that the resulting
          estimates have smaller asymptotic variances than the usual
          estimates which do not use auxiliary information. A
          Bahadur-type representation for empirical likelihood sample
          quantiles is given. Simulation results show that the
          empirical likelihood estimates perform well among a number
          of competitors and are model robust.
          10.1093/biomet/80.1.107}
}

@Article{chen:rava:2001,
  author    = "Chen, Shaohua and Ravallion, Martin",
  year      = 2001,
  title     = "How Did the World's Poorest Fare in the 1990s?",
  journal   = "Review of Income and Wealth",
  volume    = {47},
  number    = {3},
  pages     = {283--300}
}

@Article{chen:sitter:wu:2002,
  author    = {Chen, J. and Sitter, R. R. and Wu, C. },
  year      = {2002},
  title     = {Using empirical likelihood methods to obtain range
          restricted weights in regression estimators for surveys},
  doi       = {http://dx.doi.org/10.1093/biomet/89.1.230},
  journal   = {Biometrika},
  number    = {1},
  pages     = {230--237},
  volume    = {89},
  abstract  = {Design weights in surveys are often adjusted to
          accommodate auxiliary information and to meet pre-specified
          range restrictions, typically via some ad hoc algorithmic
          adjustment to a generalised regression estimator.In this
          paper, we present a simple solution to this problem using
          empirical likelihood methods or generalised regression. We
          first develop algorithms for computing empirical likelihood
          estimators and model-calibrated empirical likelihood
          estimators. The first algorithm solves the computational
          problem of the empirical likelihood method in general, both
          in survey and non-survey settings, and theoretically
          guarantees its convergence. The second exploits properties
          of the model-calibration method and is particularly simple.
          The algorithms are adapted for handling benchmark
          constraints and pre-specified range restrictions on the
          weight adjustments. 10.1093/biomet/89.1.230}
}

@Book{chen:srin:2002,
  editor    = "H. Chenery and T.N. Srinivasan",
  year      = 2002,
  title     = "Handbook of Income Distribution",
  publisher = "Elsevier",
  number    = 9,
  series    = "Handbooks in Economics"
}

@Article{chen:vari:abra:2008,
  author    = {Chen, Jiahua and Variyath, Asokan M. and Abraham, Bovas},
  year      = {2008},
  title     = {Adjusted Empirical Likelihood and its Properties},
  comment   = {A very simple yet powerful modification of the empirical
          likelihood idea: if zero is not inside the convex hull,
          let's extend the latter by adding an observation with
          asymptotically vanishing influence:

          \$g\_{n+1} = - a\_n \bar g\_n\$

          for estimating equations \$g\_i = g(x\_i, \theta)\$. The
          first order asymptotics is unchanged provided \$a\_n =
          o\_p(n^{2/3})\$; recommended \$a\_n = \max( 1, \log(n)/2
          )\$ used with trimmed mean.

          The correction makes the confidence regions larger, which
          is good for small sample properties.

          Asymptotic properties: \$\chi^2\$ limit for true
          \$\theta\_0\$; divergence to infinity for points with \$\|
          \Expect g(X,\theta) \| >0\$, the rate is at least
          \$n^{1/3}\$.

          Computing: the standard EL algorithm with inner loop over
          \$\lambda\$, the step size is limited by (iteration
          number)^{-1/2}; simplex method to solve for \$\theta\$ over
          the AEL surface. The algorithms tend to converge faster
          than regular EL; at least they can provide good starting
          values for EL.

          Simulation: for mean estimation problems, AEL better than
          EL with theoretical Bartlett correction on par with EL with
          empirical Bartlett correction better than regular EL.

          Applications: estimation of cov matrix with many known
          zeroes; variable selection by EL version of AIC/BIC:

          \$\$ EBIC = 2 \inf[ EL(\theta) ] + \dim(\theta) \log n \$\$

          },
  doi       = {10.1198/106186008X321068},
  issn      = {1061-8600},
  journal   = {Journal of Computational \& Graphical Statistics},
  number    = {2},
  pages     = {426--443},
  publisher = {American Statistical Association},
  volume    = {17}
}

@article{chen:elliott:etal:2017,
  author = {Qixuan Chen and Michael R. Elliott and David Haziza and Ye Yang
            and Malay Ghosh and Roderick J. A. Little and Joseph Sedransk and Mary Thompson},
  year = 2017,
  title = {Approaches to Improving Survey-Weighted Estimates},
  journal = {Statistical Science},
  volume = 32,
  issue = 2,
  pages = {227--248},
  doi = {10.1214/17-STS609},
  abstract = {
      In sample surveys, the sample units are typically chosen using a
      complex design. This may lead to a selection effect and, if uncorrected in the
      analysis, may lead to biased inferences. To mitigate the effect on inferences
      of deviations from a simple random sample a common technique is to use
      survey weights in the analysis. This article reviews approaches to address
      possible inefficiency in estimation resulting from such weighting.
      To improve inferences we emphasize modifications of the basic designbased
      weight, that is, the inverse of a unit’s inclusion probability. These
      techniques include weight trimming, weight modelling and incorporating
      weights via models for survey variables.We start with an introduction to survey
      weighting, including methods derived from both the design and modelbased
      perspectives. Then we present the rationale and a taxonomy of methods
      for modifying the weights. We next describe an extensive numerical study
      to compare these methods. Using as the criteria relative bias, relative mean
      square error, confidence or credible interval width and coverage probability,
      we compare the alternative methods and summarize our findings. To supplement
      this numerical study we use Texas school data to compare the distributions
      of the weights for several methods.We also make general recommendations,
      describe limitations of our numerical study and make suggestions for
      further investigation.  
  }
}

@Article{cheng:liu:2001,
  author    = {Cheng, R. C. H. and Liu, W. B.},
  year      = {2001},
  title     = {The Consistency of Estimators in Finite Mixture Models},
  abstract  = {The parameters of a finite mixture model cannot be
          consistently estimated when the data come from an embedded
          distribution with fewer components than that being fitted,
          because the distribution is represented by a subset in the
          parameter space, and not by a single point. Feng \&
          McCulloch (1996) give conditions, not easily verified,
          under which the maximum likelihood (ML) estimator will
          converge to an arbitrary point in this subset. We show that
          the conditions can be considerably weakened. Even though
          embedded distributions may not be uniquely represented in
          the parameter space, estimators of quantities of interest,
          like the mean or variance of the distribution, may
          nevertheless actually be consistent in the conventional
          sense. We give an example of some practical interest where
          the ML estimators are root of n-consistent. Similarly
          consistent statistics can usually be found to test for a
          simpler model vs a full model. We suggest a test statistic
          suitable for a general class of model and propose a
          parameter-based bootstrap test, based on this statistic,
          for when the simpler model is correct.},
  doi       = {10.1111/1467-9469.00257},
  journal   = {Scandinavian Journal of Statistics},
  number    = {4},
  pages     = {603--616},
  volume    = {28}
}

@Article{cheng:traylor,
  author    = {Cheng, R. C. H. and Traylor, L.},
  title     = {Non-Regular Maximum Likelihood Problems},
  abstract  = {Four non-regular estimation problems are reviewed and
          discussed. One (the unbounded likelihood problem) involves
          distributions with infinite spikes, for which maximum
          likelihood can fail to give consistent estimators. A
          comparison is made with modified likelihood and spacings
          methods which do give efficient estimators in this case. An
          application to the Box-Cox shifted power transform is
          given. The other three problems occur when the true
          parameter lies in some special subregion. In one (the
          constrained parameter problem) the subregion is a boundary.
          The other two (the embedded model and the indeterminate
          parameters problems) occur when the model takes on a
          special form in the subregion. These last two problems have
          previously been investigated separately. We show that they
          are equivalent in some situations. Both often arise in
          non-linear models and we give a directed graph approach
          which allows for their occurrence in nested model building.
          It is argued that many non-regular problems can be handled
          systematically without having to resort to elaborate
          technical assumptions. Relatively uncomplicated methods may
          be used provided that the underlying nature of the
          non-regularity is understood.}
}

@Book{chernick:1999,
  author    = {Chernick, Michael R.},
  year      = {1999},
  title     = {Bootstrap Methods: A Practitioner's Guide (Wiley Series in
          Probability and Statistics)},
  abstract  = {{A comprehensive, practical treatment for professionals In
          less than two decades, the bootstrap has grown from an
          obscure object of theoretical study to a widely used
          resampling method with broad applications in numerous
          real-world situations. Bootstrap Methods: A Practitioner's
          Guide provides an introduction to the bootstrap for readers
          who have professional interest in these methods but do not
          have a background in advanced mathematics. It offers
          reliable, authoritative coverage of the bootstrap's
          considerable advantages as well as its drawbacks. This book
          updates classic texts in the field by presenting results on
          improved confidence set estimation, estimation of error
          rates in discriminant analysis, and applications to a wide
          variety of hypothesis testing and estimation problems. To
          alert readers to the limitations of the method, it exhibits
          counterexamples to the consistency of bootstrap methods.
          This book also makes connections between more traditional
          resampling methods and bootstrap. Outstanding special
          features of Bootstrap Methods include:<br> * The most
          extensive and detailed bootstrap bibliography available,
          including more than 1,600 references<br> * Discussions
          enlivened with stimulating topics such as data mining<br> *
          Historical notes at the end of each chapter<br> * Examples
          and explanations of when and why bootstrap is not
          effective<br> Bootstrap Methods is a serious, useful, and
          unparalleled practical guide for professionals in
          engineering, the sciences, clinical medicine, and applied
          statistics.}},
  howpublished  = {Hardcover},
  isbn      = {0471349127},
  publisher = {Wiley-Interscience}
}

@Book{chernick:1999,
  author    = {Chernick, Michael R. },
  year      = {1999},
  title     = {Bootstrap Methods: A Practitioner's Guide (Wiley Series in
          Probability and Statistics)},
  howpublished  = {Hardcover},
  publisher = {Wiley-Interscience},
  address   = {New York}
}

@Article{chernoff:1954,
  author    = "Herman Chernoff",
  year      = 1954,
  title     = "On the Distribution of the Likelihood Ratio",
  journal   = "The Annals of Mathematical Statistics",
  volume    = 25,
  number    = 3,
  pages     = "573--578"
}
\
@Article{chintagunta:1992,
  author    = {Chintagunta, Pradeep K.},
  year      = {1992},
  title     = {Estimating a Multinomial Probit Model of Brand Choice
          Using the Method of Simulated Moments},
  abstract  = {The multinomial probit model of brand choice is
          theoretically appealing for marketing applications as it is
          free from the \&quot;independence of irrelevant
          alternatives\&quot; property of the multinomial logit
          model. However, difficulties in estimation have restricted
          its widespread use in marketing. This paper presents an
          application of the method of simulated moments, a new
          methodology that enables easy estimation of probit models
          with a large number of alternatives in the choice set. We
          describe the theoretical development of the technique and
          using pseudo-simulated data, conduct numerical experiments
          to compare the method with existing techniques for
          estimating probit models. Using the scanner panel data on
          the purchases of catsup, we provide an empirical
          application of the method of simulated moments to the
          estimation of the parameters of a multinomial probit model.
          Estimating the covariance structure associated with the
          underlying latent variable probit model enables us to
          identify broad patterns of similarities across
          alternatives. It also enables us to derive a pairwise
          similarity matrix across choice alternatives which when
          input into a multidimensional scaling routine provides us
          with a graphical representation of competitive structure in
          the catsup market. For completeness, we compare the
          substantive implications for the effects of marketing
          variables obtained from the multinomial probit model with
          those obtained from models in the extant marketing
          literature.},
  journal   = {Marketing Science},
  number    = {4},
  pages     = {386--407},
  volume    = {11}
}

@Article{chipman:george:mccull:1998,
  author    = {Hugh A. Chipman and Edward I. George and Robert E.
          McCulloch},
  year      = {1998},
  title     = {{Bayesian} {CART} Model Search},
  journal   = {Journal of the American Statistical Association},
  volume    = {93},
  number    = {443},
  pages     = {935--948}
}

@ARTICLE{chipperfield:steel:2009,
  author =       {Chipperfield, J.O. and Steel, D.G.},
  title =        {Design and Estimation for Split Questionnaire Surveys},
  journal =      {Journal of Official Statistics},
  year =         {2009},
  volume =       {25},
  pages =        {227--244}
}


@article{citro:2014,
  title={From multiple modes for surveys to multiple data sources for estimates},
  author={Citro, Constance F},
  journal={Survey Methodology},
  volume={40},
  number={2},
  pages={137--162},
  year={2014},
  publisher={Statistics Canada}
}

@ARTICLE{cho:huff:eltinge:gersh:2014,
  AUTHOR =       {Moon{J}ung Cho and John L. Eltinge and Julie Gershunskaya and Larry L. Huff},
  TITLE =        {Evaluation of Generalized Variance Functions in the Analysis of Complex Survey Data},
  JOURNAL =      {Journal of Official Statistics},
  YEAR =         {2014},
  volume =       {30},
  number =       {1},
  pages =        {63--90},
}

@TechReport{eltinge:gersh:huff:2002,
  author    = {John L. Eltinge and Julie B. Gershunskaya and Larry L. Huff},
  year      = {2002},
  title     = {Exploratory Analysis of Generalized Variance Function
          Models for the {U.S. Current Employment Survey}},
  institution   = {BLS},
  type      = {technical report},
  number    = {020120},
  note      = {http://www.bls.gov/ore/abstract/st/st020120.htm}
}



@Article{choi:2002,
  author    = "In Choi",
  year      = 2002,
  title     = "Structural Changes and Seemingly Unidentified Structural
          Equations",
  journal   = "Econometric Theory",
  volume    = 18,
  issue     = 3,
  pages     = "744-775"
}

@Article{chou:bentler:satorra:1991,
  author    = {Chou, C.P. and Bentler, P.M. and Satorra, A.},
  year      = {1991},
  title     = {Scaled test statistics and robust standard errors for
          non-normal data in covariance structure analysis: a {Monte
          Carlo} study},
  journal   = {British Journal of Mathematical and Statistial
          Psychology},
  volume    = {44},
  number    = {2},
  pages     = {347--357},
  abstract  = {Research studying robustness of maximum likelihood (ML)
          statistics in covariance structure analysis has concluded
          that test statistics and standard errors are biased under
          severe non-normality. An estimation procedure known as
          asymptotic distribution free (ADF), making no
          distributional assumption, has been suggested to avoid
          these biases. Corrections to the normal theory statistics
          to yield more adequate performance have also been proposed.
          This study compares the performance of a scaled test
          statistic and robust standard errors for two models under
          several non-normal conditions and also compares these with
          the results from ML and ADF methods. Both ML and ADF test
          statistics performed rather well in one model and
          considerably worse in the other. In general, the scaled
          test statistic seemed to behave better than the ML test
          statistic and the ADF statistic performed the worst. The
          robust and ADF standard errors yielded more appropriate
          estimates of sampling variability than the ML standard
          errors, which were usually downward biased, in both models
          under most of the non-normal conditions. ML test statistics
          and standard errors were found to be quite robust to the
          violation of the normality assumption when data had either
          symmetric and platykurtic distributions, or non-symmetric
          and zero kurtotic distributions.}
}

@article{choudhry:rao:hidiroglou:2012,
    author = {Choudhry, G. Hussain and Rao, J. N. K. and Hidiroglou, Michael A.},
    journal = {Survey Methodology},
    number = {1},
    pages = {23--29},
    title = {On sample allocation for efficient domain estimation},
    volume = {38},
    year = {2012}
}

@Book{chri:1992,
  author    = "George Christakos",
  year      = 1992,
  title     = "Random field models in earth sciences",
  publisher = "Academic Press",
  address   = "New York, London"
}

@Book{chri:2000,
  author    = "George Christakos",
  year      = 2000,
  title     = "Modern Spatiotemporal Statistics",
  publisher = "Oxford University Press",
  address   = "New York"
}

@article{chun:shapiro:2009,
  author={Chun, So Yeon and Alexander Shapiro},
  title={Normal versus Noncentral Chi-square Asymptotics of Misspecified Models},
  journal={Multivariate Behavioral Research},
  volume={44},
  number={6},
  pages={803--827},
  year={2009},
  abstract ={
    The noncentral chi-square approximation of the distribution of
    the likelihood ratio (LR) test statistic is a critical part of
    the methodology in structural equation modeling. Recently, it
    was argued by some authors that in certain situations normal
    distributions may give a better approximation of the
    distribution of the LR test statistic. The main goal of this
    article is to evaluate the validity of employing these
    distributions in practice. Monte Carlo simulation results
    indicate that the noncentral chi-square distribution describes
    behavior of the LR test statistic well under small, moderate,
    and even severe misspecifications regardless of the sample size
    (as long as it is sufficiently large), whereas the normal
    distribution, with a bias correction, gives a slightly better
    approximation for extremely severe misspecifications. However,
    neither the noncentral chi-square distribution nor the
    theoretical normal distributions give a reasonable approximation
    of the LR test statistics under extremely severe
    misspecifications. Of course, extremely misspecified models are
    not of much practical interest. We also use the Thurstone data
    (Thurstone & Thurstone, 1941) from a classic study of mental
    ability for our illustration. }
}

@Book{cochran:1977,
  author    = "William G. Cochran",
  year      = 1977,
  title     = "Sampling Techniques",
  publisher = "John Wiley and Sons",
  address   = "New York",
  edition   = "3rd"
}

@Book{cohen:cohen:west:aiken:2002,
  author    = "Patricia Cohen and Jacob Cohen and Stephen G. West and
          Leona S. Aiken",
  year      = 2002,
  title     = "Applied Multiple Regression/Correlation Analysis for the
          Behavioral Sciences",
  publisher = "Lea",
  edition   = "3rd"
}

@article{cohen:nagin:walls:wasser:1998,
    author = { Jacqueline   Cohen  and  Daniel   Nagin  and  Garrick   Wallstrom  and  Larry   Wasserman },
    title = {Hierarchical {Bayesian} Analysis of Arrest Rates},
    journal = {Journal of the American Statistical Association},
    volume = {93},
    number = {444},
    pages = {1260-1270},
    year = {1998},
    doi = {10.1080/01621459.1998.10473787},
    abstract =
        { Abstract A Bayesian hierarchical model provides the basis for
        calibrating the crimes avoided by incarceration of individuals
        convicted of drug offenses compared to those convicted of nondrug
        offenses. Two methods for constructing reference priors for
        hierarchical models both lead to the same prior in the final model.
        We use Markov chain Monte Carlo methods to fit the model to data from
        a random sample of past arrest records of all felons convicted of
        drug trafficking, drug possession, robbery, or burglary in Los
        Angeles County in 1986 and 1990. The value of this formal analysis,
        as opposed to a simpler analysis that does not use the formal
        machinery of a Bayesian hierarchical model, is to provide interval
        estimates that account for the uncertainty due to the random effects.
        }
}



@Book{conover1998practical,
  author    = {Conover, W. J.},
  year      = {1998},
  title     = {Practical Nonparametric Statistics},
  abstract  = {{This highly-regarded text serves as a quick reference
          book which offers clear, concise instructions on how and
          when to use the most popular nonparametric procedures. This
          edition features some procedures that have withstood the
          test of time and are now used by many practitioners, such
          as the Fisher Exact Test for two-by-two contingency tables,
          the Mantel-Haenszel Test for combining several contingency
          tables, the Kaplan-Meier estimates of the survival curve,
          the Jonckheere-Terpstra Test and the Page Test for ordered
          alternatives, and a discussion of the bootstrap method.}},
  howpublished  = {Hardcover},
  isbn      = {0471160687},
  publisher = {{John Wiley \& Sons}}
}

@Book{conover:1998,
  author    = "W.J. Conover",
  year      = 1998,
  title     = "Practical nonparametric statistics",
  publisher = "John Wiley and Sons",
  series    = "Wiley Series in Probability and Statistics",
  address   = "New York"
}

@Article{coulter:cowell:jenkins:1992,
  author    = {Coulter, Fiona A. E. and Cowell, Frank A. and Jenkins,
          Stephen P.},
  year      = {1992},
  title     = {Equivalence Scale Relativities and the Extent of
          Inequality and Poverty},
  doi       = {10.2307/2234376},
  issn      = {00130133},
  journal   = {The Economic Journal},
  number    = {414},
  pages     = {1067--1082},
  publisher = {Blackwell Publishing for the Royal Economic Society},
  volume    = {102}
}

@Article{cowell1989sampling,
  author    = {Cowell, Frank A.},
  year      = {1989},
  title     = {Sampling variance and decomposable inequality measures},
  abstract  = {The purpose of this paper is twofold: (1) to draw together
          and interpret results on sampling variance of this
          important class of inequality measures and (2) to
          illustrate the way in which the bivariate nature of the
          underlying problem affects the estimates of inequality
          measures and their sample variances.},
  doi       = {10.1016/0304-4076(89)90073-0},
  journal   = {Journal of Econometrics},
  number    = {1},
  pages     = {27--41},
  volume    = {42}
}

@InCollection{cowell:merc:1999,
  author    = "Cowell, Frank A. and Mercader-Prats, M.",
  editor    = "Silber, J. ",
  year      = "1999",
  title     = "Equivalence Scales and Inequality",
  booktitle = "Income Inequality Measurement: From Theory to Practice",
  publisher = "Kluwer"
}

@Article{cowell:vict-feser:1996,
  author    = "Frank A. Cowell and Maria-Pia Victoria-Feser",
  year      = "1996",
  title     = "Poverty Measurement with Contaminated Data: {A} Robust
          Approach",
  journal   = "European Economic Review",
  volume    = "40",
  pages     = "1761--1771"
}

@Book{cox1994inference,
  author    = {Cox, D. R. and Barndorff-Nielsen, O. E.},
  year      = {1994},
  title     = {Inference and Asymptotics (Monographs on Statistics and
          Applied Probability)},
  howpublished  = {Hardcover},
  isbn      = {041249440X},
  publisher = {{Chapman \& Hall/CRC}}
}

@Article{cox:2000,
  author    = "Cox, Lawrence H.",
  year      = 2000,
  title     = "Statistical issues in the study of air pollution involving
          airborne particulate matter",
  journal   = "Environmetrics",
  volume    = 11,
  pages     = "611--626"
}

@Book{cox:barn:niel:1994,
  author    = {Barndorff-Nielsen, O. E. and Cox, D. R.},
  year      = {1994},
  title     = {Inference and Asymptotics},
  isbn      = {041249440X},
  publisher = {Chapman \& Hall/CRC},
  series    = {Monographs on Statistics and Applied Probability},
  address   = {New York}
}

@ARTICLE{njcox:2010,
  AUTHOR =       {Nicholas J. Cox},
  TITLE =        {Speaking Stata: The limits of sample skewness and kurtosis},
  JOURNAL =      {The Stata Journal},
  YEAR =         {2010},
  volume =       {10},
  number =       {3},
  pages =        {482--495},
}


@TechReport{cps:2002,
  author    = "{U.S. Census Bureau}",
  year      = "2002",
  title     = "Current Population Survey: Design and Methodology",
  institution   = "U.S. Census Bureau",
  type      = "Technical Paper",
  number    = "63RV",
  address   = "Washington, DC",
  note      = "http://www.census.gov/prod/2002pubs/tp63rv.pdf"
}

@Manual{cran:sfsmisc:2006,
  author    = {Martin Maechler},
  year      = {2006},
  title     = {The \texttt{sfsmisc} package},
  organization  = {R Comprehensive Network Archive (CRAN)},
  note      = {http://cran.hu.r-project.org/doc/packages/sfsmisc.pdf}
}

@Manual{cran:survey:2007,
  author    = {Thomas Lumley},
  year      = {2007},
  title     = {The \texttt{survey} package},
  organization  = {R Comprehensive Network Archive (CRAN)},
  note      = {http://cran.r-project.org/doc/packages/survey.pdf}
}

@Article{cranley:patterson:1976,
  author    = {Cranley, R. and Patterson, T. N. L.},
  year      = {1976},
  title     = {Randomization of Number Theoretic Methods for Multiple
          Integration},
  abstract  = {A procedure is discussed for randomization of the number
          theoretic methods of the Korobov type producing stochastic
          families of multi-dimensional integration rules. These
          randomized rules have the advantage that confidence
          intervals can be given for the magnitude of error. The
          practical implementation is considered.},
  journal   = {SIAM Journal on Numerical Analysis},
  number    = {6},
  pages     = {904--914},
  volume    = {13}
}

@Article{crawford:hanfelt:2008,
  author    = {Crawford, S B B. and Hanfelt, J J J.},
  year      = {2008},
  title     = {Testing for the presence of multiple sources of
          informative dropout in longitudinal data.},
  abstract  = {Longitudinal studies tracking the rate of change are
          subject to patient dropout. This dropout process might not
          only be informative but also heterogeneous in the sense
          that different causes might contribute to multiple patterns
          of informative dropout. We propose a random-effects
          approach to test for homogeneity of informative dropout
          that accommodates the realistic situation where reasons for
          dropout are not fully understood, or perhaps are even
          entirely unknown. The proposed score test is robust in that
          it does not depend on the underlying distribution of the
          informative dropout random effects. The test allows for an
          additional level of clustering among participating
          subjects, as might be found in a family study, provided the
          informative dropout random effects have a known correlation
          structure. Copyright (c) 2008 John Wiley \& Sons, Ltd.},
  address   = {Department of Biostatistics, Rollins School of Public
          Health, Emory University, Atlanta, GA 30322, U.S.A.},
  doi       = {10.1002/sim.3287},
  issn      = {0277-6715},
  journal   = {Statistics in medicine}
}

@Book{cressie:1993,
  author    = "N. Cressie",
  year      = 1993,
  title     = "Statistics for Spatial Data",
  publisher = "Wiley",
  address   = "New York",
  edition   = "2nd"
}

@Article{cressie:huang:1999,
  author    = "Noel Cressie and Hsin-Cheng Huang",
  year      = 1999,
  title     = "Classes of Nonseparable, Spatio-Temporal Stationary
          Covariance Functions",
  journal   = "The Journal of American Statistical Association",
  volume    = 94,
  pages     = "1330--1340"
}

@Article{cressie:read:1984,
  author    = {Cressie, N. and Read, T.,},
  year      = {1984},
  title     = {Multinomial goodness-of-fit tests},
  journal   = {Journal of the Royal Statistical Society Series B},
  volume    = {46},
  pages     = {440--464}
}

@inproceedings{crouse:kott:2004,
    author = {Crouse, Chadd and Kott, Phillip S.},
    organization = {Survey Research Methodology Section},
    pages = {1509--1515},
    publisher = {American Statistical Association},
    title = {Evaluating Alternative Calibration Schemes for an Economic Survey with Large Nonresponse},
    year = {2004},
    booktitle = {Proceedings of the American Statistical Association}
}

@Article{cudeck:browne:1992,
  author    = {Cudeck, Robert and Browne, Michael},
  year      = {1992},
  title     = {Constructing a covariance matrix that yields a specified
          minimizer and a specified minimum discrepancy function
          value},
  abstract  = {A method is presented for
          constructing a covariance matrix ?*0 that is the sum of a
          matrix ?(?0) that satisfies a specified model and a
          perturbation matrix,E, such that ?*0=?(?0) +E. The
          perturbation matrix is chosen in such a manner that a class
          of discrepancy functionsF(?*0, ?(?0)), which includes
          normal theory maximum likelihood as a special case, has the
          prespecified parameter value ?0 as minimizer and a
          prespecified minimum ? A matrix constructed in this way
          seems particularly valuable for Monte Carlo experiments as
          the covariance matrix for a population in which the model
          does not hold exactly. This may be a more realistic
          conceptualization in many instances. An example is
          presented in which this procedure is employed to generate a
          covariance matrix among nonnormal, ordered categorical
          variables which is then used to study the performance of a
          factor analysis estimator.},
  doi       = {10.1007/BF02295424},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {357--369},
  volume    = {57}
}

@Book{cudeck:dutoit:sorbom:2000,
  editor    = {R. Cudeck and S. {du Toit} and D. Sorbom},
  year      = {2000},
  title     = {Structural Equation Modeling: Present and Future},
  isbn      = {0894980491},
  publisher = {Scientific Software International, Inc.}
}

@Article{cunningham:1979,
  author    = {Michael R. Cunningham},
  year      = {1979},
  title     = {Weather, Mood, and Helping Behavior: Quasi-Experiments
          with the Sunshine Samaritan},
  journal   = {Journal of Personality and Social Psychology},
  volume    = {37},
  number    = {11},
  pages     = {1947--1956}
}

@ARTICLE{curran:west:finch:1996,
  AUTHOR =       {Patrick J. Curran and Stephen G. West and John F. Finch},
  TITLE =        {The robustness of test statistics to nonnormality and
                  specification error in confirmatory factor analysis},
  JOURNAL =      {Psychological Methods},
  YEAR =         {1996},
  volume =       {1},
  number =       {1},
  pages =        {16--29},
}

@ARTICLE{curran:bollen:paxton:kirby:chen:2002,
  AUTHOR =       {Patrick J. Curran and Kenneth A. Bollen and Pamela Paxton and
                  James Kirby and Feinian Chen},
  TITLE =        {The Noncentral Chi-square Distribution in Misspecified
                  Structural Equation Models: Finite Sample Results
                  from a {Monte} {Carlo} Simulation},
  JOURNAL =      {Multivariate Behavioral Research},
  YEAR =         {2002},
  volume =       {37},
  number =       {1},
  pages =        {1--36},
  abstract =     {
                  The noncentral chi-square distribution plays a key
                  role in structural equation modeling
                  (SEM).  The likelihood ratio test statistic that
                  accompanies virtually all SEMs
                  asymptotically follows a noncentral chi-square
                  under certain assumptions relating to
                  misspecification and multivariate distribution.
                  Many scholars use the noncentral chi-square
                  distribution in the construction of fit indices,
                  such as Steiger and Lind?s (1980) Root Mean
                  Square Error of Approximation (RMSEA) or the
                  family of baseline fit indices (e.g., RNI,
                  CFI), and for the computation of statistical power
                  for model hypothesis testing.  Despite this
                  wide use, surprisingly little is known about the
                  extent to which the test statistic follows a
                  noncentral chi-square in applied research.  Our
                  study examines several hypotheses about the
                  suitability of the noncentral chi-square
                  distribution for the usual SEM test statistic
                  under                  conditions commonly
                  encountered in practice.  We designed Monte Carlo
                  computer                  simulation experiments
                  to empirically test these research hypotheses.
                  Our experimental                  conditions
                  included seven sample sizes ranging from 50 to
                  1000, and three distinct model
                  types, each with five specifications ranging from
                  a correct model to the severely misspecified
                  uncorrelated baseline model.  In general, we found
                  that for models with small to moderate
                  misspecification, the noncentral chi-square
                  distribution is well approximated when the
                  sample size is large (e.g., greater than 200), but
                  there was evidence of bias in both mean and
                  variance in smaller samples.  A key finding was
                  that the test statistics for the uncorrelated
                  variable baseline model did not follow the
                  noncentral chi-square distribution for any model
                  type across any sample size.  We discuss the
                  implications of our findings for the SEM fit
                  indices and power estimation procedures that are
                  based on the noncentral chi-square
                  distribution as well as potential directions for
                  future research.
                  },
}



@Article{curran:2003,
  author    = {Curran, Patrick J.},
  year      = {2003},
  title     = {Have Multilevel Models Been Structural Equation Models All
          Along?},
  abstract  = {A core assumption of the standard multiple regression
          model is independence of residuals, the violation of which
          results in biased standard errors and test statistics. The
          structural equation model (SEM) generalizes the regression
          model in several key ways, but the SEM also assumes
          independence of residuals. The multilevel model (MLM) was
          developed to extend the regression model to dependent data
          structures. Attempts have been made to extend the SEM in
          similar ways, but several complications currently limit the
          general application of these techniques in practice.
          Interestingly, it is well known that under a broad set of
          conditions SEM and MLM longitudinal ?growth curve? models
          are analytically and empirically identical. This is
          intriguing given the clear violation of independence in
          growth modeling that does not detrimentally affect the
          standard SEM. Better understanding the source and potential
          implications of this isomorphism is my focus here. I begin
          by exploring why SEM and MLM are analytically equivalent
          methods in the presence of nesting due to repeated
          observations over time. I then capitalize on this
          equivalency to allow for the extension of SEMs to a general
          class of nested data structures. I conclude with a
          description of potential opportunities for multilevel SEMs
          and directions for future developments.},
  doi       = {10.1207/s15327906mbr3804\_5},
  journal   = {Multivariate Behavioral Research},
  number    = {4},
  pages     = {529--569},
  publisher = {Psychology Press},
  volume    = {38}
}

@Article{czaj:hira:litt:rubin:1992,
  author    = {Czajka, John L. and Hirabayashi, Sharon M. and Little,
          Roderick J. A. and Rubin, Donald B.},
  year      = {1992},
  title     = {Projecting from Advance Data Using Propensity Modeling: An
          Application to Income and Tax Statistics},
  doi       = {10.2307/1391671},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {2},
  pages     = {117--131},
  volume    = {10},
  abstract  = {This article proposes and evaluates two new methods of
          reweighting preliminary data to obtain estimates more
          closely approximating those derived from the final data
          set. In our motivating example, the preliminary data are an
          early sample of tax returns, and the final data set is the
          sample after all tax returns have been processed. The new
          methods estimate a predicted propensity for late filing for
          each return in the advance sample and then poststratify
          based on these propensity scores. Using advance and
          complete sample data for 1982, we demonstrate that the new
          methods produce advance estimates generally much closer to
          the final estimates than those derived from the current
          advance estimation techniques. The results demonstrate the
          value of propensity modeling, a general-purpose methodology
          that can be applied to a wide range of problems, including
          adjustment for unit nonresponse and frame undercoverage as
          well as statistical matching.}
}


@article{dagdoug:goga:haziza:2023,
  authors = {Mehdi Dagdoug and Camelia Goga and David Haziza},
  title = {Imputation Procedures in Surveys Using Nonparametric and 
  Machine Learning Methods: An Empirical Comparison},
  journal = {Journal of Survey Statistics and Methodology}, 
  volume = 11, 
  issue = 1, 
  year = 2023, 
  pages = {141–-188}, 
  doi = {10.1093/jssam/smab004}
}

@InCollection{dalenius:1994,
  author    = {Dalenius, T.},
  editor    = {P. R. Krishnaiah and C. R. Rao},
  year      = {1994},
  title     = {A First Course in Survey Sampling},
  booktitle = {Handbook of Statistics: Sampling},
  publisher = {Elsevier: North Holland},
  volume    = {6},
  chapter   = {2}
}

@article{darrigo:skinner:2010,
    author = {{D'A}rrigo, Julia and Skinner, Chris},
    journal = {Survey Methodology},
    number = {2},
    pages = {181--192},
    title = {Linearization variance estimation for generalized raking estimators in the presence of nonresponse},
    volume = {36},
    year = {2010}
}

@Article{daniels:hogan:2000,
  author    = {Daniels, Michael J. and Hogan, Joseph W.},
  year      = {2000},
  title     = {Reparameterizing the Pattern Mixture Model for Sensitivity
          Analyses under Informative Dropout},
  abstract  = {Pattern mixture models are frequently used to analyze
          longitudinal data where missingness is induced by dropout.
          For measured responses, it is typical to model the complete
          data as a mixture of multivariate normal distributions,
          where mixing is done over the dropout distribution. Fully
          parameterized pattern mixture models are not identified by
          incomplete data; Little (1993, Journal of the American
          Statistical Association 88, 125-134) has characterized
          several identifying restrictions that can be used for model
          fitting. We propose a reparameterization of the pattern
          mixture model that allows investigation of sensitivity to
          assumptions about nonidentified parameters in both the mean
          and variance, allows consideration of a wide range of
          nonignorable missing-data mechanisms, and has intuitive
          appeal for eliciting plausible missing-data mechanisms. The
          parameterization makes clear an advantage of pattern
          mixture models over parametric selection models, namely
          that the missing-data mechanism can be varied without
          affecting the marginal distribution of the observed data.
          To illustrate the utility of the new parameterization, we
          analyze data from a recent clinical trial of growth hormone
          for maintaining muscle strength in the elderly. Dropout
          occurs at a high rate and is potentially informative. We
          undertake a detailed sensitivity analysis to understand the
          impact of missing-data assumptions on the inference about
          the effects of growth hormone on muscle strength.},
  doi       = {10.2307/2677064},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {4},
  pages     = {1241--1248},
  publisher = {International Biometric Society},
  volume    = {56}
}

@TechReport{datt:rava:1990,
  author    = "Datt, G. and Ravallion, M.",
  year      = 1990,
  title     = "Regional Disparities, Targeting, And Poverty In {I}ndia",
  institution   = "World Bank --- Country Economics Department",
  type      = "Paper",
  number    = {375}
}

@INCOLLECTION{datta:2009,
  AUTHOR =       {G. S. Datta},
  TITLE =        {Model-based approach to small area estimation},
  BOOKTITLE =    {Sample Surveys: Inference and Analysis},
  PUBLISHER =    {North Holland, Amsterdam},
  YEAR =         {2009},
  editor =       {D. Pfeffermann and C. R. Rao},
  volume =       {29B},
  series =       {Handbook of Statistics},
  pages =        {251--288},
}

@Book{dattoro:2006,
  author    = {Dattorro, Jon },
  year      = {2006},
  title     = {Convex Optimization \& Euclidean Distance Geometry},
  isbn      = {1847280641},
  publisher = {Meboo Publishing},
  address   = {Palo Alto, CA},
  abstract  = {Optimization is the science of making a best choice in the
          face of conflicting requirements. Any convex optimization
          problem has geometric interpretation. If a given
          optimization problem can be transformed to a convex
          equivalent, then this interpretive benefit is acquired.
          That is a powerful attraction: the ability to visualize
          geometry of an optimization problem. Conversely, recent
          advances in geometry hold convex optimization within their
          proofs' core. This book is about convex optimization,
          convex geometry (with particular attention to distance
          geometry), geometrical problems, and problems that can be
          transformed into geometrical problems. Euclidean distance
          geometry is, fundamentally, a determination of point
          conformation from interpoint distance information; e.g.,
          given only distance information, determine whether there
          corresponds a realizable configuration of points; a list of
          points in some dimension that attains the given interpoint
          distances. large black \& white paperback}
}

@Article{davdison:mackinnon:1987,
  author    = {Davidson, Russell and Mackinnon, James G.},
  year      = {1987},
  title     = {Implicit Alternatives and the Local Power of Test
          Statistics},
  abstract  = {The local power of test statistics is analyzed by
          extending the notion of Pitman sequences to sequences of
          data-generating process (DGP's) that approach the null
          hypothesis without necessarily satisfying the alternative.
          A space of probability densities is defined and endowed
          with the structure of an infinite-dimensional Hilbert
          manifold, which permits a geometrical interpretation of
          hypothesis testing. The three classical test
          statistics--LR, Wald, and LM--are shown to tend
          asymptotically to the same random variable under all
          sequences of local DGP's. The power of these statistics is
          seen to depend on the null, the alternative, and the
          sequence of DGP's in a simple and geometrically intuitive
          way. Moreover, for any test statistic that is
          asymptotically chi-squared under the null, there exists an
          "implicit alternative hypothesis" against which that
          statistic will have highest power, and which coincides with
          the explicit alternative for the classical test
          statistics.},
  doi       = {10.2307/1913558},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1305--1329},
  publisher = {The Econometric Society},
  volume    = {55}
}

@Article{davi:hink:sche:1986,
  author    = {Davison, A. C. and Hinkley, D. V. and Schechtman, E. },
  year      = {1986},
  title     = {Efficient bootstrap simulation},
  comment   = {10.1093/biomet/73.3.555},
  journal   = {Biometrika},
  number    = {3},
  pages     = {555--566},
  volume    = {73},
  abstract  = {Bootstrap methods are simulation methods for assessing
          sampling properties of statistical estimates. We discuss
          two ideas for making the simulation more efficient. The
          first idea is to balance the simulated samples, and the
          second idea is to make explicit use approximations which do
          not require simulation. Both ideas are illustrated with
          three examples. 10.1093/biomet/73.3.555}
}

@Article{davidson:duclos:2000,
  author    = {Davidson, Russell and Duclos, Jean-Yves},
  year      = {2000},
  title     = {Statistical Inference for Stochastic Dominance and for the
          Measurement of Poverty and Inequality},
  abstract  = {We derive the asymptotic sampling distribution of various
          estimators frequently used to order distributions in terms
          of poverty, welfare, and inequality. This includes
          estimators of most of the poverty indices currently in use,
          as well as estimators of the curves used to infer
          stochastic dominance of any order. These curves can be used
          to determine whether poverty, inequality, or social welfare
          is greater in one distribution than in another for general
          classes of indices and for ranges of possible poverty
          lines. We also derive the sampling distribution of the
          maximal poverty lines up to which we may confidently assert
          that poverty is greater in one distribution than in
          another. The sampling distribution of convenient dual
          estimators for the measurement of poverty is also
          established. The statistical results are established for
          deterministic or stochastic poverty lines as well as for
          paired or independent samples of incomes. Our results are
          briefly illustrated using data for four countries drawn
          from the Luxembourg Income Study data bases.},
  address   = {Dept. of Economics, Queens University, Kingston, Canada,;
          Dpt. d'Economie, Pavillon de Seve, Universit Laval},
  doi       = {10.1111/1468-0262.00167},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1435--1464},
  volume    = {68}
}

@TechReport{davidson:duclos:2006,
  author    = {Russell Davidson and Jean-Yves Duclos},
  year      = 2006,
  title     = {Testing for Restricted Stochastic Dominance},
  institution   = {CIRPEE},
  type      = {Working paper},
  number    = {0609},
  note      = {RePEc:lvl:lacicr:0609}
}

@Book{davidson:mackinnon:1992,
  author    = {Davidson, Russell and Mackinnon, James G.},
  year      = {1992},
  title     = {Estimation and Inference in Econometrics},
  abstract  = {Offering students a unifying theoretical perspective, this
          innovative text emphasizes nonlinear techniques of
          estimation, including nonlinear least squares, nonlinear
          instrumental variables, maximum likelihood and the
          generalized method of moments, but nevertheless relies
          heavily on simple geometrical arguments to develop
          intuition. One theme of the book is the use of artificial
          regressions for estimation, inference, and specification
          testing of nonlinear models, including diagnostic tests for
          parameter constancy, series correlation, heteroskedasticity
          and other types of misspecification. Other topics include
          the linear simultaneous equations model, non-nested
          hypothesis tests, influential observations and leverage,
          transformations of the dependent variable, binary response
          models, models for time-series/cross-section data,
          multivariate models, seasonality, unit roots and
          cointegration, and Monte Carlo methods, always with an
          emphasis on problems that arise in applied work. Explaining
          throughout how estimates can be obtained and tests can be
          carried out, the text goes beyond a mere algebraic
          description to one that can be easily translated into the
          commands of a standard econometric software package. A
          comprehensive and coherent guide to the most vital topics
          in econometrics today, this text is indispensable for all
          levels of students of econometrics, economics, and
          statistics on regression and related topics.},
  howpublished  = {Hardcover},
  isbn      = {0195060113},
  publisher = {Oxford University Press, USA}
}

@Book{davidson:mackinnon:eie:1993,
  author    = {Russell Davidson and James {MacKinnon}},
  year      = {1993},
  title     = {Estimation and Inference in Econometrics},
  publisher = {Oxford University Press},
  address   = {New York},
  isbn      = {0195060113}
}


@ARTICLE{davidson:beck:milligan:2009,
  AUTHOR =       {Davidson, W. B. and Beck, H. P. and Milligan, M.},
  TITLE =        {The college persistence questionnaire:
                  development and validation of an instrument that predicts student attrition},
  JOURNAL =      {Journal of College Student Development},
  YEAR =         {2009},
  volume =       {50},
  number =       {4},
  pages =        {373--390},
  source =       {Isabella Zaniletti},
}



@Article{davies:1973,
  author    = {Davies, R. B.},
  year      = {1973},
  title     = {Numerical Inversion of a Characteristic Function},
  abstract  = {A method is described for finding a bound on the error
          when a version of the usual characteristic function
          inversion formula is evaluated by numerical integration.
          The method is applied to the calculation of the
          distribution function of a quadratic form in normal random
          variables.},
  doi       = {10.2307/2334555},
  issn      = {00063444},
  journal   = {Biometrika},
  number    = {2},
  pages     = {415--417},
  publisher = {Biometrika Trust},
  volume    = {60}
}

@Article{davies:1977,
  author    = {Davies, R. B. },
  year      = {1977},
  title     = {Hypothesis Testing When a Nuisance Parameter is Present
          Only Under the Alternative},
  doi       = {10.2307/2335690},
  journal   = {Biometrika},
  number    = {2},
  pages     = {247--254},
  volume    = {64},
  abstract  = {Suppose that the distribution of a random variable
          representing the outcome of an experiment depends on two
          parameters ? and ? and that we wish to test the
          hypothesis ? = 0 against the alternative \$\xi > 0\$ . If
          the distribution does not depend on ? when ? = 0,
          standard asymptotic methods such as likelihood ratio
          testing or C(?) testing are not directly applicable.
          However, these methods may, under appropriate conditions,
          be used to reduce the problem to one involving inference
          from a Gaussian process. This simplified problem is
          examined and a test which may be derived as a likelihood
          ratio test or from the union-intersection principle is
          introduced. Approximate expressions for the significance
          level and power are obtained.}
}

@Article{davies:1987,
  author    = {Davies, Robert B. },
  year      = {1987},
  title     = {Hypothesis Testing when a Nuisance Parameter is Present
          Only Under the Alternatives},
  doi       = {10.2307/2336019},
  journal   = {Biometrika},
  number    = {1},
  pages     = {33--43},
  volume    = {74},
  abstract  = {We wish to test a simple hypothesis against a family of
          alternatives indexed by a one-dimensional parameter, ?. We
          use a test derived from the corresponding family of test
          statistics appropriate for the case when ? is given.
          Davies (1977) introduced this problem when these test
          statistics had normal distributions. The present paper
          considers the case when their distribution is chi-squared.
          The results are applied to the detection of a discrete
          frequency component of unknown frequency in a time series.
          In addition quick methods for finding approximate
          significance probabilities are given for both the normal
          and chi-squared cases and applied to the two-phase
          regression problem in the normal case.}
}

@Article{davies:2002,
  author    = {Davies, Robert B. },
  year      = {2002},
  title     = {Hypothesis testing when a nuisance parameter is present
          only under the alternative: Linear model case},
  doi       = {10.1093/biomet/89.2.484},
  journal   = {Biometrika},
  number    = {2},
  pages     = {484--489},
  volume    = {89},
  abstract  = {The results of Davies (1977, 1987) are extended to a
          linear model situation with unknown residual variance.
          10.1093/biomet/89.2.484}
}

@Article{davis:1977,
  author    = {Davis, A. W.},
  year      = {1977},
  title     = {Asymptotic theoty for principal component analysis:
          Nonnormal case},
  journal   = {Australian Journal of Statistics},
  pages     = {206--212},
  volume    = {19}
}

@article{davis:1975,
     title = {Mean Square Error Properties of Density Estimates},
     author = {Davis, Kathryn Bullock},
     journal = {The Annals of Statistics},
     volume = {3},
     number = {4},
     pages = {1025--1030},
     abstract =
        {The rate at which the mean square error decreases as sample
        size increases is evaluated for general L1 kernel estimates
        and for the Fourier integral estimate for a probability
        density function. The estimates are then compared on the
        basis of these rates.},
     year = {1975},
}


@Book{davison:hinkley:1997,
  author    = {A. C. Davison and D. V. Hinkley},
  year      = {1997},
  title     = {Bootstrap methods and their application},
  publisher = {Cambridge University Press},
  series    = {Cambridge Series in Statistical and Probabilistic
          Mathematics},
  address   = {New York}
}

@article{dean:pagano:2015,
  title={Evaluating confidence interval methods for binomial proportions in clustered surveys},
  author={Dean, Natalie and Pagano, Marcello},
  journal={Journal of Survey Statistics and Methodology},
  volume={3},
  number={4},
  pages={484--503},
  year={2015},
  publisher={Oxford University Press}
}

@Book{deaton:1992,
  author    = "Angus Deaton",
  year      = "1992",
  title     = "Understanding Consumption",
  series    = "Clarendon Lectures in Economics",
  publisher = "Oxford University Press"
}

@Book{deaton:1997,
  author    = "Angus Deaton",
  year      = "1997",
  title     = "The Analysis of Household Surveys. {A} Microeconometric
          Approach to Development Policy",
  publisher = "John Hopkins University Press",
  address   = "Baltimore"
}

@TechReport{deaton:dreze:2002,
  author    = "Angus Deaton and Jean Dr{\`e}ze",
  year      = 2002,
  title     = "Poverty and Inequality in {I}ndia: A Reexamination",
  institution   = "Centre for Development Economics, Delhi School of
          Economics",
  type      = "Working paper",
  number    = {107}
}

@book{demidenko:2004,
    author = {Demidenko, Eugene},
    howpublished = {Hardcover},
    isbn = {0471601616},
    publisher = {Wiley-Interscience},
    title = {Mixed Models: Theory and Applications},
    series = {Wiley Series in Probability and Statistics},
    year = {2004}
}

@ARTICLE{deber:spiel:deana:2004,
  AUTHOR =       {Deberard, S. M. and Spielmans, G. I. and Deana, L. J.},
  TITLE =        {Predictors of academic achievement and retention among college freshmen:
                  a longitudinal study.},
  JOURNAL =      {College Student Journal},
  YEAR =         {2004},
  volume =       {38},
  number =       {1},
  pages =        {66--81},
  source =       {Isabella Zaniletti},
}



@Article{dejong:ingram:whiteman:2000,
  author    = {David N. DeJong and Beth F. Ingram and Charles H.
          Whiteman},
  year      = {2000},
  title     = {A {Bayesian} approach to dynamic macroeconomics},
  journal   = {Journal of Econometrics},
  volume    = {98},
  pages     = {203--223},
  abstract  = { We propose and implement a coherent statistical framework
          for combining theoretical and empirical models of
          macroeconomic activity. The framework is Bayesian, and
          enables the formal yet probabilistic incorporation of
          uncertainty regarding the parameterization of theoretical
          models. The approach is illustrated using a neoclassical
          business-cycle model that builds on the Greenwood et al.
          (1988, American Economic Review 78, 402--417)
          variable-utilization framework to study out-of-sample
          forecasting of output and investment. The forecasts so
          produced are comparable with those from a Bayesian vector
          autoregression. }
}

@article{deleeuw:2005,
  title={To mix or not to mix data collection modes in surveys},
  author={{de} Leeuw, E. D.},
  journal={Journal of Official Statistics},
  volume={21},
  number={2},
  pages={233},
  year={2005}
}

@ARTICLE{delen:2010,
  AUTHOR =       {Delen, D.},
  TITLE =        {A comparative analysis of machine learning techniques for student retention management},
  JOURNAL =      {Decision Support Systems},
  YEAR =         {2010},
  volume =       {49},
  number =       {4},
  pages =        {498--506},
  source =       {Isabella Zaniletti},
}



@Article{delmoral:doucet:jasra:2006,
  author    = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year      = {2006},
  title     = {Sequential {Monte} {Carlo} samplers},
  abstract  = {We propose a methodology to sample sequentially from a
          sequence of probability distributions that are defined on a
          common space, each distribution being known up to a
          normalizing constant. These probability distributions are
          approximated by a cloud of weighted random samples which
          are propagated over time by using sequential Monte Carlo
          methods. This methodology allows us to derive simple
          algorithms to make parallel Markov chain Monte Carlo
          algorithms interact to perform global optimization and
          sequential Bayesian estimation and to compute ratios of
          normalizing constants. We illustrate these algorithms for
          various integration tasks arising in the context of
          Bayesian inference.},
  doi       = {10.1111/j.1467-9868.2006.00553.x},
  issn      = {1369-7412},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {3},
  pages     = {411--436},
  publisher = {Blackwell Publishing},
  volume    = {68}
}

@Article{deming:stephan:1940,
  author    = {Deming, Edwards W. and Stephan, Frederick F. },
  year      = {1940},
  title     = {On a Least Squares Adjustment of a Sampled Frequency Table
          When the Expected Marginal Totals are Known},
  doi       = {10.1214/aoms/1177731829},
  journal   = {Annals of Mathematical Statistics},
  number    = {4},
  pages     = {427--444},
  volume    = {11}
}

@Book{demmel:2001,
  author    = "James W. Demmel",
  year      = 1997,
  title     = "Applied Numerical Linear Algebra",
  publisher = "SIAM"
}

@Book{demmel:linal,
  author    = "James W. Demmel",
  year      = "1997",
  title     = "Applied Numerical Linear Algebra",
  publisher = "SIAM",
  address   = "Philadelphia"
}

@Article{demnati:rao:2004,
  author    = {Abdellatif Demnati and J.N.K. Rao},
  year      = {2004},
  title     = {Linearization Variance Estimators for Survey Data},
  journal   = {Survey Methodology},
  volume    = {30},
  number    = {1},
  pages     = {17--26},
  abstract  = {In survey sampling, Taylor linearization is often used to
          obtain variance estimators for calibration estimators of
          totals and nonlinear finite population (or census)
          parameters, such as ratios, regression and correlation
          coefficients, which can be expressed as smooth functions of
          totals. Taylor linearization is generally applicable to any
          sampling design, but it can lead to multiple variance
          estimators that are asymptotically design unbiased under
          repeated sampling. The choice among the variance estimators
          requires other considerations such as (i) approximate
          unbiasedness for the model variance of the estimator under
          an assumed model, (ii) validity under a conditional
          repeated sampling framework. In this paper, a new approach
          to deriving Taylor linearization variance estimators is
          proposed. It leads directly to a variance estimator which
          satisfies the above considerations at least in a number of
          important cases. The method is applied to a variety of
          problems, covering estimators of a total as well as other
          estimators defined either explicitly or implicitly as
          solutions of estimating equations. In particular,
          estimators of logistic regression parameters with
          calibration weights are studied. It leads to a new variance
          estimator for a general class of calibration estimators
          that includes generalized raking ratio and generalized
          regression estimators. The proposed method is extended to
          two-phase sampling to obtain a variance estimator that
          makes fuller use of the first phase sample data compared to
          traditional linearization variance estimators.}
}

@article{demnati:rao:2010,
    author = {Demnati, A. and Rao, J. N. K.},
    journal = {Survey Methodology},
    number = {2},
    pages = {193--202},
    title = {Linearization variance estimators for model parameters from complex survey data},
    volume = {36},
    year = {2010}
}

@Article{demp:lair:rubi:77,
  author    = "A. P. Dempster and N. M. Laird and D. B. Rubin",
  year      = 1977,
  title     = "Maximum likelihood from incomplete data via the {EM}
          algorithm (with discussion)",
  journal   = "Journal of the Royal Statistical Society B",
  volume    = 39,
  pages     = "1--38"
}

@Article{demp:rubi:tsut:1981,
  author    = {Dempster, A. P. and Rubin, D. B. and Tsutakawa, R. K.},
  year      = 1981,
  title     = {Estimation in Covariance Components Models},
  journal   = {Journal of the American Statistical Association},
  volume    = 76,
  pages     = {341--353}
}

@Article{demp:selw:pate:roth:1984,
  author    = {Dempster, Arthur P. and Selwyn, Murray R. and Patel,
          Chandu M. and Roth, Arthur J.},
  year      = 1984,
  title     = {Statistical and Computational Aspects of Mixed Model
          Analysis},
  journal   = {Applied Statistics},
  volume    = 33,
  pages     = {203--214}
}

@Article{dempster:laird:rubin:1977,
  author    = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year      = {1977},
  title     = {Maximum Likelihood from Incomplete Data via the {EM}
          Algorithm},
  abstract  = {A broadly applicable algorithm for computing maximum
          likelihood estimates from incomplete data is presented at
          various levels of generality. Theory showing the monotone
          behaviour of the likelihood and convergence of the
          algorithm is derived. Many examples are sketched, including
          missing value situations, applications to grouped, censored
          or truncated data, finite mixture models, variance
          component estimation, hyperparameter estimation,
          iteratively reweighted least squares and factor analysis.},
  doi       = {10.2307/2984875},
  issn      = {00359246},
  journal   = {Journal of the Royal Statistical Society. Series B
          (Methodological)},
  number    = {1},
  pages     = {1--38},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  volume    = {39}
}

@Article{deressa:ali:berhane:2007,
  author    = {Deressa, W. and Ali, A. and Berhane, Y.},
  year      = {2007},
  title     = {Household and socioeconomic factors associated with
          childhood febrile illnesses and treatment seeking behavior
          in an area of epidemic malaria in rural {E}thiopia},
  journal   = {Transactions of the Royal Society of Tropical Medicine and
          Hygiene},
  volume    = {101},
  number    = {9},
  pages     = {939--947}
}

@article{dette:melas:pepel:2004,
     title = {Optimal Designs for a Class of Nonlinear Regression Models},
     author = {Dette, Holger and Melas, Viatcheslav B. and Pepelyshev, Andrey},
     journal = {The Annals of Statistics},
     volume = {32},
     number = {5},
     pages = {2142--2167},
     note = {http://www.jstor.org/stable/3448568},
     year = {2004},
     abstract =
        {For a broad class of nonlinear regression models we
        investigate the local E- and c-optimal design problem. It is
        demonstrated that in many cases the optimal designs with
        respect to these optimality criteria are supported at the
        Chebyshev points, which are the local extrema of the
        equi-oscillating best approximation of the function $f_0
        \equiv 0$ by a normalized linear combination of the
        regression functions in the corresponding linearized model.
        The class of models includes rational, logistic and
        exponential models and for the rational regression models
        the E- and c-optimal design problem is solved explicitly in
        many cases.},
}

@ARTICLE{devaud:tille:2019,
  author =       {Denis Devaud and Yves Till{\'e}},
  title =        {Deville and Sarndal's calibration:
                    revising a 25-years-old successful optimization problem
                    (with discussion)},
  journal =      {Test},
  year =         {2019},
  volume =       {28},
  number =       {4},
  pages =        {1033--1065},
  doi =          {10.1007/s11749-019-00681-3}
}

@Book{devellis:2003,
  author    = {De{V}ellis, Robert F. },
  year      = {2003},
  title     = {Scale Development: Theory and Applications},
  isbn      = {0761926054},
  publisher = {Sage Publications, Inc},
  series    = {Applied Social Research Methods},
  edition   = {2nd}
}


@ARTICLE{dever:valliant:2010,
  AUTHOR =       {Jill A. Dever and Richard Valliant},
  TITLE =        {A comparison of variance estimators for poststratification to estimated control totals},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2010},
  volume =       {36},
  number =       {1},
  pages =        {45--56},
}


@Article{deville:1999,
  author    = {Deville, Jean-Claude },
  year      = {1999},
  title     = {Variance estimation for complex statistics and estimators:
          linearization and residual techniques},
  journal   = {Survey Methodology},
  number    = {2},
  pages     = {219--230},
  comment   = {http://www.statcan.ca/bsolc/english/bsolc?catno=12-001-XPB19990024882},
  volume    = {25}
}

@article{deville:tille:1998,
    author = {Deville, Jean-Claude and Till\'{e}, Yves},
    doi = {10.1093/biomet/85.1.89},
    journal = {Biometrika},
    number = {1},
    pages = {89--101},
    title = {Unequal probability sampling without replacement through a splitting method},
    volume = {85},
    year = {1998},
    abstract =
        {A very general class of sampling methods without replacement and
        with unequal probabilities is proposed. It consists of splitting the
        inclusion probability vector into several new inclusion probability
        vectors. One of these vectors is chosen randomly; thus, the initial
        problem is reduced to another sampling problem with unequal
        probabilities. This splitting is then repeated on these new vectors
        of inclusion probabilities; at each step, the sampling problem is
        reduced to a simpler problem. The simplicity of this technique allows
        one to generate easily new sampling procedures with unequal
        probabilities. The splitting method also generalises well-known
        methods such as the Midzuno method, the elimination procedure and the
        Chao procedure. Next, a sufficient condition is given in order that a
        splitting method satisfies the {Sen-Yates}-Grundy condition. Finally,
        it is shown that the elimination procedure satisfies the Gabler
        sufficient condition.},
}

@article{deville:sarndal:1992,
    author = {Deville, Jean C. and S{\"a}rndal, Carl E.},
    doi = {10.2307/2290268},
    journal = {Journal of the American Statistical Association},
    number = {418},
    pages = {376--382},
    title = {Calibration Estimators in Survey Sampling},
    volume = {87},
    year = {1992},
}

@Article{deville:sarndal:sautory:1993,
  author    = {Deville, Jean C. and S{\"a}rndal, Carl E. and Sautory, Olivier},
  year      = {1993},
  title     = {Generalized Raking Procedures in Survey Sampling},
  doi       = {http://dx.doi.org/10.2307/2290793},
  journal   = {Journal of the American Statistical Association},
  number    = {423},
  pages     = {1013--1020},
  volume    = {88}
}

@Article{diam:wink:2001,
  author    = {Diamantopoulos, Adamantios and Winklhofer, Heidi M. },
  year      = {2001},
  title     = {Index Construction with Formative Indicators: An
          Alternative to Scale Development},
  journal   = {Journal of Marketing Research},
  number    = {2},
  pages     = {269--277},
  abstract  = {Although the methodological literature is replete with
          advice regarding the development and validation of
          multi-item scales based on reflective measures, the issue
          of index construction using formative measures has received
          little attention. The authors seek to address this gap by
          (1) examining the nature of formative indicators, (2)
          discussing ways in which the quality of formative measures
          can be assessed, and (3) illustrating the proposed
          procedures with empirical data. The aim is to enhance
          researchers? understanding of formative measures and assist
          them in their index construction efforts.},
  volume    = {38}
}

@Article{diciccio1989adjustments,
  author    = {Diciccio, Thomas J. and Romano, Joseph P.},
  year      = {1989},
  title     = {On Adjustments Based on the Signed of the Empirical
          Likelihood Ratio Statistic},
  abstract  = {The standard multivariate normal approximation to the
          distribution of the signed root of the empirical likelihood
          ratio statistic is considered in cases where inference is
          required for a smooth function of the mean of the
          distribution from which the sample is drawn. The error in
          this approximation is of order O(n-1/2), where n is the
          sample size, and it is shown that the error can be reduced
          to order O(n-1) by using a mean adjustment. When a scalar
          parameter is of interest, use of the mean adjustment
          produces confidence intervals whose endpoints each have
          coverage error of order O(n-1). This is in contrast to use
          of the unadjusted signed root, or equivalently, use of the
          chi-squared approximation for the empirical likelihood
          ratio statistic, which produces one-sided limits having
          coverage error of order O(n-1/2). For a vector parameter of
          interest, a procedure asymptotically equivalent to the mean
          adjustment is developed that avoids explicit calculation of
          the signed square root. It is argued for the scalar
          parameter case that the coverage error of one-sided limits
          can be further reduced to O(n-3/2) by use of both mean and
          variance adjustments. The variance adjustment is given in
          the case when a scalar mean is of interest. The coverage
          accuracy of confidence limits obtained by the various
          methods is illustrated by simulation.},
  doi       = {10.2307/2336111},
  issn      = {00063444},
  journal   = {Biometrika},
  number    = {3},
  pages     = {447--456},
  publisher = {Biometrika Trust},
  volume    = {76}
}

@Article{diciccio:hall:romano:1991,
  author    = {{DiC}iccio, Thomas and Hall, Peter and Romano, Joseph },
  year      = {1991},
  title     = {Empirical Likelihood is {B}artlett-Correctable},
  doi       = {10.2307/2242100},
  journal   = {The Annals of Statistics},
  number    = {2},
  pages     = {1053--1061},
  volume    = {19},
  abstract  = {It is shown that, in a very general setting, the empirical
          likelihood method for constructing confidence intervals is
          Bartlett-correctable. This means that a simple adjustment
          for the expected value of log-likelihood ratio reduces
          coverage error to an extremely low O(n-2), where n denotes
          sample size. That fact makes empirical likelihood
          competitive with methods such as the bootstrap which are
          not Bartlett-correctable and which usually have coverage
          error of size n-1. Most importantly, our work demonstrates
          a strong link between empirical likelihood and parametric
          likelihood, since the Bartlett correction had previously
          only been available for parametric likelihood. A general
          formula is given for the Bartlett correction, valid in a
          very wide range of problems, including estimation of mean,
          variance, covariance, correlation, skewness, kurtosis, mean
          ratio, mean difference, variance ratio, etc. The efficacy
          of the correction is demonstrated in a simulation study for
          the case of the mean.}
}

@Article{diciccio:monti:2001,
  author    = {Diciccio, Thomas J. and Monti, Anna C.},
  year      = {2001},
  title     = {Approximations to the profile empirical likelihood
          function for a scalar parameter in the context of
          $M$-estimation},
  doi       = {10.1093/biomet/88.2.337},
  journal   = {Biometrika},
  number    = {2},
  pages     = {337--351},
  abstract  = {Empirical likelihood possesses many of the important
          properties of genuine parametric likelihood, but it is
          computationally burdensome, especially when nuisance
          parameters are present. This paper presents two
          approximations to the profile empirical likelihood function
          for a scalar parameter of interest in the context of
          M-estimation; the simpler approximation is based on the
          third derivative of the profile log empirical likelihood
          function at its maximising point, while the more accurate
          approximation involves both the third and fourth
          derivatives.Formulae are given for these higher-order
          derivatives that can be evaluated using ordinary matrix
          operations, so computation of the approximations is very
          easy. The accuracy of the approximations is demonstrated in
          several numerical examples. The computational simplicity of
          the approximations makes it feasible to use them in
          conjunction with bootstrap calibration for constructing
          accurate confidence intervals and limits. The derivatives
          are also helpful for exploring the shape of the profile log
          empirical likelihood function and for determining suitable
          parameterisations for studentised statistics.
          10.1093/biomet/88.2.337},
  volume    = {88}
}

@Article{diciccio:romano:1989,
  author    = {Diciccio, Thomas J. and Romano, Joseph P.},
  year      = {1989},
  title     = {On adjustments based on the signed root of the empirical
          likelihood ratio statistic},
  abstract  = {The standard multivariate normal approximation to the
          distribution of the signed root of the empirical likelihood
          ratio statistic is considered in cases where inference is
          required for a smooth function of the mean of the
          distribution from which the sample is drawn. The error in
          this approximation is of order O(n-[1/2]) where n is the
          sample size, and it is shown that the error can be reduced
          to order O(n-1) by using a mean adjustment. When a scalar
          parameter is of interest, use of the mean adjustment
          produces confidence intervals whose endpoints each have
          coverage error of order O(n-1). This is in contrast to use
          of the unadjusted signed root, or equivalently, use of the
          chi-squared approximation for the empirical likelihood
          ratio statistic, which produces one-sided limits having
          coverage error of order O(n-[1/2]). For a vector parameter
          of interest, a procedure asymptotically equivalent to the
          mean adjustment is developed that avoids explicit
          calculation of the signed square root. It is argued for the
          scalar parameter case that the coverage error of one-sided
          limits can be further reduced to O(n-3/2 by use of both
          mean and variance adjustments. The variance adjustment is
          given in the case when a scalar mean is of interest. The
          coverage accuracy of confidence limits obtained by the
          various methods is illustrated by simulation.
          10.1093/biomet/76.3.447},
  doi       = {10.1093/biomet/76.3.447},
  journal   = {Biometrika},
  number    = {3},
  pages     = {447--456},
  volume    = {76}
}

@article{dickey:fuller:1979,
     title = {Distribution of the Estimators for Autoregressive Time Series With a Unit Root},
     author = {Dickey, David A. and Fuller, Wayne A.},
     journal = {Journal of the American Statistical Association},
     volume = {74},
     number = {366},
     pages = {427--431},
     comment = {http://www.jstor.org/stable/2286348},
     year = {1979},
}

@Book{diggle:heag:liang:zeger:2002,
  author    = "Peter Diggle and Patrick Heagerty and Kung-Yee Liang and
          Scott Zeger",
  year      = 2002,
  title     = "Analysis of Longitudinal Data",
  publisher = "Oxford University Press",
  edition   = "2nd"
}

@Book{diggle:heagerty:liang:zeger:2nd:2002,
  author    = {Diggle, Peter and Heagerty, Patrick and Liang, Kung-Yee
          and Zeger, Scott},
  year      = {2002},
  title     = {Analysis of Longitudinal Data},
  abstract  = {{ The new edition of this important text has been
          completely revised and expanded to become the most
          up-to-date and thorough professional reference text in this
          fast-moving and important area of biostatistics. Two new
          chapters have been added on fully parametric models for
          discrete repeated<br>measures data and on statistical
          models for time-dependent predictors where there may be
          feedback between the predictor and response variables. It
          also contains the many useful features of the previous
          edition such as, design issues, exploratory methods of
          analysis, linear models for continuous<br>data, and models
          and methods for handling data and missing values. }},
  edition   = {2},
  howpublished  = {Hardcover},
  isbn      = {0198524846},
  publisher = {Oxford University Press, USA}
}

@Article{diggle:kenward:1994,
  author    = {Diggle, P. and Kenward, M. G.},
  year      = {1994},
  title     = {Informative Drop-Out in Longitudinal Data Analysis},
  abstract  = {A model is proposed for continuous longitudinal data with
          non-ignorable or informative drop-out (ID). The model
          combines a multivariate linear model for the underlying
          response with a logistic regression model for the drop-out
          process. The latter incorporates dependence of the
          probability of drop-out on unobserved, or missing,
          observations. Parameters in the model are estimated by
          using maximum likelihood (ML) and inferences drawn through
          conventional likelihood procedures. In particular,
          likelihood ratio tests can be used to assess the
          informativeness of the drop-out process through comparison
          of the full model with reduced models corresponding to
          random drop-out (RD) and completely random processes. A
          simulation study is used to assess the procedure in two
          settings: the comparison of time trends under a linear
          regression model with autocorrelated errors and the
          estimation of period means and treatment differences from a
          four-period four-treatment crossover trial. It is seen in
          both settings that, when data are generated under an ID
          process, the ML estimators from the ID model do not suffer
          from the bias that is present in the ordinary least squares
          and RD ML estimators. The approach is then applied to three
          examples. These derive from a milk protein trial involving
          three groups of cows, milk yield data from a study of
          mastitis in dairy cattle and data from a multicentre
          clinical trial on the study of depression. All three
          examples provide evidence of an underlying ID process, two
          with some strength. It is seen that the assumption of an ID
          rather than an RD process has practical implications for
          the interpretation of the data.},
  doi       = {10.2307/2986113},
  issn      = {00359254},
  journal   = {Journal of the Royal Statistical Society. Series C
          (Applied Statistics)},
  number    = {1},
  pages     = {49--93},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  volume    = {43}
}

@Article{dijkstra:1992,
  author    = {Dijkstra, Taeke K.},
  year      = {1992},
  title     = {On Statistical Inference With Parameter Estimates on the
          Boundary of the Parameter Space},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {45},
  pages     = {289--309},
  abstract  = {Inadmissible estimates, like negative variance estimates,
          frequently occur when a theoretical covariance matrix is
          fitted to a sample covariance matrix. When this happens, it
          is common practice to re-estimate subject to non-negativity
          constraints and to count the ensuing zeros as true zeros.
          We derive conditions under which this approach has a valid
          frequentist interpretation}
}

@Article{dill:kumar:mula:1987,
  author    = {William R. Dillon and Ajith Kumar and Narendra Mulani},
  year      = {1987},
  title     = {Offending Estimates in Covariance Structure Analysis:
          Comments on the Causes and Solutions to {H}eywood Cases},
  journal   = {Psychological Bulletin},
  volume    = {101},
  pages     = {126--135}
}

@Book{diller:latex:byline,
  author    = "Antoni Diller",
  year      = "1999",
  title     = "\LaTeX: Line by Line: Tips and Techniques for Document
          Processing",
  edition   = "2nd",
  publisher = "John Wiley and Sons"
}

@InCollection{dipillo:palagi:2002,
  author    = "Gianni {Di~Pillo} and Laura Palagi",
  editor    = "Panos M. Pardalos and Mauricio G. C. Resende",
  year      = 2002,
  title     = "Nonlinear programming",
  booktitle = "Handbook of Applied Optimization",
  publisher = "Oxford University Press"
}

@Article{distefano:2002,
  author    = "Christine {DiSte}fano",
  year      = 2002,
  title     = "The Impact of Categorization With Confirmatory Factor
          Analysis",
  journal   = "Structural Equations Modeling",
  volume    = 9,
  pages     = "327--346"
}

@Article{do1991quasirandom,
  author    = {Do, Kim-Anh and Hall, Peter},
  year      = {1991},
  title     = {Quasi-random resampling for the bootstrap},
  abstract  = {Quasi-random sequences are known to give efficient
          numerical integration rules in many Bayesian statistical
          problems where the posterior distribution can be
          transformed into periodic functions on then-dimensional
          hypercube. From this idea we develop a quasi-random
          approach to the generation of resamples used for Monte
          Carlo approximations to bootstrap estimates of bias,
          variance and distribution functions. We demonstrate a major
          difference between quasi-random bootstrap resamples, which
          are generated by deterministic algorithms and have no true
          randomness, and the usual pseudo-random bootstrap resamples
          generated by the classical bootstrap approach. Various
          quasi-random approaches are considered and are shown via a
          simulation study to result in approximants that are
          competitive in terms of efficiency when compared with other
          bootstrap Monte Carlo procedures such as balanced and
          antithetic resampling.},
  doi       = {10.1007/BF01890833},
  journal   = {Statistics and Computing},
  number    = {1},
  pages     = {13--22},
  volume    = {1}
}

@Article{do:hall:1991,
  author    = {Do, Kim-Anh and Hall, Peter },
  year      = {1991},
  title     = {Quasi-random resampling for the bootstrap},
  journal   = {Statistics and Computing},
  number    = {1},
  pages     = {13--22},
  comment   = {http://dx.doi.org/10.1007/BF01890833},
  volume    = {1}
}

@Article{dolan:1994,
  author    = "Conor V. Dolan",
  year      = 1994,
  title     = "Factor analysis with 2, 3, 5 and 7 response categories: A
          comparison of categorical variable estimators using
          simulated data",
  journal   = "British Journal of Mathetmatical and Statistical
          Psychology",
  volume    = 47,
  pages     = "309--326"
}

@Article{donald:imbens:newey:2003,
  author    = {Donald, Stephen G. and Imbens, Guido W. and Newey, Whitney
          K. },
  year      = {2003},
  title     = {Empirical likelihood estimation and consistent tests with
          conditional moment restrictions},
  abstract  = {This paper is about efficient estimation and consistent
          tests of conditional moment restrictions. We use
          unconditional moment restrictions based on splines or other
          approximating functions for this purpose. Empirical
          likelihood estimation is particularly appropriate for this
          setting, because of its relatively low bias with many
          moment conditions. We give conditions so that efficiency of
          estimators and consistency of tests is achieved as the
          number of restrictions grows with the sample size. We also
          give results for generalized empirical likelihood,
          generalized method of moments, and nonlinear instrumental
          variable estimators.},
  doi       = {10.1016/S0304-4076(03)00118-0},
  journal   = {Journal of Econometrics},
  number    = {1},
  pages     = {55--93},
  volume    = {117}
}

@article{dong:stuart:lenis:nguyen:2020,
  author = {Nianbo Dong and Elizabeth A. Stuart and David Lenis and Trang Quynh Nguyen},
  title = {Using Propensity Score Analysis of Survey Data to Estimate 
          Population Average Treatment Effects: A Case Study Comparing Different Methods},
  journal = {Evaluation Review},
  volume = {44},
  number = {1},
  pages = {84-108},
  year = {2020},
  doi = {10.1177/0193841X20938497},
  note ={PMID: 32672113},
  URL = {https://doi.org/10.1177/0193841X20938497},
  abstract = 
      { Background:Many studies in psychological and educational research
      aim to estimate population average treatment effects (PATE) using
      data from large complex survey samples, and many of these studies use
      propensity score methods. Recent advances have investigated how to
      incorporate survey weights with propensity score methods. However, to
      this point, that work had not been well summarized, and it was not
      clear how much difference the different PATE estimation methods would
      make empirically.Purpose:The purpose of this study is to
      systematically summarize the appropriate use of survey weights in
      propensity score analysis of complex survey data and use a case study
      to empirically compare the PATE estimates using multiple analysis
      methods that include ordinary least squares regression, weighted
      least squares regression, and various propensity score
      applications.Methods:We first summarize various propensity score
      methods that handle survey weights. We then demonstrate the
      performance of various analysis methods using a nationally
      representative data set, the Early Childhood Longitudinal
      Study–Kindergarten to estimate the effects of preschool on children’s
      academic achievement. The correspondence of the results was evaluated
      using multiple criteria.Results and Conclusions:It is important for
      researchers to think carefully about their estimand of interest and
      use methods appropriate for that estimand. If interest is in drawing
      inferences to the survey target population, it is important to take
      the survey weights into account, particularly in the outcome analysis
      stage for estimating the PATE. The case study shows, however, not
      much difference among various analysis methods in one applied
      example.}
}



@Book{doukhan:1994,
  author    = "Doukhan, P.",
  year      = 1994,
  title     = "Mixing. Properties and Examples",
  publisher = "Springer",
  volume    = 85,
  series    = "Lecture Notes in Statistics",
  address   = "New York"
}

@Article{drakos:2002,
  author    = "Konstantinos Drakos",
  year      = 2002,
  title     = "Common Factor in Eurocurrency Rates: A Dynamic Analysis",
  journal   = "Journal of Economic Integration",
  volume    = 17,
  pages     = "164--184"
}

@Book{draper:smith:1998,
  author    = "Norman P. Draper and Harry Smith",
  year      = 1998,
  title     = "Applied Regression Analysis",
  publisher = "John Wiley and Sons",
  address   = "New York",
  edition   = "3rd"
}

@BOOK{drechsler:2011,
  author =       {J{\"o}rg Drechsler},
  title =        {Synthetic Datasets for Statistical Disclosure Control:
  Theory and Implementation},
  publisher =    {Springer},
  year =         {2011},
  series =       {Lecture Notes in Statistics},
  address =      {New York, NY},
}


@Article{driel:1978,
  author    = {Otto P. {van Driel}},
  year      = {1978},
  title     = {On Various Causes of Improper Solutions in Maximum
          Likelihood Factor Analysis},
  journal   = {Psychometrika},
  volume    = {43},
  pages     = {225--43}
}

@Article{drukker:gates:2006,
  author    = {David M. Drukker and Richard Gates},
  year      = {2006},
  title     = {Generating {Halton} sequences using {Mata}},
  journal   = {Stata Journal},
  volume    = {6},
  number    = {2},
  pages     = {214--228}
}

@TechReport{duclos:davidson:2006,
  author    = {Davidson, Russell and Duclos, Jean-Yves},
  year      = {2006},
  title     = {Testing for Restricted Stochastic Dominance},
  howpublished  = {Working paper},
  institution   = {CIRPEE},
  number    = {0609}
}

@BOOK{duclos:araar:2006,
  AUTHOR =       {Jean-Yves Duclos and Abdelkrim Araar},
  TITLE =        {Poverty and Equity: Measurement, Policy, and Estimation with {DAD}},
  PUBLISHER =    {Springer/International Development Research Center},
  YEAR =         {2006},
}


@Book{duda2000pattern,
  author    = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  year      = {2000},
  title     = {Pattern Classification (2nd Edition)},
  edition   = {2},
  howpublished  = {Hardcover},
  isbn      = {0471056693},
  publisher = {Wiley-Interscience}
}

@Book{duda:hart:stork:2000,
  author    = {Duda, Richard O. and Hart, Peter E. and Stork, David G. },
  year      = {2000},
  title     = {Pattern Classification},
  publisher = {Wiley-Interscience},
  address   = {New York},
  edition   = {2nd}
}

@BOOK{dudley:2002,
  AUTHOR =       {R. M. Dudley},
  TITLE =        {Real analysis and probability},
  PUBLISHER =    {Cambridge University Press},
  YEAR =         {2002},
  series =       {Cambridge series in advanced mathematics},
  address =      {Cambridge, UK},
  edition =      {2nd},
}


@book{dummit:foote:2003,
    author = {Dummit, David S. and Foote, Richard M.},
    edition = {3rd},
    howpublished = {Hardcover},
    isbn = {0471433349},
    publisher = {Wiley},
    address = {New York},
    title = {Abstract Algebra},
    year = {2003}
}


@Article{dumouchel1983using,
  author    = {Dumouchel, William H. and Duncan, Greg J.},
  year      = {1983},
  title     = {Using Sample Survey Weights in Multiple Regression
          Analyses of Stratified Samples},
  abstract  = {The rationale for the use of sample survey weights in a
          least squares regression analysis is examined with respect
          to four increasingly general specifications of the
          population regression model. The appropriateness of the
          weighted regression estimate depends on which model is
          chosen. A proposal is made to use the difference between
          the weighted and unweighted estimates as an aid in choosing
          the appropriate model and hence the appropriate estimator.
          When applied to an analysis of the familial and
          environmental determinants of the educational level
          attained by a sample of young adults, the methods lead to a
          revision of the initial additive model in which interaction
          terms between county unemployment and race, as well as
          between sex and mother's education, are included.},
  journal   = {Journal of the American Statistical Association},
  number    = {383},
  pages     = {535--543},
  volume    = {78}
}

@TECHREPORT{dutoit:1993,
  AUTHOR =       {{du Toit}, Stephen H. C.},
  TITLE =        {Analysis of Multilevel Models: Theoretical aspects},
  INSTITUTION =  {HSRC},
  YEAR =         {1993},
  type =         {research report},
  address =      {South Africa},
  source =       {du Toit private conversation},
}

@INPROCEEDINGS{dwork:lei:2009,
  author =       {Dwork, Cynthia and Lei, Jing},
  title =        {Differential privacy and robust statistics},
  year =         {2009},
  series =       {Proceedings of the forty-first annual ACM symposium on Theory of computing},
  pages =        {371--380},
  note =         {doi: 10.1145/1536414.1536466},
  abstract =     {
    We show by means of several examples that robust statistical estimators
    present an excellent starting point for differentially private
    estimators. Our algorithms use a new paradigm for differentially private
    mechanisms, which we call Propose-Test-Release (PTR), and for which we
    give a formal definition and general composition theorems.
    }
}


@Article{eckler:1955,
  author    = "A. R. Eckler",
  year      = "1955",
  title     = "Rotation Sampling",
  journal   = "Annals of Mathematical Statistics",
  volume    = "26",
  number    = "4",
  pages     = "664--685"
}

@Article{eddings:marchenko:2012,
  author={Wesley Eddings and Yulia Marchenko},
  title={Diagnostics for multiple imputation in {Stata}},
  journal={Stata Journal},
  year=2012,
  volume={12},
  number={3},
  pages={353--367},
  month={September},
  abstract=
    {Our new command midiagplots makes diagnostic plots for multiple
    imputations created by mi impute. The plots compare the distribution of
    the imputed values with that of the observed values so that problems with
    the imputation model can be corrected before the imputed data are
    analyzed. We include an example and suggest extensions to other
    diagnostics. Copyright 2012 by StataCorp LP.},
}

@Book{edgi:1995,
  author    = "E. S. Edgington",
  year      = 1995,
  title     = "Randomization Tests",
  edition   = "3rd",
  publisher = "Marcel Dekker",
  address   = "New York"
}

@Article{efron:hinkley:1978,
  author    = {Efron, Bradley and Hinkley, David V.},
  year      = {1978},
  title     = {Assessing the accuracy of the maximum likelihood
          estimator: Observed versus expected Fisher information},
  abstract  = {This paper concerns normal approximations to the
          distribution of the maximum likelihood estimator in
          one-parameter families. The traditional variance
          approximation is 1/[\&\#167;], where {theta} is the maximum
          likelihood estimator and [\&\#167;] is the expected total
          Fisher information. Many writers, including R. A. Fisher,
          have argued in favour of the variance estimate 1/I(x),
          where I(x) is the observed information, i.e. minus the
          second derivative of the log likelihood function at {theta}
          given data x. We give a frequentist justification for
          preferring 1/I(x) to 1/[\&\#167;]. The former is shown to
          approximate the conditional variance of 8 given an
          appropriate ancillary statistic which to a first
          approximation is I(x). The theory may be seen to flow
          naturally from Fisher's pioneering papers on likelihood
          estimation. A large number of examples are used to
          supplement a small amount of theory. Our evidence indicates
          preference for the likelihood ratio method of obtaining
          confidence limits. 10.1093/biomet/65.3.457},
  comment   = {This is a deep and wonderful paper that can be discussed
          for at least three weeks in a serious statistical
          theory/asymptotics class. The class of problems considered
          is when the likelihood affords an ancillary or an
          approximately ancillary statistic in a (curved) exponential
          family setting.

          The authors show that in some examples when the finite
          sample distribution of the main statistic given the
          ancillary is known, the variance of the former is better
          approximated by the observed information rather than Fisher
          (expected) information. In some cases an exact equality is
          achieved (Cox' 1958 example with two unbiased instruments
          that have different variances). They suggest the
          generalization: conditioning on ancillary statistic when
          one is available gives more meaningful measure of
          variability than the expected information; we would want to
          use this extra information, anyway!

          In translation families, the shape of the conditional
          distribution of the main statistic given the ancillary can
          be derived directly from the likelihood (although the
          normalizing constant has to be found by integration). When
          that conditional distribution is normal, the variance is
          given by the observed information. In the general
          non-normal case, an approximation can be obtained with
          higher order derivatives of the log-likelihood. Explicit
          expressions for the first two conditional moments are
          derived. The main term of the approximation for the
          variance is the observed information, and the relative size
          of the remainder is \$O\_p(n^{-1})\$ (Lemma 1).

          The relation between the observed and expected information
          is obtained as a function of statistical curvature (Efron
          1975). The curvature shows the deviation of the density
          from an exponential family, and is obtained as a somewhat
          involved function of the fist two dervatives of the
          log-density. The ratio of the observed to the expected
          information is asymptotically normal (with the scaling
          factor of \$\sqrt{n}\$), with the asymptotic variance being
          squared curvature.

          The next section considers confidence intervals for the
          location parameter. Higher order approximations are
          obtained for the p-values of the Wald test that uses the
          observed information, and the likelihood ratio, both
          conditional on the ancillary statistic. They are also
          compared to the unconditional Wald test based on the
          expected information. In simulations, it was shown that the
          LRT has the best performance of them all. The Wald test
          with the observed information tends to overreject; it is
          not clear how to qualify the performance of an
          unconditional test in this situation.

          For the non-translational families, an approximate
          asymptotic ancillary statistic is suggested, as a function
          of the observed and expected information, and the
          curvature. It leads to a local reparameterization with
          conditioning on this statistic, which in turn leads to
          better asymptotic normality of the main statistic in the
          new coordinates. An example is bivariate standard normal
          distribution: the new construction leads to the Fisher's
          variance stabilizing z-transformation!

          For curved exponential families, the proposed ancillary
          statistic has the interpretation of (Mahalanobis)
          distance/length of the projection from the sufficient
          statistic to the family.},
  doi       = {10.1093/biomet/65.3.457},
  journal   = {Biometrika},
  number    = {3},
  pages     = {457--483},
  volume    = {65}
}

@Book{efron:1987:siam,
  author    = {Efron, Bradley},
  year      = {1987},
  title     = {The Jackknife, the Bootstrap, and Other Resampling Plans},
  series    = {CBMS-NSF Regional Conference Series in Applied Mathematics},
  howpublished  = {Paperback},
  isbn      = {0898711797},
  publisher = {Society for Industrial \& Applied Mathematics}
}

@Article{efron:1979,
  author    = {Bradley Efron},
  year      = {1979},
  title     = {Bootstrap methods: Another look at the jackknife},
  journal   = {Annals of Statistics},
  volume    = {7},
  pages     = {1--26}
}

@Article{efron:1987,
  author    = {Bradley Efron},
  year      = {1987},
  title     = {Better bootstrap confidence intervals (with discussion)},
  journal   = {Journal of the American Statistical Association},
  volume    = {82},
  pages     = {171--200}
}

@Article{efron:1990,
  author    = {Efron, Bradley },
  year      = {1990},
  title     = {More Efficient Bootstrap Computations},
  journal   = {Journal of the American Statistical Association},
  number    = {409},
  pages     = {79--89},
  volume    = {85},
  abstract  = { This article concerns computational methods for the
          bootstrap that are more efficient than the straightforward
          Monte Carlo methods usually used. The bootstrap is
          considered in its simplest form: in a one-sample
          nonparametric problem, where the goal is to estimate the
          bias or variance of some statistic by bootstrap sampling,
          or to set approximate confidence intervals for a parameter
          of interest in terms of various percentiles of the
          bootstrap distribution. The methods of this article can, in
          favorable situations, reduce the necessary number of
          bootstrap replications manyfold. Moreover, simple
          diagnostics are available to see whether or not any
          particular case is accessible to these methods.}
}

@Book{efron:tibs:1994,
  author    = {Efron, Bradley and Tibshirani, Robert J. },
  year      = {1994},
  title     = {An Introduction to the Bootstrap},
  howpublished  = {Hardcover},
  isbn      = {0412042312},
  publisher = {{Chapman \& Hall/CRC}},
  abstract  = {Statistics is a subject of many uses and surprisingly few
          effective practitioners. The traditional road to
          statistical knowledge is blocked, for most, by a formidable
          wall of mathematics. The approach in An Introduction to the
          Bootstrap avoids that wall. It arms scientists and
          engineers, as well as statisticians, with the computational
          techniques they need to analyze and understand complicated
          data sets.}
}

@Book{ehrgott:2005,
  author    = {Ehrgott, Matthias },
  year      = {2005},
  title     = {Multicriteria Optimization},
  address   = {New York},
  publisher = {Springer},
  series    = {Lecture Notes in Economics and Mathematical Systems},
  comment   = {http://www.springer.com/business/operations+research/book/978-3-540-21398-7}
          ,
  volume    = {491}
}

@Article{eichen:hansen:single:1988,
  author    = {Eichenbaum, Martin S. and Hansen, Lars P. and Singleton,
          Kenneth J.},
  year      = {1988},
  title     = {A Time Series Analysis of Representative Agent Models of
          Consumption and Leisure Choice under Uncertainty},
  doi       = {10.2307/1882642},
  journal   = {The Quarterly Journal of Economics},
  number    = {1},
  pages     = {51--78},
  volume    = {103},
  abstract  = {This paper investigates empirically a model of aggregate
          consumption and leisure decisions in which utility from
          goods and leisure is nontime-separable. The nonseparability
          of preferences accommodates intertemporal substitution or
          complementarity of leisure and thereby affects the
          comovements in aggregate compensation and hours worked.
          These cross-relations are examined empirically using
          postwar monthly U. S. data on quantities, real wages, and
          the real return on the one-month Treasury bill. The
          estimated values of the parameters governing preferences
          differ significantly from the values assumed in several
          studies of real business models. Several possible
          explanations of these discrepancies are discussed.}
}

@InProceedings{eicker:1967,
  author    = {F. Eicker},
  year      = 1967,
  title     = {Limit theorems for regressions with unequal and dependent
          errors},
  booktitle = "Proceedings of the Fifth Berkeley Symposium on
          Mathematical Statistics and Probability",
  volume    = 1,
  pages     = "59--82",
  publisher = "University of California Press",
  address   = "Berkeley"
}

@article{elliott:2008,
    author = {Elliott, Michael R.},
    journal = {Journal of Official Statistics},
    number = {4},
    pages = {517--540},
    title = {Model Averaging Methods for Weight Trimming},
    volume = {24},
    year = {2008}
}

@article{elli:zasl:gold:lehr:hamb:beck:gior:2009,
    abstract = {
            To evaluate the need for survey mode adjustments to hospital care
            evaluations by discharged inpatients and develop the appropriate
            adjustments. A total of 7,555 respondents from a 2006 national
            random sample of 45 hospitals who completed the {CAHPS} Hospital
            ({HCAHPS} [Hospital Consumer Assessments of Healthcare Providers
            and Systems]) Survey. {STUDY} {DESIGN}/{DATA}
            {COLLECTION}/{EXTRACTION} {METHODS}: We estimated mode effects in
            linear models that predicted each {HCAHPS} outcome from
            hospital-fixed effects and patient-mix adjustors. Patients
            randomized to the telephone and active interactive voice response
            ({IVR}) modes provided more positive evaluations than patients
            randomized to mail and mixed (mail with telephone follow-up)
            modes, with some effects equivalent to more than 30 percentile
            points in hospital rankings. Mode effects are consistent across
            hospitals and are generally larger than total patient-mix
            effects. Patient-mix adjustment accounts for any nonresponse bias
            that could have been addressed through weighting. Valid
            comparisons of hospital performance require that reported
            hospital scores be adjusted for survey mode and patient mix. },
    author = {Elliott, Marc N. and Zaslavsky, Alan M. and Goldstein, Elizabeth
        and Lehrman, William and Hambarsoomians, Katrin and Beckett, Megan K. and Giordano, Laura},
    doi = {10.1111/j.1475-6773.2008.00914.x},
    issn = {1475-6773},
    journal = {Health services research},
    number = {2 Pt 1},
    pages = {501--518},
    pmid = {19317857},
    title = {Effects of survey mode, patient mix, and nonresponse on {CAHPS} hospital survey scores.},
    volume = {44},
    year = {2009}
}

@Article{eltinge:1996,
  author    = {John Eltinge},
  year      = {1996},
  title     = {Discussion of ``Resampling Methods in Sample Surveys'' by
          J. Shao},
  journal   = {Statistics},
  volume    = {27},
  pages     = {241--244}
}

@INPROCEEDINGS{eltinge:2013,
  author =       {John L. Eltinge},
  title =        {Integration of Matrix Sampling and Multiple-Frame Methodology},
  booktitle =    {Proceedings of the 59th ISI World Statistics Congress},
  year =         {2013},
  address =      {Hong Kong},
  pages =        {323--328},
}


@Book{embr:reise:reise:2000,
  author    = {Susan E. Embretson and Steve Reise and Steven Paul Reise},
  year      = {2000},
  title     = {Item Response Theory for Psychologists},
  publisher = {Lawrence Erlbaum Associates},
  series    = {Multivariate Applications},
  address   = {Mahwah, New Jersey}
}

@Misc{epa:1997,
  author    = "{EPA}",
  year      = 1997,
  title     = "{EPA}'s Revised Particulate Matter Standards, Fact Sheet",
  note      = "http://www.epa.gov/ttn/oarpg/naaqsfin/pmfact.html"
}

@Article{epa:1997:standard,
  author    = "{EPA}",
  year      = 1997,
  title     = "National Ambient Air Quality Standards for
          Par\-ti\-cu\-late Matter",
  journal   = "Federal Register",
  volume    = 62,
  issue     = 138,
  pages     = "38651--38760",
  note      = "http://www.epa.gov/ttn/oarpg/naaqsfin/pmnaaqs.pdf"
}

@TechReport{ernst:1999,
  author    = {Lawrence R. Ernst},
  year      = {1999},
  title     = {The Maximization and Minimization of Sample Overlap
          Problems: A Half Century of Results},
  institution   = {U.S.\ Bureau of Labor Statistics}
}

@article{estevao:sarndal:2000,
    author = {Estevao, Victor M. and S{\"{a}}rndal, Cark-Erik},
    journal = {Journal of Official Statistics},
    number = {4},
    pages = {379--399},
    title = {A Functional Form Approach to Calibration},
    volume = {16},
    year = {2000}
}

@article{estevao:sarndal:2006,
    author = {Estevao, Victor M. and S\"{a}rndal, Carl-Erik},
    doi = {10.1111/j.1751-5823.2006.tb00165.x},
    journal = {International Statistical Review},
    number = {2},
    pages = {127--147},
    title = {Survey Estimates by Calibration on Complex Auxiliary Information},
    volume = {74},
    year = {2006}
}


@Book{everitt:landau:leese:2001,
  author    = {Brian S. Everitt and Sabine Landau and Morven Leese},
  year      = {2001},
  title     = {Cluster Analysis},
  publisher = {Arnold Publishers},
  edition   = {4th}
}

@Article{faes:mole:aert:verb:kenw:2009,
  author    = {Faes, Christel and Molenberghs, Geert and Aerts, Marc and
          Verbeke, Geert and Kenward, Michael G.},
  year      = {2009},
  title     = {The Effective Sample Size and an Alternative Small-Sample
          Degrees-of-Freedom Method},
  abstract  = {Correlated data frequently arise in contexts such as, for
          example, repeated measures and meta-analysis. The amount of
          information in such data depends not only on the sample
          size, but also on the structure and strength of the
          correlations among observations from the same independent
          block. A general concept is discussed, the effective sample
          size, as a way of quantifying the amount of information in
          such data. It is defined as the sample size one would need
          in an independent sample to equal the amount of information
          in the actual correlated sample. This concept is widely
          applicable, for Gaussian data and beyond, and provides
          important insight. For example, it helps explain why
          fixed-effects and random-effects inferences of
          meta-analytic data can be so radically divergent. Further,
          we show that in some cases the amount of information is
          bounded, even when the number of measures per independent
          block approaches infinity. We use the method to devise a
          new denominator degrees-of-freedom method for fixed-effects
          testing. It is compared to the classical Satterthwaite and
          Kenward–Roger methods for performance and, more
          importantly, to enhance insight. A key feature of the
          proposed degrees-of-freedom method is that it, unlike the
          others, can be used for non-Gaussian data, too.},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {389--399},
  volume    = {63}
}

@Article{falaris:peters:1998,
  author    = {Falaris, Evangelos M. and Peters, Elizabeth H.},
  year      = {1998},
  title     = {Survey Attrition and Schooling Choices},
  abstract  = {We use data from three cohorts of the National
          Longitudinal Surveys of Labor Market Experience and from
          the Panel Study of Income Dynamics to study the effect of
          survey attrition on estimates of statistical models of
          schooling choices. We estimate regressions using data on
          people who always respond to the surveys (stayers) and on
          people who miss some surveys (attritors) and test whether
          the same statistical model describes the behavior of
          stayers and attritors. In general (with a few exceptions)
          we find that attrition either has no effect on the
          regression estimates or only affects the estimates of the
          intercept (and sometimes the coefficients of birth year
          dummies) and does not affect estimates of family background
          slope coefficients.},
  doi       = {10.2307/146440},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {531--554},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@TechReport{fan:zhang:zhang:2002,
  author    = "Fan, Shenggen and Zhang, Linxiu and Zhang, Xiaobo",
  year      = 2002,
  title     = "Growth, inequality, and poverty in rural China",
  institution   = "International Food Policy Research Institute (IFPRI)",
  type      = {Research report},
  note      = {RePEc handle: RePEc:fpr:resrep:125},
  number    = {125}
}

@Book{fang:zhang:1990,
  author    = {K. T. Fang and Y. T. Zhang},
  year      = {1990},
  title     = {Generalized Multivariate Analysis},
  publisher = {Springer},
  address   = {New York}
}

@article{faure:lemieux:2009,
   author = {Faure, Henri and Lemieux, Christiane},
   title = {Generalized Halton sequences in 2008: A comparative study},
   journal = {ACM Transactions on Modeling and Compututer Simulations},
   volume = {19},
   number = {4},
   year = {2009},
   pages = {1--31},
   doi = {10.1145/1596519.1596520},
   publisher = {ACM},
   address = {New York, NY, USA},
}

@INPROCEEDINGS{fay:train:1995,
  AUTHOR =       {Fay, R. E. and G. Train},
  TITLE =        {Aspects of survey and model-based postcensal estimation of income
                  and poverty characteristics for states and counties},
  BOOKTITLE =    {Proceedings of the Government Statistics Section},
  YEAR =         {1995},
  pages =        {154--159},
  organization = {The American Statistical Association},
}


@Article{fay:1996,
  author    = {Fay, Robert E. },
  year      = {1996},
  title     = {Alternative Paradigms for the Analysis of Imputed Survey
          Data},
  journal   = {Journal of the American Statistical Association},
  number    = {434},
  pages     = {490--498},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28199606%2991%3A434%3C490%3AAPFTAO%3E2.0.CO%3B2-9}
          ,
  volume    = {91},
  abstract  = {Rubin has offered multiple imputation as a general
          approach to inference from survey data sets with missing
          values filled in through imputation. In many situations the
          multiple imputation variance estimator is consistent. In
          turn, this observation has lent support to a number of
          complex applications. In fact, however, the multiple
          imputation variance estimator is inconsistent under some
          simple conditions. This article extends previous work of
          Rao and Shao and of Fay directed toward consistent variance
          estimation under wider conditions. Extensions of Rao and
          Shao's results to fractionally weighted imputation combines
          the estimation efficiency of multiple imputation and the
          consistency of the Rao-Shao variance estimator.}
}

@Article{fay:herriot:1979,
  author    = {Fay, Robert E. and Herriot, Roger A. },
  year      = {1979},
  title     = {Estimates of Income for Small Places: An Application of
          {J}ames-{S}tein Procedures to Census Data},
  journal   = {Journal of the American Statistical Association},
  number    = {366},
  pages     = {269--277},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28197906%2974%3A366%3C269%3AEOIFSP%3E2.0.CO%3B2-E}
          ,
  volume    = {74},
  abstract  = {An adaptation of the James-Stein estimator is applied to
          sample estimates of income for small places (i.e.,
          population less than 1,000) from the 1970 Census of
          Population and Housing. The adaptation incorporates linear
          regression in the context of unequal variances. Evidence is
          presented that the resulting estimates have smaller average
          error than either the sample estimates or an alternate
          procedure of using county averages. The new estimates for
          these small places now form the basis for the Census
          Bureau's updated estimates of per capita income for the
          General Revenue Sharing Program.}
}

@Article{fayers:hand:2002,
  author    = {Fayers, Peter M. and Hand, David J. },
  year      = {2002},
  title     = {Causal Variables, Indicator Variables and Measurement
          Scales: An Example from Quality of Life},
  doi       = {10.2307/3559926},
  journal   = {Journal of the Royal Statistical Society. Series A
          (Statistics in Society)},
  number    = {2},
  pages     = {233--261},
  abstract  = {There is extensive literature on the development and
          validation of multi-item measurement scales. Much of this
          is based on principles derived from psychometric theory and
          assumes that the individual items form parallel tests, so
          that simple weighted or unweighted summation is an
          appropriate method of aggregation. More recent work either
          continues to promulgate these methods or places emphasis on
          modern techniques centred on item response theory. In fact,
          however, clinical measuring instruments often have
          different underlying principles, so adopting such
          approaches is inappropriate. We illustrate, using
          health-related quality of life, that clinimetric and
          psychometric ideas need to be combined to yield a suitable
          measuring instrument. We note the fundamental distinction
          between indicator and causal variables and propose that
          this distinction suffices to explain fully the need for
          both clinimetric and psychometric techniques, and
          identifies their respective roles in scale development,
          validation and scoring.},
  volume    = {165}
}

@Article{feder:1968,
  author    = {Feder, Paul I. },
  year      = {1968},
  title     = {On the Distribution of the Log Likelihood Ratio Test
          Statistic When the True Parameter is "Near" the Boundaries
          of the Hypothesis Regions},
  journal   = {The Annals of Mathematical Statistics},
  number    = {6},
  pages     = {2044--2055},
  volume    = {39}
}

@BOOK{fedorov:1972,
  AUTHOR =       {V. V. Fedorov},
  TITLE =        {Theory of Optimal Experiments},
  PUBLISHER =    {Academic Press},
  YEAR =         {1972},
}


@Book{feller:1968,
  author    = {William Feller},
  year      = {1968},
  title     = {An Introduction to Probability Theory and Its
          Applications},
  publisher = {Wiley},
  volume    = {1},
  address   = {New York}
}

@Article{feng:mcculloch:1994,
  author    = {Feng, Z. D. and Mc{C}ulloch, C. E. },
  year      = {1994},
  title     = {On the Likelihood Ratio Test Statistic for the Number of
          Components in a Normal Mixture with Unequal Variances},
  doi       = {10.2307/2533453},
  journal   = {Biometrics},
  number    = {4},
  pages     = {1158--1162},
  volume    = {50},
  abstract  = {An important but difficult problem in practice is to
          determine the number of components in a normal mixture
          model with unequal variances. When the likelihood ratio
          test statistic -2log? is used, it is unbounded above and
          fails to satisfy standard regularity conditions. A
          restricted maximization procedure must therefore be used,
          which makes the procedure ad hoc. A consequence of this may
          explain the discrepancies among the simulation results of
          previous investigations.}
}

@Article{feng:mcculloch:1996,
  author    = {Feng, Z. D. and Mcculloch, C. E. },
  title     = {Using Bootstrap Likelihood Ratios in Finite Mixture
          Models},
  doi       = {10.2307/2345897},
  journal   = {Journal of the Royal Statistical Society, Series B},
  number    = {3},
  pages     = {609--617},
  volume    = {58},
  abstract  = {Statistical inference using the likelihood ratio statistic
          for the number of components in a mixture model is
          complicated when the true number of components is less than
          that of the proposed model since this represents a
          non-regular problem: the true parameter is on the boundary
          of the parameter space and in some cases the true parameter
          is in a non-identifiable subset of the parameter space. The
          maximum likelihood estimator is shown to converge to the
          subset characterized by the same density function, and
          connection is made to the bootstrap method proposed by
          Aitkin and co-workers and McLachlan for testing the number
          of components in a finite mixture and deriving confidence
          regions in a finite mixture.}
}

@Article{ferger:1995,
  author    = {Ferger, D. },
  year      = {1995},
  title     = {Nonparametric Tests for Nonstandard Change-Point
          Problems},
  doi       = {10.2307/2242548},
  journal   = {The Annals of Statistics},
  number    = {5},
  pages     = {1848--1861},
  volume    = {23}
}

@article{ferguson:1958,
    author = {Ferguson, Thomas S.},
    doi = {10.2307/2236945},
    issn = {00034851},
    journal = {The Annals of Mathematical Statistics},
    number = {4},
    pages = {1046--1062},
    publisher = {Institute of Mathematical Statistics},
    title = {A Method of Generating Best Asymptotically Normal Estimates
             with Application to the Estimation of Bacterial Densities},
    volume = {29},
    year = {1958},
    abstract =
      {Various minimum \$\chi^2\$ methods used for generating
       B.A.N. estimates are summarized, and a new method which
       generates B.A.N. estimates as roots of certain linear forms
       is introduced and investigated. As a particular application
       of the method, the estimation of the bacterial density in an
       experiment using dilution series is considered.},
}

@Book{ferguson:1996,
  author    = {Thomas S. Ferguson},
  year      = {1996},
  title     = {A Course in Large Sample Theory},
  publisher = {Chapman \& Hall/CRC},
  series    = {Texts in Statistical Science},
  address   = {London}
}

@Article{fernald:2007,
  author    = {Fernald, L. C. H.},
  year      = {2007},
  title     = {Socio-economic status and body mass index in low-income
          {M}exican adults},
  journal   = {Social Science and Medicine},
  volume    = {64},
  number    = {10},
  pages     = {2030--2042}
}

@Article{fgt:1984,
  author    = "James Foster and Joel Greer and Erik Thorbecke",
  year      = 1984,
  title     = "A Class of Decomposable Poverty Indices",
  journal   = "Econometrica",
  volume    = 52,
  pages     = "761--766"
}

@Article{field:welsh:2007,
  author    = {Field, C. A. and Welsh, A. H.},
  year      = {2007},
  title     = {Bootstrapping clustered data},
  doi       = {10.1111/j.1467-9868.2007.00593.x},
  issn      = {1369-7412},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {3},
  pages     = {369--390},
  publisher = {Blackwell Publishing},
  volume    = {69},
  abstract  = {Various bootstraps have been proposed for bootstrapping
          clustered data from one-way arrays. The simulation results
          in the literature suggest that some of these methods work
          quite well in practice; the theoretical results are limited
          and more mixed in their conclusions. For example, McCullagh
          reached negative conclusions about the use of
          non-parametric bootstraps for one-way arrays. The purpose
          of this paper is to extend our understanding of the issues
          by discussing the effect of different ways of modelling
          clustered data, the criteria for successful bootstraps used
          in the literature and extending the theory from functions
          of the sample mean to include functions of the between and
          within sums of squares and non-parametric bootstraps to
          include model-based bootstraps. We determine that the
          consistency of variance estimates for a bootstrap method
          depends on the choice of model with the residual bootstrap
          giving consistency under the transformation model whereas
          the cluster bootstrap gives consistent estimates under both
          the transformation and the random-effect model. In addition
          we note that the criteria based on the distribution of the
          bootstrap observations are not really useful in assessing
          consistency.}
}

@Article{filmer:pritchett:2001,
  author    = "Deon Filmer and Lant Pritchett",
  year      = 2001,
  title     = "Estimating Wealth Effect without Expenditure Data --- or
          Tears: An Application to Educational Enrollments in States
          of {India}",
  journal   = "Demography",
  volume    = 38,
  pages     = "115--132"
}

@TechReport{filmer:pritchett:tears,
  author    = "Deon Filmer and Lant Pritchett",
  year      = 1998,
  title     = "Estimating Wealth Effect without Expenditure Data --- or
          Tears: An Application to Educational Enrollments in States
          of {I}ndia",
  institution   = "The World Bank",
  number    = "No. 1994",
  type      = "World Bank Policy Research Working Paper",
  address   = "Washington, DC"
}

@Article{firth:1993,
  author    = {Firth, David},
  year      = {1993},
  title     = {Bias reduction of maximum likelihood estimates},
  abstract  = {It is shown how, in regular parametric problems, the
          first-order term is removed from the asymptotic bias of
          maximum likelihood estimates by a suitable modification of
          the score function. In exponential families with canonical
          parameterization the effect is to penalize the likelihood
          by the Jeffreys invariant prior. In binomial logistic
          models, Poisson log linear models and certain other
          generalized linear models, the Jeffreys prior penalty
          function can be imposed in standard regression software
          using a scheme of iterative adjustments to the data.
          10.1093/biomet/80.1.27},
  doi       = {10.1093/biomet/80.1.27},
  journal   = {Biometrika},
  number    = {1},
  pages     = {27--38},
  volume    = {80}
}

@Article{fisher:1925,
  author    = "R. A. Fisher",
  year      = 1925,
  title     = "Theory of Statistical Estimation",
  journal   = "Proceedings of the Cambridge Philosophical Society",
  volume    = 22,
  pages     = "700--"
}

@Book{fisher:1971,
  author    = "R. A. Fisher",
  year      = 1971,
  title     = "The Design of Experiments",
  edition   = "8th",
  publisher = "Hafner Publishing",
  address   = "New York"
}

@Article{fitzg:gotts:moffi:1998a,
  author    = {Fitzgerald, John and Gottschalk, Peter and Moffitt,
          Robert},
  year      = {1998},
  title     = {An Analysis of Sample Attrition in Panel Data: The
          Michigan Panel Study of Income Dynamics},
  abstract  = {By 1989 the Michigan Panel Study on Income Dynamics (PSID)
          had experienced approximately 50 percent sample loss from
          cumulative attrition from its initial 1968 membership. We
          study the effect of this attrition on the unconditional
          distributions of several socioeconomic variables and on the
          estimates of several sets of regression coefficients. We
          provide a statistical framework for conducting tests for
          attrition bias that draws a sharp distinction between
          selection on unobservables and on observables and that
          shows that weighted least squares can generate consistent
          parameter estimates when selection is based on observables,
          even when they are endogenous. Our empirical analysis shows
          that attrition is highly selective and is concentrated
          among lower socioeconomic status individuals. We also show
          that attrition is concentrated among those with more
          unstable earnings, marriage, and migration histories.
          Nevertheless, we find that these variables explain very
          little of the attrition in the sample, and that the
          selection that occurs is moderated by
          regression-to-the-mean effects from selection on transitory
          components that fade over time. Consequently, despite the
          large amount of attrition, we find no strong evidence that
          attrition has seriously distorted the representativeness of
          the PSID through 1989, and considerable evidence that its
          cross-sectional representativeness has remained roughly
          intact.},
  doi       = {10.2307/146433},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {251--299},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Article{fitzg:gotts:moffi:1998b,
  author    = {Fitzgerald, John and Gottschalk, Peter and Moffitt,
          Robert},
  year      = {1998},
  title     = {An Analysis of the Impact of Sample Attrition on the
          Second Generation of Respondents in the Michigan Panel
          Study of Income Dynamics},
  doi       = {10.2307/146434},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {300--344},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Article{fitzm:2003,
  author    = {Fitzmaurice, G. M.},
  year      = {2003},
  title     = {Methods for handling dropouts in longitudinal clinical
          trials},
  abstract  = {This paper focuses on the monotone missing data patterns
          produced by dropouts and presents a review of the
          statistical literature on approaches for handling dropouts
          in longitudinal clinical trials. A variety of ad hoc
          procedures for handling dropouts are widely used. The
          rationale for many of these procedures is not well-founded
          and they can result in biased estimates of treatment
          comparisons. A fundamentally difficult problem arises when
          the probability of dropout is thought to be related to the
          specific value that in principle should have been obtained;
          this is often referred to as informative or non-ignorable
          dropout. Joint models for the longitudinal outcomes and the
          dropout times have been proposed in order to make
          corrections for non-ignorable dropouts. Two broad classes
          of joint models are reviewed: selection models and
          pattern-mixture models. Finally, when there are dropouts in
          a longitudinal clinical trial the goals of the analysis
          need to be clearly specified. In this paper we review the
          main distinctions between a "pragmatic" and an
          "explanatory"" analysis. We note that many of the
          procedures for handling dropouts that are widely used in
          practice come closest to producing an explanatory rather
          than a pragmatic analysis. {\copyright} VVS, 2003.},
  address   = {Department of Biostatistics, Harvard School of Public
          Health, 655 Huntington Avenue, Boston, MA 02115, United
          States},
  doi       = {10.1111/1467-9574.00222},
  journal   = {Stat. Neerl.},
  number    = {1},
  pages     = {75--99},
  volume    = {57}
}

@Article{fitzm:heath:cliff:1996,
  author    = {Fitzmaurice, Garrett M. and Heath, Anthony F. and
          Clifford, Peter},
  year      = {1996},
  title     = {Logistic Regression Models for Binary Panel Data with
          Attrition},
  abstract  = {We discuss ways of analysing panel data when the response
          is binary and there is attrition or drop-out. In general,
          informative or non-ignorable drop-out models are
          non-identifiable and arbitrary constraints on the drop-out
          model must be imposed before carrying out a statistical
          analysis. The problem is particularly acute when predictors
          as well as response variables are lost by attrition. We
          describe a likelihood-based method for dealing with the
          drop-out process in this difficult case and show how the
          effect of non-identifiability can be reduced by importing
          additional data from a cross-sectional survey of the same
          population. The methods are primarily motivated by data
          from the 1987-92 British Election Panel Study and the 1992
          British Election Study.},
  doi       = {10.2307/2983172},
  issn      = {09641998},
  journal   = {Journal of the Royal Statistical Society. Series A
          (Statistics in Society)},
  number    = {2},
  pages     = {249--263},
  publisher = {Blackwell Publishing for the Royal Statistical Society},
  volume    = {159}
}

@Article{fitzm:laird:2000,
  author    = {Fitzmaurice, Garrett M. and Laird, Nan M.},
  year      = {2000},
  title     = {Generalized linear mixture models for handling
          nonignorable dropouts in longitudinal studies},
  abstract  = {This paper presents a method for analysing longitudinal
          data when there are dropouts. In particular, we develop a
          simple method based on generalized linear mixture models
          for handling nonignorable dropouts for a variety of
          discrete and continuous outcomes. Statistical inference for
          the model parameters is based on a generalized estimating
          equations (GEE) approach (Liang and Zeger, 1986). The
          proposed method yields estimates of the model parameters
          that are valid when nonresponse is nonignorable under a
          variety of assumptions concerning the dropout process.
          Furthermore, the proposed method can be implemented using
          widely available statistical software. Finally, an example
          using data from a clinical trial of contracepting women is
          used to illustrate the methodology.
          10.1093/biostatistics/1.2.141},
  doi       = {10.1093/biostatistics/1.2.141},
  journal   = {Biostat},
  number    = {2},
  pages     = {141--156},
  volume    = {1}
}

@ARTICLE{fleishman:1978,
  AUTHOR =       {Fleishman, A. I.},
  TITLE =        {A method for simulating nonnormal distributions},
  JOURNAL =      {Psychometrika},
  YEAR =         {1978},
  volume =       {43},
  pages =        {521--532},
}


@Book{fletcher:1980,
  author    = {R. Fletcher},
  year      = {1980},
  title     = {Practical Methods of Optimization},
  publisher = {Wiley Interscience},
  address   = {New York}
}

@Book{flury:1988,
  author    = "Bernard Flury",
  year      = 1988,
  title     = "Common Principal Components and Related Multivariate
          Methods",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Book{fomby:hill:2003,
  editor    = {Thomas B. Fomby and R. Carter Hill},
  year      = {2003},
  title     = {Maximum Likelihood Estimation of Misspecified Models:
          Twenty Years Later},
  publisher = {Elsevier},
  address   = {New York}
}

@ARTICLE{foldnes:olsson:foss:2011,
  AUTHOR =       {Nj{\o a}l Foldnes and Ulf Henning Olsson and Tron Foss},
  TITLE =        {The effect of kurtosis on the power of two test statistics in covariance structure analysis},
  JOURNAL =      {British Journal of Mathematical and Statistical Psychology},
  YEAR =         {2011},
  volume =       {forthcoming},
  number =       {??},
  pages =        {??--??},
}


@Article{forster:smith:1998,
  author    = {Forster, Jonathan J. and Smith, Peter W. F. },
  year      = {1998},
  title     = {Model-based inference for categorical survey data subject
          to non-ignorable non-response},
  doi       = {10.1111/1467-9868.00108},
  journal   = {Journal of the Royal Statistical Society: Series B:
          Statistical Methodology},
  number    = {1},
  pages     = {57--70},
  volume    = {60},
  abstract  = {We consider non-response models for a single categorical
          response with categorical covariates whose values are
          always observed. We present Bayesian methods for ignorable
          models and a particular non-ignorable model, and we argue
          that standard methods of model comparison are inappropriate
          for comparing ignorable and non-ignorable models.
          Uncertainty about ignorability of non-response is
          incorporated by introducing parameters describing the
          extent of non-ignorability into a pattern mixture
          specification and integrating over the prior uncertainty
          associated with these parameters. Our approach is
          illustrated using polling data from the 1992 British
          general election panel survey. We suggest sample size
          adjustments for surveys when non-ignorable non-response is
          expected.}
}

@Article{forsythe:1996,
  author    = "Frank P. Forsythe",
  year      = 1996,
  title     = "Viewing the University Gap",
  journal   = "Education Economics",
  volume    = 4,
  pages     = "45--63"
}

@Article{fost:gree:thor:1984,
  author    = "Foster, James and Greer, Joel and Thorbecke, Erik",
  year      = 1984,
  title     = "A Class of Decomposable Poverty Measures",
  journal   = "Econometrica",
  volume    = {52},
  number    = {3},
  pages     = {761--66}
}

@Article{foster:greer:thorbecke:1984,
  author    = {Foster, James and Greer, Joel and Thorbecke, Erik},
  year      = {1984},
  title     = {A Class of Decomposable Poverty Measures},
  doi       = {10.2307/1913475},
  journal   = {Econometrica},
  number    = {3},
  pages     = {761--766},
  volume    = {52}
}

@Article{foster:shora:1991,
  author    = "James E. Foster and Anthony F. Shorrocks",
  year      = 1991,
  title     = "Subgroup consistent poverty indices",
  journal   = "Econometrica",
  volume    = 59,
  issue     = 3,
  pages     = "687--709"
}

@Book{fox:1997,
  author    = "Fox, J.",
  year      = 1997,
  title     = "Applied Regression Analysis, Linear Models, And Related
          Methods",
  publisher = "SAGE",
  address   = "Thousand Oaks, CA"
}

@Book{fox:1999:qmc,
  author    = {Fox, Bennett L.},
  year      = {1999},
  title     = {Strategies for Quasi-{Monte} {Carlo}},
  comment   = {The book deals extensively with non-smooth and discrete
          problems that can be solved using quasi-Monte Carlo. Also
          the idea of separating the variates into important ones to
          be sampled by QMC, and auxiliary ones, to be sampled by
          random MC, is pursued throughout.},
  howpublished  = {ISBN 978-0-7923-8580-6},
  publisher = {Springer}
}

@TechReport{fral:raft:1998,
  author    = "Chris Fraley and Adrian E. Raftery",
  year      = 1998,
  title     = "How Many Clusters? {W}hich Clustering Method? {A}nswers
          Via Model-Based Cluster Analysis",
  institution   = "University of Washington, Department of Statistics",
  type      = "Technical Report",
  number    = 329
}

@Article{fral:raft:1998:cj,
  author    = {Fraley, Chris and Raftery, Adrian E. },
  year      = {1998},
  title     = {How Many Clusters? Which Clustering Method? Answers Via
          Model-Based Cluster Analysis},
  journal   = {The Computer Journal},
  doi       = {doi:10.1093/comjnl/41.8.578 },
  number    = {8},
  pages     = {578--588},
  volume    = {41},
  abstract  = { We consider the problem of determining the structure of
          clustered data, without prior knowledge of the number of
          clusters or any other information about their composition.
          Data are represented by a mixture model in which each
          component corresponds to a different cluster. Models with
          varying geometric properties are obtained through Gaussian
          components with different parametrizations and
          cross-cluster constraints. Noise and outliers can be
          modelled by adding a Poisson process component. Partitions
          are determined by the expectation-maximization (EM)
          algorithm for maximum likelihood, with initial values from
          agglomerative hierarchical clustering. Models are compared
          using an approximation to the Bayes factor based on the
          Bayesian information criterion (BIC); unlike significance
          tests, this allows comparison of more than two models at
          the same time, and removes the restriction that the models
          compared be nested. The problems of determining the number
          of clusters and the clustering method are solved
          simultaneously by choosing the best model. Moreover, the EM
          result provides a measure of uncertainty about the
          associated classification of each data point. Examples are
          given, showing that this approach can give performance that
          is much better than standard procedures, which often fail
          to identify groups that are either overlapping or of
          varying sizes and shapes. }
}

@TechReport{fral:raft:1998:tech,
  author    = "Chris Fraley and Adrian E. Raftery",
  year      = 1998,
  title     = "How Many Clusters? {W}hich Clustering Method? {A}nswers
          Via Model-Based Cluster Analysis",
  institution   = "University of Washington, Department of Statistics",
  type      = "Technical Report",
  number    = 329
}

@Article{fraley1998how,
  author    = {Fraley, Chris and Raftery, Adrian E.},
  year      = {1998},
  title     = {How Many Clusters? Which Clustering Method? Answers Via
          Model-Based Cluster Analysis},
  abstract  = {We consider the problem of determining the structure of
          clustered data, without prior knowledge of the number of
          clusters or any other information about their composition.
          Data are represented by a mixture model in which each
          component corresponds to a different cluster. Models with
          varying geometric properties are obtained through Gaussian
          components with different parametrizations and
          cross-cluster constraints. Noise and outliers can be
          modelled by adding a Poisson process component. Partitions
          are determined by the expectation-maximization (EM)
          algorithm for maximum likelihood, with initial values from
          agglomerative hierarchical clustering. Models are compared
          using an approximation to the Bayes factor based on the
          Bayesian information criterion (BIC); unlike significance
          tests, this allows comparison of more than two models at
          the same time, and removes the restriction that the models
          compared be nested. The problems of determining the number
          of clusters and the clustering method are solved
          simultaneously by choosing the best model. Moreover, the EM
          result provides a measure of uncertainty about the
          associated classification of each data point. Examples are
          given, showing that this approach can give performance that
          is much better than standard procedures, which often fail
          to identify groups that are either overlapping or of
          varying sizes and shapes.},
  comment   = {The authors propose a flexible method of fitting a variety
          of normal mixture models allowing for identical or
          different (i) shape (ratios of eigenvalues), (ii) size (the
          magnitude of eigenvalues), and (iii) orientation
          (eigenvectors) of the components of a mixture. Very neat
          idea. See also
          http://www.citeulike.org/user/ctacmo/article/781684 for
          later add-ons and developments.},
  doi       = {doi:10.1093/comjnl/41.8.578},
  journal   = {The Computer Journal},
  number    = {8},
  pages     = {578--588},
  volume    = {41}
}

@Article{fraley:raftery:2002,
  author    = {Fraley, C. and Raftery, A.},
  year      = {2002},
  title     = {Model-Based Clustering, Discriminant Analysis, and Density
          Estimation},
  abstract  = {Cluster analysis is the automated search for groups of
          related observations in a dataset. Most clustering done in
          practice is based largely on heuristic but intuitively
          reasonable procedures, and most clustering methods
          available in commercial software are also of this type.
          However, there is little systematic guidance associated
          with these methods for solving important practical
          questions that arise in cluster analysis, such as how many
          clusters are there, which clustering method should be used,
          and how should outliers be handled. We review a general
          methodology for model-based clustering that provides a
          principled statistical approach to these issues. We also
          show that this can be useful for other problems in
          multivariate analysis, such as discriminant analysis and
          multivariate density estimation. We give examples from
          medical diagnosis, minefield detection, cluster recovery
          from noisy data, and spatial density estimation. Finally,
          we mention limitations of the methodology and discuss
          recent developments in model-based clustering for
          non-Gaussian data, high-dimensional datasets, large
          datasets, and Bayesian estimation.},
  comment   = {This is a further development of the earlier work (Fraley
          and Raftery 1998;
          http://www.citeulike.org/user/ctacmo/article/543355) to
          include relations to discriminant analysis and density
          estimation. Earlier version came out as a Working Paper @
          Washington.},
  journal   = {Journal of the American Statistical Association},
  number    = {458},
  pages     = {611--631},
  volume    = {97}
}

@Article{francisco:fuller:1991,
  author    = {Francisco, Carol A. and Fuller, Wayne A.},
  year      = {1991},
  title     = {Quantile Estimation with a Complex Survey Design},
  abstract  = {Estimation of the finite population distribution function
          and related statistics, such as the median and
          interquartile range, is considered. Large-sample properties
          of estimators constructed from stratified cluster samples,
          and properties of large-sample confidence intervals, are
          established. The results are obtained within the context of
          a sequence of finite populations generated from a
          superpopulation.},
  doi       = {10.2307/2241867},
  issn      = {00905364},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {454--469},
  publisher = {Institute of Mathematical Statistics},
  volume    = {19}
}

@InBook{freedman:2005,
  author    = {Freedman, David A.},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {On specifying graphical models for causation and the
          identification problems},
  abstract  = {This paper (which is mainly expository) sets up graphical
          models for causation, having a bit less than the usual
          complement of hypothetical counterfactuals. Assuming the
          invariance of error distributions may be essential for
          causal inference, but the errors themselves need not be
          invariant. Graphs can be interpreted using conditional
          distributions, so that we can better address connections
          between the mathematical framework and causality in the
          world. The identification problem is posed in terms of
          conditionals. As will be seen, causal relationships cannot
          be inferred from a data set by running regressions unless
          there is substantial prior knowledge about the mechanisms
          that generated the data. The idea can be made more precise
          in several ways. There are few successful applications of
          graphical models, mainly because few causal pathways can be
          excluded on a priori grounds. The invariance conditions
          themselves remain to be assessed.},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {4},
  comment   = {For an SEM person, a pretty funny note very cautiously
          looking at the graphical ways to specify causal relations.
          Economists, do it!},
  publisher = {Cambridge University Press}
}

@Article{freedman:2006,
  author    = {Freedman, David A. },
  year      = {2006},
  title     = {On the So-Called ``{H}uber Sandwich Estimator'' and
          ``Robust Standard Errors''},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {299--302},
  comment   = {http://dx.doi.org/10.1198/000313006X152207},
  volume    = {60}
}

@Article{freedman:2007:tas,
  author    = {Freedman, David A. },
  year      = 2007,
  title     = {How Can the Score Test Be Inconsistent?},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {291--295},
  volume    = {61},
  abstract  = {The score test can be inconsistent because at the MLE
          under the null hypothesis the observed information matrix
          generates negative variance estimates. The test can also be
          inconsistent if the expected likelihood equation has
          spurious roots.}
}

@Book{friedman:1957,
  author    = {Friedman, Milton},
  year      = {1957},
  title     = {A Theory of the Consumption Function},
  publisher = {Princeton University Press},
  address   = {Princeton, NJ, USA}
}


@article{friedman:stuetzle:1981,
author = { Jerome H.   Friedman  and  Werner   Stuetzle },
title = {Projection Pursuit Regression},
journal = {Journal of the American Statistical Association},
volume = {76},
number = {376},
pages = {817-823},
year = {1981},
doi = {10.1080/01621459.1981.10477729},
    abstract =
    {A new method for nonparametric multiple regression is presented. The
    procedure models the regression surface as a sum of general smooth
    functions of linear combinations of the predictor variables in an
    iterative manner. It is more general than standard stepwise and stagewise
    regression procedures, does not require the definition of a metric in the
    predictor space, and lends itself to graphical interpretation. }
}


@Book{fudenberg:tirole:1991,
  author    = {Fudenberg, Drew and Tirole, Jean },
  year      = {1991},
  title     = {Game Theory},
  publisher = {{MIT Press}},
  address   = {Cambridge, MA},
  comment   = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262061414}
          ,
  abstract  = {This advanced text introduces the principles of
          noncooperative game theory in a direct and uncomplicated
          style that will acquaint students with the broad spectrum
          of the field while highlighting and explaining what they
          need to know at any given point.}
}

@Article{fuentes:2002,
  author    = "Monteserrat Fuentes",
  year      = 2002,
  title     = "Spectral methods for nonstationary spatial processes",
  journal   = "Biometrika",
  volume    = 89,
  issue     = 1,
  pages     = "197--210"
}

@Book{fuller2006measurement,
  author    = {Fuller, Wayne A.},
  year      = {2006},
  title     = {Measurement Error Models},
  series    = {Wiley Series in Probability and Statistics},
  abstract  = {{The Wiley-Interscience Paperback Series consists of
          selected books that have been made more accessible to
          consumers in an effort to increase global appeal and
          general circulation. With these new unabridged softcover
          volumes, Wiley hopes to extend the lives of these works by
          making them available to future generations of
          statisticians, mathematicians, and scientists.<br> <br>
          "The effort of Professor Fuller is commendable . . . [the
          book] provides a complete treatment of an important and
          frequently ignored topic. Those who work with measurement
          error models will find it valuable. It is the fundamental
          book on the subject, and statisticians will benefit from
          adding this book to their collection or to university or
          departmental libraries."<br> -Biometrics<br> <br> "Given
          the large and diverse literature on measurement
          error/errors-in-variables problems, Fuller's book is most
          welcome. Anyone with an interest in the subject should
          certainly have this book."<br> -Journal of the American
          Statistical Association<br> <br> "The author is to be
          commended for providing a complete presentation of a very
          important topic. Statisticians working with measurement
          error problems will benefit from adding this book to their
          collection."<br> -Technometrics<br> <br> " . . . this book
          is a remarkable achievement and the product of impressive
          top-grade scholarly work."<br> -Journal of Applied
          Econometrics<br> <br> Measurement Error Models offers
          coverage of estimation for situations where the model
          variables are observed subject to measurement error.
          Regression models are included with errors in the
          variables, latent variable models, and factor models.
          Results from several areas of application are discussed,
          including recent results for nonlinear models and for
          models with unequal variances. The estimation of true
          values for the fixed model, prediction of true values under
          the random model, model checks, and the analysis of
          residuals are addressed, and in addition, procedures are
          illustrated with data drawn from nearly twenty real data
          sets.}},
  howpublished  = {Paperback},
  isbn      = {0470095717},
  publisher = {Wiley-Interscience}
}

@Article{fuller:1975,
  author    = {Wayne A. Fuller},
  year      = {1975},
  title     = {Regression analysis for sample survey},
  journal   = {Sankhya Series C},
  volume    = {37},
  pages     = {117--132},
  abstract  = { We investigate the estimation of regression equations for
          a sample selected from a finite population. In all
          derivations, the finite population is treated as a sample
          from an infinite population. The regression coefficients
          are shown to be asymptotically normal, given mild
          assumptions. Relatively simple expressions for the
          covariance matrix of the regression coefficients are
          presented. Procedures for estimating the structural
          parameters in the presence of response error are presented.
          Given knowledge of the response variance , the computations
          required to estimate the structural parameters and their
          standard errors are essentially equivalent to those
          required for the computation of the regression coefficient
          and its error in the absence of response error. }
}

@Book{fuller:1987,
  author    = "Wayne A. Fuller",
  year      = 1987,
  title     = "Measurement Error Models",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{fuller:1990,
  author    = "Wayne A. Fuller",
  year      = "1990",
  title     = "Analysis of Repeated Surveys",
  journal   = "Survey Methodology",
  volume    = "16",
  number    = "2",
  pages     = "167--180"
}

@Article{fuller:1999,
  author    = "Wayne A. Fuller",
  year      = "1999",
  title     = "Environmental Surveys over Time",
  journal   = "Journal of Agricultural, Biological and Environmental
          Statistics",
  volume    = "4",
  number    = "4",
  pages     = "331--345"
}

@Article{fuller:2002,
  author    = {Wayne A. Fuller},
  year      = {2002},
  title     = {Regression Estimation for Survey Samples (with
          discussion)},
  journal   = {Survey Methodology},
  volume    = {28},
  number    = {1},
  pages     = {5--23},
  abstract  = { Regression and regression related procedures have become
          common in survey estimation. We review the basic properties
          of regression estimators, discuss implementation of
          regression estimation, and investigate variance estimation
          for regression estimators. The role of models in
          constructing regression estimators and the use of
          regression in nonresponse adjustment are explored. }
}

@BOOK{fuller:2009,
  AUTHOR =       {Wayne A. Fuller},
  TITLE =        {Sampling Statistics},
  PUBLISHER =    {Wiley},
  YEAR =         {2009},
  address =      {Hoboken, New Jersey},
}


@Book{gallant:1987,
  author    = "A. Ronald Gallant",
  year      = 1987,
  title     = "Nonlinear Statistical Models",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@inproceedings{ganesh:2017:truenorth,
  title = {Combining Probability and Non-Probability Samples Using Small Area Estimation},
  author = {Nada Ganesh and Vicki Pineau and Adrijo Chakraborty and J. Michael Dennis},
  note = {Available at http://www.asasrms.org/Proceedings/y2017/files/593906.pdf},
  BOOKTITLE =    {Proceedings of the Survey Research Methods Section},
  YEAR =         {2017},
  address =      {Alexandria, VA},
  organization = {The American Statistical Association}
}

@article{gelfand:smith:1990,
    author = { Alan E.   Gelfand  and  Adrian F. M.   Smith },
    title = {Sampling-Based Approaches to Calculating Marginal Densities},
    journal = {Journal of the American Statistical Association},
    volume = {85},
    number = {410},
    pages = {398-409},
    year = {1990},
    doi = {10.1080/01621459.1990.10476213},
    abstract =
        {Stochastic substitution, the Gibbs sampler, and the
        sampling-importance-resampling algorithm can be viewed as three
        alternative sampling- (or Monte Carlo-) based approaches to the
        calculation of numerical estimates of marginal probability
        distributions. The three approaches will be reviewed, compared, and
        contrasted in relation to various joint probability structures
        frequently encountered in applications. In particular, the relevance
        of the approaches to calculating Bayesian posterior densities for a
        variety of structured models will be discussed and illustrated. }
}

@article{gelman:rubin:1995,
  title={Avoiding model selection in {Bayesian} social research},
  author={Gelman, Andrew and Rubin, Donald B},
  journal={Sociological methodology},
  volume={25},
  pages={165--173},
  year={1995},
}

@Book{gelman2003bayesian,
  author    = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and
          Rubin, Donald B.},
  year      = {2003},
  title     = {Bayesian Data Analysis, Second Edition (Texts in
          Statistical Science)},
  abstract  = {{Incorporating new and updated information, this second
          edition of THE bestselling text in Bayesian data analysis
          continues to emphasize practice over theory, describing how
          to conceptualize, perform, and critique statistical
          analyses from a Bayesian perspective. Its world-class
          authors provide guidance on all aspects of Bayesian data
          analysis and include examples of real statistical analyses,
          based on their own research, that demonstrate how to solve
          complicated problems. Changes in the new edition include:
          \&\#183;Stronger focus on MCMC\&\#183;Revision of the
          computational advice in Part III\&\#183;New chapters on
          nonlinear models and decision analysis\&\#183;Several
          additional applied examples from the authors' recent
          research\&\#183;Additional chapters on current models for
          Bayesian data analysis such as nonlinear models,
          generalized linear mixed models, and
          more\&\#183;Reorganization of chapters 6 and 7 on model
          checking and data collectionBayesian computation is
          currently at a stage where there are many reasonable ways
          to compute any given posterior distribution. However, the
          best approach is not always clear ahead of time. Reflecting
          this, the new edition offers a more pluralistic
          presentation, giving advice on performing computations from
          many perspectives while making clear the importance of
          being aware that there are different ways to implement any
          given iterative simulation computation. The new approach,
          additional examples, and updated information make Bayesian
          Data Analysis an excellent introductory text and a
          reference that working scientists will use throughout their
          professional life.}},
  edition   = {2},
  howpublished  = {Hardcover},
  isbn      = {158488388X},
  publisher = {Chapman \& Hall/CRC}
}

@Book{gelman:carlin:stern:rubin:2004,
  author    = {Andrew Gelman and John B. Carlin and Hal B. Stern and
          Donald B. Rubin},
  year      = {2004},
  title     = {Bayesian Data Analysis},
  publisher = {Chapman \& Hall/CRC},
  address   = {Boca Raton, FL},
  edition   = {2nd}
}

@BOOK{gelman:hill:2006,
  AUTHOR =       {Andrew Gelman and Jennifer Hill},
  TITLE =        {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  PUBLISHER =    {Cambridge University Press},
  YEAR =         {2006},
  address =      {New York},
}

@article{gelman:little:1997,
    author = {Gelman, A. and Little, T. C.},
    journal = {Survey Methodology},
    pages = {127--135},
    title = {Poststratification into many categories using hierarchical logistic regression},
    volume = {23},
    year = {1997}
}

@Book{genton:2004,
  year      = {2004},
  title     = {Skew-Elliptical Distributions and Their Applications: A
          Journey Beyond Normality},
  abstract  = {In recent years, research has generated important advances
          in theory and applications of skew-elliptical
          distributions. This is an exciting and fast- growing field
          of research that brings together both frequentist and
          Bayesian statisticians. Along with an explosion of interest
          in this new area of research has come a virtually unlimited
          potential for applications. This book reviews the
          state-of-the-art advances in skew-elliptical distributions
          and provides many new developments in a single volume,
          collecting theoretical results and applications previously
          scattered throughout the literature. The main goal of this
          research area is to develop flexible parametric classes of
          distributions beyond the classical normal distribution. The
          book is divided into two parts. The first part discusses
          theory and inference for skew- elliptical distribution. The
          second part presents applications and case studies in areas
          such as biostatistics, finance, oceanography, environmental
          science, and engineering. Each chapter is authored by
          leading specialists in key areas of research. With a fine
          balance between theory and application, this book is
          intended for a large readership, including statisticians
          and practitioners from other fields. With exceptional
          cohesion for a contributed work, Skew-Elliptical
          Distributions and Their Applications offers a unique
          opportunity to explore this exciting new field, understand
          its most recent advances, and apply them to your own area
          of research and applications.},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {1584884312},
  publisher = {Chapman \& Hall/CRC}
}

@Article{geweke:1999,
  author    = {John Geweke},
  year      = {1999},
  title     = {Using Simulation Methods for {Bayesian} Econometric Models:
          Inference, Development and Communication (with discussion
          and rejoinder)},
  journal   = {Econometric Reviews},
  volume    = {18},
  pages     = {1--126}
}

@Article{geweke:singleton:1980,
  author    = {Geweke, John F. and Singleton, Kenneth J. },
  year      = {1980},
  title     = {Interpreting the Likelihood Ratio Statistic in Factor
          Models when Sample Size is Small},
  journal   = {Journal of the American Statistical Association},
  number    = {369},
  pages     = {133--137},
  volume    = {75},
  abstract  = {The use of the likelihood ratio statistic in testing the
          goodness of fit of the exploratory factor model has no
          formal justification when, as is often the case in
          practice, the usual regularity conditions are not met. In a
          Monte Carlo experiment it is found that the asymptotic
          theory seems to be appropriate when the regularity
          conditions obtain and sample size is at least 30. When the
          regularity conditions are not satisfied, the asymptotic
          theory seems to be misleading in all sample sizes
          considered.}
}

@article{geyer:1992,
    author = {Geyer, Charles J.},
    doi = {10.1214/ss/1177011137},
    issn = {0883-4237},
    journal = {Statistical Science},
    number = {4},
    pages = {473--483},
    title = {Practical {Markov} Chain {Monte} {Carlo}},
    volume = {7},
    year = {1992}
}
@Article{ghosh:nata:stroud:carlin:1998,
  author    = {Ghosh, Malay and Natarajan, Kannan and Stroud, T. W. F.
          and Carlin, Bradley P. },
  year      = {1998},
  title     = {Generalized Linear Models for Small-Area Estimation},
  journal   = {Journal of the American Statistical Association},
  number    = {441},
  pages     = {273--282},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28199803%2993%3A441%3C273%3AGLMFSE%3E2.0.CO%3B2-V}
          ,
  volume    = {93},
  abstract  = {Bayesian methods have been used quite extensively in
          recent years for solving small-area estimation problems.
          Particularly effective in this regard has been the
          hierarchical or empirical Bayes approach, which is
          especially suitable for a systematic connection of local
          areas through models. However, the development to date has
          mainly concentrated on continuous-valued variates. Often
          the survey data are discrete or categorical, so that
          hierarchical or empirical Bayes techniques designed for
          continuous variates are inappropriate. This article
          considers hierarchical Bayes generalized linear models for
          a unified analysis of both discrete and continuous data. A
          general theorem is provided that ensures the propriety of
          posteriors under diffuse priors. This result is then
          extended to the case of spatial generalized linear models.
          The hierarchical Bayes procedure is implemented via Markov
          chain Monte Carlo integration techniques. Two examples (one
          featuring spatial correlation structure) are given to
          illustrate the general method.}
}

@Article{ghosh:rao:1994,
  author    = {Ghosh, M. and Rao, J. N. K. },
  year      = {1994},
  title     = {Small Area Estimation: An Appraisal},
  journal   = {Statistical Science},
  number    = {1},
  pages     = {55--76},
  comment   = {http://links.jstor.org/sici?sici=0883-4237%28199402%299%3A1%3C55%3ASAEAA%3E2.0.CO%3B2-5}
          ,
  volume    = {9},
  abstract  = {Small area estimation is becoming important in survey
          sampling due to a growing demand for reliable small area
          statistics from both public and private sectors. It is now
          widely recognized that direct survey estimates for small
          areas are likely to yield unacceptably large standard
          errors due to the smallness of sample sizes in the areas.
          This makes it necessary to "borrow strength" from related
          areas to find more accurate estimates for a given area or,
          simultaneously, for several areas. This has led to the
          development of alternative methods such as synthetic,
          sample size dependent, empirical best linear unbiased
          prediction, empirical Bayes and hierarchical Bayes
          estimation. The present article is largely an appraisal of
          some of these methods. The performance of these methods is
          also evaluated using some synthetic data resembling a
          business population. Empirical best linear unbiased
          prediction as well as empirical and hierarchical Bayes, for
          most purposes, seem to have a distinct advantage over other
          methods.}
}

@Book{gifi:1990,
  author    = {Gifi, Albert},
  year      = {1990},
  title     = {Nonlinear multivariate analysis},
  pages     = {579},
  publisher = {John Wiley \& Sons}
}

@Article{giles:giles:1993,
  author    = {J. A. Giles and D. E. A. Giles},
  year      = {1993},
  title     = {Pre-Test Estimation and Testing in Econometrics: Recent
          Developments},
  journal   = {Journal of Economic Surveys},
  volume    = {7},
  pages     = {145--197}
}

@Article{giles:lieb:giles:1992,
  author    = {David E. A. Giles and Offer Lieberman and Judith A.
          Giles},
  year      = {1992},
  title     = {The Optimal Size of a Preliminary Test of Linear
          Restrictions in a Misspecified Regression Model},
  journal   = {Journal of the American Statistical Association},
  volume    = {87},
  number    = {420},
  pages     = {1153--1157},
  abstract  = {When the choice of estimator for the coefficients in a
          linear regression model is determined by the outcome of a
          prior test of the validity of restrictions on the model, it
          is well known that a minimax (risk) regret criterion leads
          to the simple rule that the optimal critical value for the
          preliminary test is approximately two in value, regardless
          of the degrees of freedom. We show that this result no
          longer holds in the (likely) event that relevant regressors
          are excluded from the model at the outset.}
}

@Book{gilks:rich:spieg:1996,
  editor    = {W. R. Gilks and S. Richardson and D. J. Spiegelhalter},
  year      = {1996},
  title     = {{Markov} Chain {Monte} {Carlo} in Practice},
  publisher = {Chapman \& Hall/CRC},
  series    = {Interdisciplinary Statistics Series},
  address   = {Boca Raton, FL}
}

@Book{gill:2007,
  author    = {Gill, Jeff},
  year      = {2007},
  title     = {Bayesian Methods: A Social and Behavioral Sciences
          Approach, Second Edition (Statistics in the Social and
          Behavioral Sciences)},
  edition   = {2},
  howpublished  = {Hardcover},
  isbn      = {1584885629},
  publisher = {Chapman \& Hall/CRC}
}

@book{gill:2014,
    author = {Gill, Jeff},
    edition = {3},
    publisher = {Chapman and Hall/CRC},
    series = {Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
    title = {Bayesian Methods: A Social and Behavioral Sciences Approach},
    year = {2014}
}

@article{gill:2008,
    abstract =
        {Increasingly, political science researchers are turning to Markov
        chain Monte Carlo methods to solve inferential problems with complex
        models and problematic data. This is an enormously powerful set of
        tools based on replacing difficult or impossible analytical work with
        simulated empirical draws from the distributions of interest.
        Although practitioners are generally aware of the importance of
        convergence of the Markov chain, many are not fully aware of the
        difficulties in fully assessing convergence across multiple
        dimensions. In most applied circumstances, every parameter dimension
        must be converged for the others to converge. The usual culprit is
        slow mixing of the Markov chain and therefore slow convergence
        towards the target distribution. This work demonstrates the partial
        convergence problem for the two dominant algorithms and illustrates
        these issues with empirical examples.},
    author = {Gill, Jeff},
    doi = {10.1093/pan/mpm019},
    journal = {Political Analysis},
    number = {2},
    pages = {153--178},
    title = {Is Partial-Dimension Convergence a Problem for Inferences from {MCMC} Algorithms?},
    volume = {16},
    year = {2008}
}

@article{gill:casella:2009,
    abstract =
        {A generalized linear mixed model, ordered probit, is used to
        estimate levels of stress in presidential political appointees as a
        means of understanding their surprisingly short tenures. A Bayesian
        approach is developed, where the random effects are modeled with a
        Dirichlet process mixture prior, allowing for useful incorporation of
        prior information, but retaining some vagueness in the form of the
        prior. Applications of Bayesian models in the social sciences are
        typically done with ?uninformative? priors, although some use of
        informed versions exists. There has been disagreement over this, and
        our approach may be a step in the direction of satisfying both camps.
        We give a detailed description of the data, show how to implement the
        model, and describe some interesting conclusions. The model utilizing
        a nonparametric prior fits better and reveals more information in the
        data than standard approaches.},
    author = {Gill, Jeff and Casella, George},
    doi = {10.1198/jasa.2009.0039},
    journal = {Journal of the American Statistical Association},
    number = {486},
    pages = {453--454},
    title = {Nonparametric Priors for Ordinal {Bayesian} Social Science Models: Specification and Estimation},
    volume = {104},
    year = {2009}
}

@InCollection{gill:2010,
  author    = {Gill, Jeff},
  year      = {2010},
  title     = {Critical Differences in {B}ayesian and Non-{B}ayesian Inference
               And Why the Former is Better},
  editor    = {Kolenikov, Stanislav and Steinley, Douglas and Thombs, Lori},
  booktitle = {Statistics in the Social Sciences: Current Methodological Developments},
  publisher = {John Wiley \& Sons}
}

@article{gill:witko:2013,
    abstract =
        {In this article we describe in detail the Bayesian perspective on
        statistical inference and demonstrate that it provides a more
        principled approach to modeling public administration data. Because
        many datasets in public administration are population-level, one-time
        unique collections, or descriptive of fluid events, the Bayesian
        reliance on probability as a description of unknown quantities is a
        superior paradigm than that borrowed from Frequentist methods in the
        natural sciences where experimentation is routine. Here we provide a
        thorough, but accessible, introduction to Bayesian methods and then
        demonstrate our points with data on interest group influence in {US}
        state administrative agencies},
    author = {Gill, Jeff and Witko, Christopher},
    doi = {10.1093/jopart/mus091},
    journal = {Journal of Public Administration Research and Theory},
    number = {2},
    pages = {457--494},
    title = {Bayesian Analytical Methods: A Methodological Prescription for Public Administration},
    volume = {23},
    year = {2013}
}

@Article{glan:paxt:2007,
  author    = {Glanville, Jennifer L. and Paxton, Pamela},
  year      = {2007},
  title     = {How do We Learn to Trust? A Confirmatory Tetrad Analysis
          of the Sources of Generalized Trust},
  issn      = {0190-2725},
  journal   = {Social Psychology Quarterly},
  number    = {3},
  pages     = {230--242},
  publisher = {American Sociological Association},
  volume    = {70}
}

@Article{gleason:1988,
  author    = {Gleason, John R.},
  year      = {1988},
  title     = {Algorithms for Balanced Bootstrap Simulations},
  abstract  = {Efron's nonparametric bootstrap method simulates the
          distributional properties of a statistic by repeated
          resampling of a given sample. A balanced bootstrap
          simulation is one in which each sample observation is
          reused exactly equally often. Three algorithms for balanced
          bootstrap sampling are described, and it is shown that a
          balanced bootstrap simulation costs little more than an
          ordinary unbalanced one.},
  doi       = {10.2307/2685134},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {263--266},
  volume    = {42}
}

@Book{glym:sche;spir;kell:1987,
  author    = {C. Glymour and R. Scheines and P. Spirtes and K. Kelly},
  year      = {1987},
  title     = {Discovering causal structure: Artificial intelligence,
          philosophy of science, and statistical modeling},
  publisher = {Academic},
  address   = {Orlando, Florida}
}

@Book{glym:spir:schie:1993,
  author    = {Glymour P. Spirtes and R. Schienes},
  year      = {1993},
  title     = {Causation, Prediction, and Search},
  publisher = {Springer-Verlag},
  address   = {New York}
}

@Article{gneiting:2002,
  author    = "Tilmann Gneiting",
  year      = 2002,
  title     = "Nonseparable, Stationary Covariance Functions for
          Space-Time Data",
  journal   = "The Journal of the American Statistical Association",
  volume    = 97,
  pages     = "590--601"
}

@Article{godambe:thompson:1978,
  author    = {Godambe, V. P. and Thompson, M.},
  year      = {1978},
  journal   = {Journal of Statistical Planning and Inference},
  title     = {Some aspects of the theory of estimating equations},
  pages     = {95--104},
  volume    = {2}
}

@Article{godambe:thompson:1984,
  author    = {Godambe, V. P. and Thompson, M. E. },
  year      = {1984},
  title     = {Robust Estimation Through Estimating Equations},
  doi       = {10.2307/2336403},
  journal   = {Biometrika},
  number    = {1},
  pages     = {115--125},
  volume    = {71},
  abstract  = {Utilizing the theory of estimating equations (Godambe,
          1960; Godambe & Thompson, 1978), this paper develops
          concepts of parameter defining function and effective
          parameter. The paper provides theory and techniques for
          choosing from a given set of robust parameters one that is
          most effective, or one that can most efficiently be
          estimated.}
}

@InCollection{godambe:thompson:2009,
  author    = {Godambe, V. P. and Thompson, M. E.},
  year      = {2009},
  title     = {Estimation Functions and Survey Sampling},
  booktitle = {Handbook of Statistics, Vol.\ 29B, Sample Surveys: Inference and Analysis},
  editor    = {Danny Pfeffermann and C R Rao},
  publisher = {North Holland}
}


@Article{godambe:thompson:1986,
  author    = {Godambe, V. P. and Thompson, M. E.},
  year      = {1986},
  title     = {Parameters of Superpopulation and Survey Population: Their
          Relationships and Estimation},
  abstract  = {Utilizing the theory of estimating functions (Godambe,
          1960; Godambe \&amp; Thompson, 1978, 1984), this paper
          relates superpopulation parameters with those of a survey
          population under study Further it establishes optimal
          estimation, simultaneously, of both types of parameters.},
  journal   = {International Statistical Review},
  number    = {2},
  pages     = {127--138},
  volume    = {54}
}



@Book{godfrey:1988,
  author    = {L. G. Godfrey},
  year      = {1988},
  title     = {Misspecification Tests in Econometrics},
  publisher = {Cambridge University Press},
  volume    = {16},
  series    = {Econometric Society Monographs},
  address   = {New York}
}

@Article{godfrey:orme:2001,
  author    = {Godfrey, L. G. and Orme, C. D.},
  year      = {2001},
  title     = {On improving the robustness and reliability of Rao's score
          test},
  abstract  = {The results of misspecification tests, based on Rao's
          score principle, are now routinely reported in applied
          econometric work. This paper draws together some important
          recent results which are designed to improve: (a) the
          robustness of standard score tests; and (b) the reliability
          of the asymptotic approximations used for inferential
          purposes. The discussion of robustness includes (i)
          parametric, (ii) distributional, and (iii) higher-order
          moment robustness. The issue of finite sample reliability
          focuses on controlling the size of the score test using (i)
          different variance estimators in conjunction with standard
          asymptotic theory, (ii) finite sample corrections
          obtainable from higher-order asymptotic analysis, and (iii)
          bootstrap procedures.},
  doi       = {10.1016/S0378-3758(00)00351-7},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {1},
  pages     = {153--176},
  volume    = {97}
}

@Article{goeman2006testing,
  author    = {Goeman, Jelle J. and van de Geer, Sara A. and van
          Houwelingen, Hans C.},
  year      = {2006},
  title     = {Testing against a high dimensional alternative},
  abstract  = {As the dimensionality of the alternative hypothesis
          increases, the power of classical tests tends to diminish
          quite rapidly. This is especially true for high dimensional
          data in which there are more parameters than observations.
          We discuss a score test on a hyperparameter in an empirical
          Bayesian model as an alternative to classical tests. It
          gives a general test statistic which can be used to test a
          point null hypothesis against a high dimensional
          alternative, even when the number of parameters exceeds the
          number of samples. This test will be shown to have optimal
          power on average in a neighbourhood of the null hypothesis,
          which makes it a proper generalization of the locally most
          powerful test to multiple dimensions. To illustrate this
          new locally most powerful test we investigate the case of
          testing the global null hypothesis in a linear regression
          model in more detail. The score test is shown to have
          significantly more power than the F-test whenever under the
          alternative the large variance principal components of the
          design matrix explain substantially more of the variance of
          the outcome than do the small variance principal
          components. The score test is also useful for detecting
          sparse alternatives in truly high dimensional data, where
          its power is comparable with the test based on the maximum
          absolute t-statistic.},
  comment   = {The authors consider testing against a broad range of
          alternatives where "interesting" alternatives are suggested
          by the prior distributions of the parameters.},
  doi       = {10.1111/j.1467-9868.2006.00551.x},
  issn      = {1369-7412},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {3},
  pages     = {477--493},
  publisher = {Blackwell Publishing},
  volume    = {68}
}

@Article{golan:maasoumi:2008,
  author    = {Golan, Amos and Maasoumi, Esfandiar },
  year      = {2008},
  title     = {Information Theoretic and Entropy Methods: An Overview},
  doi       = {10.1080/07474930801959685},
  journal   = {Econometric Reviews},
  number    = {4},
  pages     = {317--328},
  volume    = {27}
}

@Article{goldberger:1972,
  author    = {Goldberger, Arthur S.},
  year      = {1972},
  title     = {Structural Equation Methods in the Social Sciences},
  abstract  = {This survey of the use of structural equation models and
          methods by social scientists emphasizes the treatment of
          unobservable variables and attempts to redress economists'
          neglect of the work of Sewall Wright.},
  doi       = {10.2307/1913851},
  issn      = {00129682},
  journal   = {Econometrica},
  number    = {6},
  pages     = {979--1001},
  publisher = {The Econometric Society},
  volume    = {40}
}

@InBook{goldberger:2005,
  author    = {Goldberger, Arthur S.},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {Structural Equation Models in Human Behavior Genetics},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  publisher = {Cambridge University Press}
}

@Book{golds:2002,
  author    = "Harvey Goldstein",
  year      = 2002,
  title     = "Multilevel Statistical Models",
  publisher = "Arnold Publishers",
  edition   = "3rd"
}

@Article{golds:mcdon:1988,
  author    = {Goldstein, Harvey and Mc{D}onald, Roderick },
  year      = {1988},
  title     = {A general model for the analysis of multilevel data},
  comment   = {10.1007/BF02294400},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {455--467},
  volume    = {53},
  abstract  = {A general model is developed for the analysis of
          multivariate multilevel data structures. Special cases of
          the model include repeated measures designs, multiple
          matrix samples, multilevel latent variable models, multiple
          time series, and variance and covariance component
          models.}
}

@Article{goldstein1988general,
  author    = {Goldstein, Harvey and Mcdonald, Roderick},
  year      = {1988},
  title     = {A general model for the analysis of multilevel data},
  abstract  = {Abstract\&nbsp;\&nbsp;A general model is developed for the
          analysis of multivariate multilevel data structures.
          Special cases of the model include repeated measures
          designs, multiple matrix samples, multilevel latent
          variable models, multiple time series, and variance and
          covariance component models.},
  doi       = {10.1007/BF02294400},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {455--467},
  volume    = {53}
}

@Article{gonzalez:griffith:2001,
  author    = {Gonzalez, Richard and Griffin, Dale},
  year      = {2001},
  title     = {Testing parameters in structural equation modeling: Every
          "one" matters.},
  number    = {3},
  pages     = {258--269},
  volume    = {6},
  journal   = {Psychological Methods}
}


@INPROCEEDINGS{gonzalez:eltinge:2007,
  author =       {Jeffrey M. Gonzalez and John L. Eltinge},
  title =        {Multiple Matrix Sampling: A Review},
  booktitle =    {Proceedings of the American Statistical Association},
  year =         {2007},
  series =       {Survey Research Methods Section},
  pages =        {3069--3075},
  address =      {Alexandria, VA},
  organization = {American Statistical Association},
}


@Book{good2004permutation,
  author    = {Good, Phillip I.},
  year      = {2004},
  title     = {Permutation, Parametric, and Bootstrap Tests of Hypotheses
          (Springer Series in Statistics)},
  abstract  = {{This book provides a step-by-step manual on the
          application of permutation tests in biology, business,
          medicine, science, and engineering. Its intuitive and
          informal style will ideally suit it as a text for students
          and researchers whether experienced or coming to these
          resampling methods for the first time. The real-world
          problems of missing and censored data, multiple
          comparisons, nonresponders, after-the-fact covariates, and
          outliers are dealt with at length. The book's main features
          include: * detailed consideration of one-, two-, and
          k-sample tests, contingency tables, experimental design,
          clinical trials, cluster analysis, multiple comparisons,
          multivariate data, regression, and sample size reduction; *
          numerous practical applications in archeology, biology,
          climatology, economics, education, medicine, and the social
          sciences; * valuable techniques for reducing computation
          time; * practical advice on experimental design; *
          comparisons with bootstrap, parametric, and nonparametric
          techniques; * an extensive three-part bibliography
          featuring more than 1,000 articles. This new edition has
          more than 100 additional pages, and includes streamlined
          statistics for the k-sample comparison and analysis of
          variance plus expanded sections on computational
          techniques, multiple comparisons, multiple regression,
          comparing variances, and testing interactions in balanced
          designs. Comprehensive author and subject indexes, plus an
          expert-system guide to methods, provide for further ease of
          use. The invaluable exercises at the end of every chapter
          have been supplemented with drills and a number of
          graduate-level thesis problems.}},
  edition   = {3rd},
  howpublished  = {Hardcover},
  isbn      = {038720279X},
  publisher = {Springer}
}

@Book{good:2004,
  author    = {Good, Phillip I. },
  year      = {2004},
  title     = {Permutation, Parametric, and Bootstrap Tests of
          Hypotheses},
  comment   = {038720279X},
  publisher = {Springer},
  series    = {Springer Series in Statistics}
}

@Article{gould:2001,
  author    = {William Gould},
  year      = 2001,
  title     = {Statistical software certification},
  journal   = {Stata Journal},
  volume    = {1},
  number    = {1},
  pages     = {29-50}
}

@Book{gould:pitt:srib:mle2,
  author    = "William Gould and Jeffrey Pittblado and William Sribney",
  year      = 2003,
  title     = "Maximum Likelihood Estimation with Stata",
  edition   = "2nd",
  publisher = "Stata Press",
  address   = "College Station, TX"
}

@Book{gould:pitt:srib:mle3,
  author    = "William Gould and Jeffrey Pittblado and William Sribney",
  year      = 2006,
  title     = "Maximum Likelihood Estimation with Stata",
  edition   = "3rd",
  publisher = "Stata Press",
  address   = "College Station, TX"
}

@Book{gould:srib:ml,
  author    = "William Gould and William Sribney",
  year      = 1999,
  title     = "Maximum Likelihood Estimation with Stata",
  publisher = "Stata Press",
  address   = "College Station"
}

@ARTICLE{gould:2003:tip3,
title = {Stata tip 3: How to be assertive},
author = {Gould, William},
year = {2003},
journal = {Stata Journal},
volume = {3},
number = {4},
}



@Article{gour:monf:trog:1984a,
  author    = {Gourieroux, C. and Monfort, A. and Trognon, A. },
  year      = {1984},
  title     = {Pseudo Maximum Likelihood Methods: Theory},
  journal   = {Econometrica},
  number    = {3},
  pages     = {681--700},
  comment   = {http://links.jstor.org/sici?sici=0012-9682%28198405%2952%3A3%3C681%3APMLMT%3E2.0.CO%3B2-N},
  volume    = {52},
  abstract  = {Estimators obtained by maximizing a likelihood function
          are studied in the case where the true p.d.f. does not
          necessarily belong to the family chosen for the likelihood
          function. When such a procedure is applied to the
          estimation of the parameters of the first order moments, it
          is possible to prove a necessary and sufficient condition
          for its consistency. Asymptotic normality is shown as well
          as the existence of a lower bound for the asymptotic
          covariance matrix. It is also seen that this bound can be
          reached if consistent estimates are available for the
          parameters of the second order moments. Finally, a
          necessary and sufficient condition for the consistency if
          the pseudo maximum likelihood estimation of the first and
          second moments is given.}
}

@Article{gour:monf:trog:1984b,
  author    = {Gourieroux, C. and Monfort, A. and Trognon, A. },
  year      = {1984},
  title     = {Pseudo Maximum Likelihood Methods: Applications to
          {P}oisson Models},
  journal   = {Econometrica},
  number    = {3},
  pages     = {701--720},
  comment   = {http://links.jstor.org/sici?sici=0012-9682%28198405%2952%3A3%3C701%3APMLMAT%3E2.0.CO%3B2-4},
  volume    = {52},
  abstract  = {Pseudo maximum likelihood techniques are applied to basic
          Poisson models and to Poisson models with specification
          errors. In the latter case it is shown that consistent and
          asymptotically normal estimators can be obtained without
          specifying the p.d.f. of the disturbances. These estimators
          are compared both from the finite sample and the asymptotic
          point of view. Quasi generalized PML estimators, which
          asymptotically dominate all PML estimators, are also
          proposed. Finally, bivariate and panel data Poisson models
          are discussed.}
}

@INCOLLECTION{gour:monf:1994,
  AUTHOR =       {Gourieroux, C. and Monfort, A.},
  TITLE =        {Testing Non-Nested Hypotheses},
  BOOKTITLE =    {Handbook of Econometrics, volume IV},
  PUBLISHER =    {Elsevier},
  YEAR =         {1994},
  editor =       {R. F. Engle and D. L. Mc{F}adden},
  address =      {Oxford, UK},
}


@Book{gourieroux:2000,
  author    = {Gourieroux, Christian },
  year      = {2000},
  title     = {Econometrics of Qualitative Dependent Variables},
  isbn      = {0521331498},
  publisher = {Cambridge University Press},
  series    = {Themes in Modern Econometrics}
}

@Article{grac:puge:1997,
  author    = {J.B. Grace and B. Pugesek},
  year      = {1997},
  title     = {A structural equation model of plant species richness and
          its application to a coastal wetland},
  journal   = {American Naturalist},
  volume    = {149},
  pages     = {436--460}
}

@Book{grace:2006,
  author    = {Grace, James B. },
  year      = {2006},
  title     = {Structural Equation Modeling and Natural Systems},
  isbn      = {0521546532},
  publisher = {Cambridge University Press}
}

@Article{graham:hinkley:john:shi:1990,
  author    = {Graham, R. L. and Hinkley, D. V. and John, P. W. M. and
          Shi, S. },
  year      = {1990},
  title     = {Balanced Design of Bootstrap Simulations},
  journal   = {Journal of the Royal Statistical Society},
  number    = {1},
  pages     = {185--202},
  series    = {B},
  volume    = {52},
  abstract  = {Davison et al. (1986) have shown that finite bootstrap
          simulations can be improved by forcing balance in the
          aggregate of simulated data sets. Their methods yield
          first-order balance, which principally affects bootstrap
          estimation of bias. Here we extend the methodology to
          second-order balance, which principally affects bootstrap
          estimation of variance. The particular techniques involve
          Latin square and balanced incomplete block designs.
          Numerical examples are given to illustrate both the
          positive and the negative features of the balanced
          simulations.}
}

@Article{grayson:marsh:1994,
  author    = {Grayson, David and Marsh, Herbert },
  year      = {1994},
  title     = {Identification with deficient rank loading matrices in
          confirmatory factor analysis: Multitrait-multimethod
          models},
  abstract  = {This paper presents some results on identification in
          multitrait-multimethod (MTMM) confirmatory factor analysis
          (CFA) models. Some MTMM models are not identified when the
          (factorial-patterned) loadings matrix is of deficient
          column rank. For at least one other MTMM model,
          identification does exist despite such deficiency. It is
          also shown that for some MTMM CFA models, Howe's (1955)
          conditions sufficient for rotational uniqueness can fail,
          yet the model may well be identified and rotationally
          unique. Implications of these results for CFA models in
          general are discussed.},
  doi       = {http://dx.doi.org/10.1007/BF02294271},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {121--134},
  volume    = {59}
}

@Book{green:silv:1994,
  author    = "Green, P.~J. and Silverman, B.~W.",
  year      = 1994,
  title     = "Nonparametric Regression and Generalized Linear Models: A
          Roughness Penalty Approach",
  series    = "Monographs on Statistics and Applied Probability",
  volume    = 58,
  publisher = "Chapman \& Hall"
}

@Book{greene:2007:6ed,
  author    = {Greene, William H.},
  year      = {2007},
  title     = {Econometric Analysis},
  abstract  = {**** \_Econometric Analysisi, 6/e\_ serves as a bridge
          between an introduction to the field of econometrics and
          the professional literature for  social scientists and
          other professionals in the field of social sciences,
          focusing on applied econometrics and theoretical
          background. This book provides a broad survey of the field
          of econometrics that allows the reader to move from here to
          practice in one or more specialized areas. At the same
          time, the reader will gain an appreciation of the common
          foundation of all the fields presented and use the tools
          they employ. **** This book gives space to a wide range of
          topics including basic econometrics, Classical, Bayesian,
          GMM, and Maximum likelihood, and gives special emphasis to
          new topics such a time series and panels. ****For social
          scientists and other professionals in the field who want a
          thorough introduction to applied econometrics that will
          prepare them for advanced study and practice in the field.},
  edition   = {6th},
  howpublished  = {Hardcover},
  isbn      = {0135132452},
  publisher = {Prentice Hall}
}

@Article{grendar:judge:2008,
  author    = {Grend\'{a}r, Marian and Judge, George },
  year      = {2008},
  title     = {Large-Deviations Theory and Empirical Estimator Choice},
  abstract  = {In this article, we consider the problem of criterion
          choice in information recovery and inference in a
          large-deviations (LD) context. Kitamura and Stutzer
          recognize that the Maximum Entropy Empirical Likelihood
          estimator can be given a LD justification (Kitamura and
          Stutzer, 2002). We demonstrate there exists a similar LD
          justification for Owen's Empirical Likelihood estimator
          (Owen, 2001). We tie the two empirical estimators and
          related LD theorems to two basic ill-posed inverse problems
          \&agr; and \&b.beta;. We note that other estimators in this
          family lack an LD footing and provide an extensive
          discussion of the implications of these results. The
          appendix contains formal statements regarding relevant LD
          theorems.},
  doi       = {10.1080/07474930801960402},
  journal   = {Econometric Reviews},
  number    = {4},
  pages     = {513--525},
  volume    = {27}
}

@Book{grif:high:1997,
  author    = "D. F. Griffiths and D. J. Higham",
  year      = 1997,
  title     = "Learning \LaTeX",
  publisher = "SIAM"
}

@article{grimmer:2011,
    abstract =
        {Markov chain Monte Carlo ({MCMC}) methods have facilitated an
        explosion of interest in Bayesian methods. {MCMC} is an incredibly
        useful and important tool but can face difficulties when used to
        estimate complex posteriors or models applied to large data sets. In
        this paper, we show how a recently developed tool in computer science
        for fitting Bayesian models, variational approximations, can be used
        to facilitate the application of Bayesian models to political science
        data. Variational approximations are often much faster than {MCMC}
        for fully Bayesian inference and in some instances facilitate the
        estimation of models that would be otherwise impossible to estimate.
        As a deterministic posterior approximation method, variational
        approximations are guaranteed to converge and convergence is easily
        assessed. But variational approximations do have some limitations,
        which we detail below. Therefore, variational approximations are best
        suited to problems when fully Bayesian inference would otherwise be
        impossible. Through a series of examples, we demonstrate how
        variational approximations are useful for a variety of political
        science research. This includes models to describe legislative voting
        blocs and statistical models for political texts. The code that
        implements the models in this paper is available in the supplementary
        material.},
    author = {Grimmer, Justin},
    doi = {10.1093/pan/mpq027},
    journal = {Political Analysis},
    number = {1},
    pages = {32--47},
    title = {An Introduction to {Bayesian} Inference via Variational Approximations},
    volume = {19},
    year = {2011}
}

@book{griva:nash:sofer:2008,
    author = {Griva, Igor and Nash, Stephen G. and Sofer, Ariela},
    edition = {2},
    publisher = {Society for Industrial Mathematics},
    title = {Linear and Nonlinear Optimization, Second Edition},
    address = {Philadelphia},
    year = {2008}
}

@Book{grov:fowl:coup:lepk:sing:tour:2004,
  author    = {Groves, Robert M. and Couper, Mick P. and Lepkowski, James
          M. and Singer, Eleanor and Tourangeau, Roger },
  year      = {2004},
  title     = {Survey Methodology},
  publisher = {John Wiley and Sons},
  series    = {Wiley Series in Survey Methodology},
  address   = {New York}
}

@Book{grov:fowl:coup:lepk:sing:tour:2009,
  author    = {Groves, Robert M. and Fowler, Floyd J. and Couper, Mick P. and Lepkowski, James
          M. and Singer, Eleanor and Tourangeau, Roger },
  year      = {2009},
  title     = {Survey Methodology},
  publisher = {John Wiley and Sons},
  series    = {Wiley Series in Survey Methodology},
  address   = {New York},
  edition   = {2nd}
}

@book{groves:dillman:eltinge:little:2001,
    author = {Groves, Robert M. and Dillman, Don A. and Eltinge, John L.
              and Little, Roderick J. A.},
    isbn = {0471396273},
    publisher = {Wiley-Interscience},
    title = {Survey Nonresponse},
    series = {Wiley Series in Survey Methodology},
    year = {2001}
}

@Book{grov:surv:1989,
  author    = "Groves, Robert M.",
  year      = 1989,
  title     = "Survey Errors and Survey Costs",
  pages     = 590,
  publisher = "John Wiley \& Sons",
  address   = "New York"
}

@article{groves:2006,
    abstract =
        {Many surveys of the {U.S}. household population are experiencing
        higher refusal rates. Nonresponse can, but need not, induce
        nonresponse bias in survey estimates. Recent empirical findings
        illustrate cases when the linkage between nonresponse rates and
        nonresponse biases is absent. Despite this, professional standards
        continue to urge high response rates. Statistical expressions of
        nonresponse bias can be translated into causal models to guide
        hypotheses about when nonresponse causes bias. Alternative designs to
        measure nonresponse bias exist, providing different but incomplete
        information about the nature of the bias. A synthesis of research
        studies estimating nonresponse bias shows the bias often present. A
        logical question at this moment in history is what advantage
        probability sample surveys have if they suffer from high nonresponse
        rates. Since postsurvey adjustment for nonresponse requires auxiliary
        variables, the answer depends on the nature of the design and the
        quality of the auxiliary variables.},
    author = {Groves, Robert M.},
    doi = {10.1093/poq/nfl033},
    journal = {Public Opinion Quarterly},
    volume = {70},
    number = {5},
    pages = {646--675},
    title = {Nonresponse Rates and Nonresponse Bias in Household Surveys},
    year = {2006}
}

@article{groves:lyberg:2010,
    abstract =
        {? Total survey error? is a conceptual framework describing
        statistical error properties of sample survey statistics. Early in
        the history of sample surveys, it arose as a tool to focus on
        implications of various gaps between the conditions under which
        probability samples yielded unbiased estimates of finite population
        parameters and practical situations in implementing survey design.
        While the framework permits design-based estimates of various error
        components, many of the design burdens to produce those estimates are
        large, and in practice most surveys do not implement them. Further,
        the framework does not incorporate other, nonstatistical, dimensions
        of quality that are commonly utilized in evaluating statistical
        information. The importation of new modeling tools brings new promise
        to measuring total survey error components, but also new challenges.
        A lasting value of the total survey error framework is at the design
        stage of a survey, to attempt a balance of costs and various errors.
        Indeed, this framework is the central organizing structure of the
        field of survey methodology.},
    author = {Groves, Robert M. and Lyberg, Lars},
    doi = {10.1093/poq/nfq065},
    issn = {1537-5331},
    journal = {Public Opinion Quarterly},
    number = {5},
    pages = {849--879},
    publisher = {Oxford University Press},
    title = {Total Survey Error: Past, Present, and Future},
    volume = {74},
    year = {2010}
}

@Article{guggen:2008,
  author    = {Guggenberger, Patrik },
  year      = {2008},
  title     = {Finite Sample Evidence Suggesting a Heavy Tail Problem of
          the Generalized Empirical Likelihood Estimator},
  abstract  = {Comprehensive Monte Carlo evidence is provided that
          compares the finite sample properties of generalized
          empirical likelihood (GEL) estimators to the ones of
          <i>k</i>-class estimators in the linear instrumental
          variables (IV) model. We focus on sample median, mean, mean
          squared error, and on the coverage probability and length
          of confidence intervals obtained from inverting a
          t-statistic based on the various estimators. The results
          indicate that in terms of the above criteria, all the GEL
          estimators and the limited information maximum likelihood
          (LIML) estimator behave very similarly. This suggests that
          GEL estimators might also share the T?no-momentT? problem
          of LIML. At sample sizes as in our Monte Carlo study, there
          is no systematic bias advantage of GEL estimators over
          <i>k</i>-class estimators. On the other hand, the standard
          deviation of GEL estimators is pronouncedly higher than for
          some of the <i>k</i>-class estimators. Therefore, if mean
          squared error is used as the underlying loss function, our
          study suggests the use of computationally simple
          estimators, such as two-stage least squares, in the linear
          IV model rather than GEL. Based on the properties of
          confidence intervals, we cannot recommend the use of GEL
          estimators either in the linear IV model.},
  doi       = {10.1080/07474930801960410},
  journal   = {Econometric Reviews},
  number    = {4},
  pages     = {526--541},
  volume    = {27}
}

@Article{guggen:hahn:2005,
  author    = {Guggenberger, Patrik and Hahn, Jinyong },
  year      = {2005},
  title     = {Finite Sample Properties of the Two-Step Empirical
          Likelihood Estimator},
  abstract  = {We investigate the finite sample properties of two-step
          empirical likelihood (EL) estimators. These estimators are
          shown to have the same third-order bias properties as EL
          itself. The Monte Carlo study provides evidence that (i)
          higher order asymptotics fails to provide a good
          approximation in the sense that the bias of the two-step EL
          estimators can be substantial and sensitive to the number
          of moment restrictions and (ii) the two-step EL estimators
          may have heavy tails.},
  doi       = {10.1080/07474930500242987},
  journal   = {Econometric Reviews},
  number    = {3},
  pages     = {247--263},
  volume    = {24}
}

@Article{guggen:smith:2005,
  author    = {Guggenberger, Patrik and Smith, Richard J. },
  year      = {2005},
  title     = {Generalized Empirical Likelihood Estimators and Tests
          under Partial, Weak, and Strong Identification},
  doi       = {10.1017/S0266466605050371},
  journal   = {Econometric Theory},
  number    = {04},
  pages     = {667--709},
  volume    = {21}
}

@Article{guo:ratcliffe:tenhave:2004,
  author    = {Guo, Wensheng and Ratcliffe, Sarah J. and {Have}, Thomas
          {.}},
  year      = {2004},
  title     = {A Random Pattern-Mixture Model for Longitudinal Data With
          Dropouts},
  abstract  = {Pattern-mixture models are frequently used for
          longitudinal data analysis with dropouts because they do
          not require explicit specification of the dropout
          mechanism. These models stratify the data according to time
          to dropout and formulate a model for each stratum. This
          usually results in underindentifiability, because we need
          to estimate many pattern-specific parameters even though
          the eventual interest is usually on the marginal
          parameters. In this article we extend this framework to a
          random pattern-mixture model, where the pattern-specific
          parameters are treated as nuisance parameters and modeled
          as random instead of fixed. The pattern is defined
          according to a surrogate for the dropout process. A
          constraint is then put on the pattern by linking it to the
          time to dropout using a random-effects survival model. We
          assume, conditional on the latent pattern effects, that the
          longitudinal outcome and the dropout process are
          independent. This model retains the robustness of the
          traditional pattern-mixture models, while avoiding the
          overparameterization problem. When we define each subject
          as a separate stratum, this model reduces to the shared
          parameter model. Maximum likelihood estimates are obtained
          using an EM Newton– Raphson algorithm. We apply the
          method to the depression data from the Prevention of
          Suicide in Primary Care Elderly Collaborative Trial
          (PROSPECT). We show when the dropout information is
          adjusted for under the proposed model, the treatment seems
          to reduce depression in the elderly.},
  doi       = {10.1198/016214504000000674},
  journal   = {Journal of the American Statistical Association},
  number    = {468},
  pages     = {929--937},
  volume    = {99}
}

@Article{gupta:2003,
  author    = {Gupta, A. K. },
  year      = {2003},
  title     = {Multivariate Skew $t$-distribution},
  doi       = {10.1080/715019247},
  journal   = {Statistics},
  number    = {4},
  pages     = {1},
  publisher = {Taylor \& Francis},
  volume    = {37},
  abstract  = {In this paper, we define multivariate skew t-distribution
          which has some of the properties of multivariate
          t-distribution and has a shape parameter to represent
          skewness. Some of its properties are also studied including
          the moments. Multivariate skew-Cauchy distribution is given
          as a special case.}
}

@Article{gupta:gonza:domin:2003,
  author    = {Gupta, Arjun K. and Gonz\'{a}lez-Far\ias, Graciela and
          Dom\inguez-Molina, Armando J.},
  year      = {2004},
  title     = {A multivariate skew normal distribution},
  abstract  = {In this paper, we define a new class of multivariate
          skew-normal distributions. Its properties are studied. In
          particular we derive its density, moment generating
          function, the first two moments and marginal and
          conditional distributions. We illustrate the contours of a
          bivariate density as well as conditional expectations. We
          also give an extension to construct a general multivariate
          skew normal distribution.},
  doi       = {10.1016/S0047-259X(03)00131-3},
  issn      = {0047259X},
  journal   = {Journal of Multivariate Analysis},
  number    = {1},
  pages     = {181--190},
  volume    = {89}
}

@Article{gupta:gonza:domin:2004,
  author    = {Gupta, Arjun K. and Gonz{\'a}lez-Far{\'\i}as, Graciela and
          Dom{\'\i}nguez-Molina, Armando J. },
  year      = {2004},
  title     = {A multivariate skew normal distribution},
  doi       = {10.1016/S0047-259X(03)00131-3},
  journal   = {Journal of Multivariate Analysis},
  number    = {1},
  pages     = {181--190},
  volume    = {89},
  abstract  = {In this paper, we define a new class of multivariate
          skew-normal distributions. Its properties are studied. In
          particular we derive its density, moment generating
          function, the first two moments and marginal and
          conditional distributions. We illustrate the contours of a
          bivariate density as well as conditional expectations. We
          also give an extension to construct a general multivariate
          skew normal distribution.}
}

@Article{gupta:nigam:1987,
  author    = {Gupta, V. K. and Nigam, A. K. },
  year      = {1987},
  title     = {Mixed Orthogonal Arrays for Variance Estimation with
          Unequal Numbers of Primary Selections per Stratum},
  journal   = {Biometrika},
  number    = {4},
  pages     = {735--742},
  volume    = {74},
  abstract  = {For estimating the variance of nonlinear statistics in
          large-scale complex surveys, the method of balanced
          repeated replications has received special attention. The
          method at present is useful in stratified clustered designs
          with equal number of primary selections from each stratum.
          This paper extends the method to general sample designs
          with arbitrary number of selections from each stratum. It
          is shown that mixed orthogonal arrays of strength two, or
          equivalently, equal frequency orthogonal main-effect plans
          for asymmetrical factorials give a set of balanced
          subsamples useful in variance estimation.},
  comment   = {Their variance estimator works only in linear case. For
          non-linear case, see Wu (1991),
          http://www.citeulike.org/user/ctacmo/article/782748.},
}

@Article{gurney:jewett:1975,
  author    = {Gurney, Margaret and Jewett, Robert S. },
  year      = {1975},
  title     = {Constructing Orthogonal Replications for Variance
          Estimation},
  journal   = {Journal of the American Statistical Association},
  number    = {352},
  pages     = {819--821},
  volume    = {70},
  abstract  = {Recently the Bureau of the Census used repeated orthogonal
          replications to estimate the variance for one of its
          surveys. This article explains how the proper replication
          patterns were selected, and gives a step-by-step general
          method for obtaining orthogonal replications.}
}

@TechReport{gwat:rust:john:pand:wags:2003a,
  author    = "Davidson R. Gwatkin and Shear Rustein and Kiersten Johnson
          and Eldaw Abdalla Suliman and Adam Wagstaff",
  year      = "2003",
  title     = "Socio-Economic Differences in Health, Nutrition, and
          Population",
  note      = "Volume 1: {A}rmenia -- {K}yrgyz {R}epublic",
  institution   = "World Bank"
}

@TechReport{gwat:rust:john:pand:wags:2003b,
  author    = "Davidson R. Gwatkin and Shear Rustein and Kiersten Johnson
          and Eldaw Abdalla Suliman and Adam Wagstaff",
  year      = "2003",
  title     = "Socio-Economic Differences in Health, Nutrition, and
          Population",
  note      = "Volume 2: {M}adagascar -- {Z}imbabwe",
  institution   = "World Bank"
}

@Article{haas:2002,
  author    = "T. Haas",
  year      = 2002,
  title     = "New systems for modeling, estimating, and predicting a
          multivariate spatio-temporal process",
  journal   = "Environmetrics",
  volume    = 13,
  pages     = "311--332"
}

@Article{hagg:1983,
  author    = {Goesta H{\"a}gglund},
  year      = {1983},
  title     = {Factor analysis by instrumental methods: A {Monte} {Carlo}
          study of some estimation procedures},
  journal   = {University of Uppsala. Department of Statistics, Uppasla,
          Sweden}
}

@Article{hahn:hausman:2002,
  author    = {Hahn, Jinyong and Hausman, Jerry},
  year      = {2002},
  title     = {A New Specification Test for the Validity of Instrumental
          Variables},
  abstract  = {We develop a new specification test for IV estimators
          adopting a particular second order approximation of Bekker.
          The new specification test compares the difference of the
          forward (conventional) 2SLS estimator of the coefficient of
          the right-hand side endogenous variable with the reverse
          2SLS estimator of the same unknown parameter when the
          normalization is changed. Under the null hypothesis that
          conventional first order asymptotics provide a reliable
          guide to inference, the two estimates should be very
          similar. Our test sees whether the resulting difference in
          the two estimates satisfies the results of second order
          asymptotic theory. Essentially the same idea is applied to
          develop another new specification test using second-order
          unbiased estimators of the type first proposed by Nagar. If
          the forward and reverse Nagar-type estimators are not
          significantly different we recommend estimation by LIML,
          which we demonstrate is the optimal linear combination of
          the Nagar-type estimators (to second order). We also
          demonstrate the high degree of similarity for k-class
          estimators between the approach of Bekker and the Edgeworth
          expansion approach of Rothenberg. An empirical example and
          Monte Carlo evidence demonstrate the operation of the new
          specification test.},
  doi       = {10.2307/2692166},
  issn      = {00129682},
  journal   = {Econometrica},
  number    = {1},
  pages     = {163--189},
  publisher = {The Econometric Society},
  volume    = {70}
}

@Article{hahn:hausman:2003,
  author    = {Hahn, J. and Hausman, J. },
  year      = {2003},
  title     = {Weak Instruments: Diagnosis and Cures in Empirical
          Econometrics},
  doi       = {10.1257/000282803321946912},
  journal   = {The American Economic Review},
  pages     = {118--125},
  volume    = 93,
  issue     = 2
}

@Article{hajek:1960,
  author    = "J. H{\'a}jek",
  year      = "1960",
  title     = "Limiting Distributions in Simple Random Sampling from a
          Finite Population",
  journal   = "Publications of Mathematical Institute of Hungarian
          Academy of Sciences, Series A",
  volume    = "5",
  pages     = "361--374"
}

@Article{hall:1989,
  author    = {Hall, Peter},
  year      = {1989},
  title     = {On efficient bootstrap simulation},
  abstract  = {It is shown that three new, efficient bootstrap algorithms
          are asymptotically equivalent. This is done in two ways.
          First, asymptotic formulae for variances and mean squared
          errors are derived, and shown to be identical. Secondly, it
          is demonstrated that two of the methods may be viewed as
          approximations to the third. The three algorithms
          considered are the balanced bootstrap and the linear
          approximation method proposed by Davison, Hinkley \&
          Schechtman (1986), and a centring method proposed by Efron
          in the context of bias estimation. It is shown that each
          reduces the order of magnitude of mean squared error by the
          factor n-1, where n is sample size. These results apply to
          smooth functions of means. 10.1093/biomet/76.3.613},
  doi       = {10.1093/biomet/76.3.613},
  journal   = {Biometrika},
  number    = {3},
  pages     = {613--617},
  volume    = {76}
}

@Article{hall:hoss:nasab:2006,
  author    = {Hall, Peter and Hosseini-Nasab, Mohammad},
  year      = {2006},
  title     = {On properties of functional principal components
          analysis},
  abstract  = {Functional data analysis is intrinsically infinite
          dimensional; functional principal component analysis
          reduces dimension to a finite level, and points to the most
          significant components of the data. However, although this
          technique is often discussed, its properties are not as
          well understood as they might be. We show how the
          properties of functional principal component analysis can
          be elucidated through stochastic expansions and related
          results. Our approach quantifies the errors that arise
          through statistical approximation, in successive terms of
          orders \$n^{-1/2}\$, \$n^{-1}\$, \$n^{-3/2}\$, …, where n
          denotes sample size. The expansions show how spacings among
          eigenvalues impact on statistical performance. The term of
          size \$n^{-1/2}\$ illustrates first-order properties and
          leads directly to limit theory which describes the dominant
          effect of spacings. Thus, for example, spacings are seen to
          have an immediate, first-order effect on properties of
          eigenfunction estimators, but only a second-order effect on
          eigenvalue estimators. Our results can be used to explore
          properties of existing methods, and also to suggest new
          techniques. In particular, we suggest bootstrap methods for
          constructing simultaneous confidence regions for an
          infinite number of eigenvalues, and also for individual
          eigenvalues and eigenvectors.},
  doi       = {10.1111/j.1467-9868.2005.00535.x},
  issn      = {1369-7412},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {1},
  pages     = {109--126},
  publisher = {Blackwell Publishing},
  volume    = {68}
}

@Book{hall:1992,
  author    = {Hall, Peter },
  year      = {1992},
  title     = {The Bootstrap and {E}dgeworth Expansion},
  address   = {New York},
  publisher = {Springer},
  series    = {Springer Series in Statistics}
}

@Book{hall:2005,
  author    = "Alastair R. Hall",
  year      = 2005,
  title     = "Generalized Method of Moments",
  publisher = "Oxford University Press"
}

@Book{hall:2005:gmm,
  author    = {Hall, Alastair},
  year      = {2005},
  title     = {Generalized Method of Moments},
  address   = {New York},
  publisher = {Oxford University Press}
}

@Article{hall:horowitz:1996,
  author    = {Hall, Peter and Horowitz, Joel L. },
  year      = {1996},
  title     = {Bootstrap Critical Values for Tests Based on
          Generalized-Method-of-Moments Estimators},
  doi       = {10.2307/2171849},
  journal   = {Econometrica},
  number    = {4},
  pages     = {891--916},
  volume    = {64},
  abstract  = {Monte Carlo experiments have shown that tests based on
          generalized-method-of-moments estimators often have true
          levels that differ greatly from their nominal levels when
          asymptotic critical values are used. This paper gives
          conditions under which the bootstrap provides asymptotic
          refinements to the critical values of t tests and the test
          of overidentifying restrictions. Particular attention is
          given to the case of dependent data. It is shown that with
          such data, the bootstrap must sample blocks of data and
          that the formulae for the bootstrap versions of test
          statistics differ from the formulae that apply with the
          original data. The results of Monte Carlo experiments on
          the numerical performance of the bootstrap show that it
          usually reduces the errors in level that occur when
          critical values based on first-order asymptotic theory are
          used. The bootstrap also provides an indication of the
          accuracy of critical values obtained from first-order
          asymptotic theory.}
}

@Article{hall:inoue:jana:shin:2007,
  author    = {Hall, Alastair R. and Inoue, Atsushi and Jana, Kalidas and
          Shin, Changmock },
  year      = {2007},
  title     = {Information in generalized method of moments estimation
          and entropy-based moment selection},
  abstract  = {In this paper, we make five contributions to the
          literature on information and entropy in generalized method
          of moments (GMM) estimation. First, we introduce the
          concept of the long run canonical correlations (LRCCs)
          between the true score vector and the moment function
          f(vt,[theta]0) and show that they provide a metric for the
          information contained in the population moment condition
          E[f(vt,[theta]0)]=0. Second, we show that the entropy of
          the limiting distribution of the GMM estimator can be
          written in terms of these LRCCs. Third, motivated by the
          above results, we introduce an information criterion based
          on this entropy that can be used as a basis for moment
          selection. Fourth, we introduce the concept of nearly
          redundant moment conditions and use it to explore the
          connection between redundancy and weak identification.
          Fifth, we analyse the behaviour of the aforementioned
          entropy-based moment selection method in two scenarios of
          interest; these scenarios are: (i) nonlinear dynamic models
          where the parameter vector is identified by all the
          combinations of moment conditions considered; (ii) linear
          static models where the parameter vector may be weakly
          identified for some of the combinations considered. The
          first of these contributions rests on a generalized
          information equality that is proved in the paper, and may
          be of interest in its own right.},
  booktitle = {'Information and Entropy Econometrics' - A Volume in Honor
          of Arnold Zellner},
  doi       = {10.1016/j.jeconom.2006.05.006},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {488--512},
  volume    = {138}
}

@Article{hall:jing:1996,
  author    = {Hall, Peter and Jing, Bingyi },
  year      = {1996},
  title     = {On Sample Reuse Methods for Dependent Data},
  journal   = {Journal of the Royal Statistical Society. Series B
          (Methodological)},
  number    = {4},
  pages     = {727--737},
  volume    = {58}
}

@Article{hall:lascala:1990,
  author    = {Hall, Peter and {La Scala}, Barbara },
  year      = {1990},
  title     = {Methodology and Algorithms of Empirical Likelihood},
  doi       = {10.2307/1403462},
  journal   = {International Statistical Review},
  number    = {2},
  pages     = {109--127},
  volume    = {58}
}

@Article{hall:wilson:1991,
  author    = {Hall, Peter and Wilson, Susan R.},
  year      = {1991},
  title     = {Two Guidelines for Bootstrap Hypothesis Testing},
  abstract  = {Two guidelines for nonparametric bootstrap hypothesis
          testing are highlighted. The first recommends that
          resampling be done in a way that reflects the null
          hypothesis, even when the true hypothesis is distant from
          the null. The second guideline argues that bootstrap
          hypothesis tests should employ methods that are already
          recognized as having good features in the closely related
          problem of confidence interval construction. Violation of
          the first guideline can seriously reduce the power of a
          test. Sometimes this reduction is spectacular, since it is
          most serious when the null hypothesis is grossly in error.
          The second guideline is of some importance when the
          conclusion of a test is equivocal. It has no direct bearing
          on power, but improves the level accuracy of a test.},
  doi       = {10.2307/2532163},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {2},
  pages     = {757--762},
  publisher = {International Biometric Society},
  volume    = {47}
}

@ARTICLE{halton:1960,
  AUTHOR =       {J. H. Halton},
  TITLE =        {On the efficiency of certain quasi-random sequences of points
                  in evaluating multi-dimensional integrals},
  JOURNAL =      {Numerische Mathematik},
  YEAR =         {1960},
  volume =       {2},
  pages =        {84--90},
}


@Article{hamp:infl:1974,
  author    = {Hampel, Frank R.},
  year      = {1974},
  title     = {The Influence Curve and Its Role in Robust Estimation},
  journal   = {Journal of the American Statistical Association},
  volume    = {69},
  pages     = {383--393}
}

@Book{hamp:ronc:rous:stah:2005,
  author    = {Hampel, Frank R. and Ronchetti, Elvezio M. and Rousseeuw,
          Peter J. and Stahel, Werner A. },
  year      = {2005},
  title     = {Robust Statistics: The Approach Based on Influence
          Functions},
  address   = {New York},
  edition   = {revised},
  isbn      = {0471735779},
  publisher = {Wiley-Interscience},
  series    = {Wiley Series in Probability and Statistics},
  abstract  = {Introducing concepts, theory, and applications, <i>Robust
          Statistics</i> is accessible to a broad audience, avoiding
          allusions to high-powered mathematics while emphasizing
          ideas, heuristics, and background. The text covers the
          approach based on the influence function (the effect of an
          outlier on an estimater, for example) and related notions
          such as the breakdown point. It also treats the
          change-of-variance function, fundamental concepts and
          results in the framework of estimation of a single
          parameter, and applications to estimation of covariance
          matrices and regression parameters. <i>Robust
          Statistics</i> is a leading-edge resource suitable for use
          both as a textbook or a reference on robust statistics for
          all practitioners and students.}
}

@article {han:philips:2006,
    author = {Han, Chirok and Phillips, Peter C. B.},
    title = {{GMM} with Many Moment Conditions},
    journal = {Econometrica},
    volume = {74},
    number = {1},
    publisher = {Blackwell Publishing Ltd},
    issn = {1468-0262},
    doi = {10.1111/j.1468-0262.2006.00652.x},
    pages = {147--192},
    year = {2006},
}


@Article{han:rao:ravi:1988,
  author    = {C.-P. Han and C. V. Rao and J. Ravichandran},
  year      = {1988},
  title     = {Inference Based on Conditional Specification: A Second
          Bibliography},
  journal   = {Communications in Statitstics, Part A---Theory and
          Methods},
  volume    = {17},
  pages     = {1945--1964}
}

@Book{hancock:mueller:2006,
  editor    = {Hancock, Gregory R. and Mueller, Ralph O. },
  year      = {2006},
  title     = {Structural Equation Modeling: A Second Course},
  isbn      = {1593110146},
  publisher = {IAP - Information Age Publishing Inc.},
  series    = {Quantitative Methods in Education and the Behavioral
          Sciences}
}

@INCOLLECTION{hancock:2006,
  AUTHOR =       {Hancock, Gregory R. },
  TITLE =        {Power Analysis in Covariance Structure Modeling},
  editor    = {Hancock, Gregory R. and Mueller, Ralph O. },
  YEAR      = {2006},
  BOOKTITLE = {Structural Equation Modeling: A Second Course},
  isbn      = {1593110146},
  PUBLISHER = {IAP - Information Age Publishing Inc.},
  series    = {Quantitative Methods in Education and the Behavioral
          Sciences},
}


@Book{hancock:mueller:2006,
  editor    = {Hancock, Gregory R. and Mueller, Ralph O. },
  year      = {2006},
  title     = {Structural Equation Modeling: A Second Course},
  isbn      = {1593110146},
  publisher = {IAP - Information Age Publishing Inc.},
  series    = {Quantitative Methods in Education and the Behavioral
          Sciences}
}

@Book{hand:2004,
  author    = {Hand, David J. },
  year      = {2004},
  title     = {Measurement Theory and Practice},
  isbn      = {034067783X},
  publisher = {Hodder Arnold Publication},
  series    = {Kendall's Library of Statistics}
}

@Article{handcock:raftery:tantrum:2007,
  author    = {Handcock, Mark S. and Raftery, Adrian E. and Tantrum,
          Jeremy M. },
  year      = {2007},
  title     = {Model-based clustering for social networks},
  comment   = {10.1111/j.1467-985X.2007.00471.x},
  journal   = {Journal of the Royal Statistical Society: Series A
          (Statistics in Society)},
  number    = {2},
  pages     = {301--354},
  volume    = {170}
}

@Article{hanley2006pdf,
  author    = {Hanley, James A. and Teltsch, Dana},
  year      = {2006},
  title     = {The PDF of a Function of a Random Variable: Teaching its
          Structure by Transforming Formalism into Intuition},
  comment   = {Use this when explaining the change of variables, always a
          mirky topic!},
  doi       = {10.1198/000313006X89947},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {1},
  pages     = {61--67},
  publisher = {American Statistical Association},
  volume    = {60}
}

@Article{hannig:marron:2006,
  author    = {Hannig, J. and Marron, J. S.},
  year      = {2006},
  title     = {Advanced Distribution Theory for SiZer},
  abstract  = {SiZer is a powerful method for exploratory data analysis.
          In this article approximations to the distributions
          underlying the simultaneous statistical inference are
          investigated, and large improvements are made in the
          approximation using extreme value theory. This results in
          improved size, and also in an improved global inference
          version of SiZer. The main points are illustrated with real
          data and simulated examples.},
  comment   = {Development of theory for SiZer controlling for row-wise
          (i.e., given window width) and global type I error. Nice to
          see the good old tool being updated!},
  doi       = {10.1198/016214505000001294},
  issn      = {0162-1459},
  journal   = {Journal of the American Statistical Association},
  number    = {474},
  pages     = {484--499},
  publisher = {American Statistical Association},
  volume    = {101}
}

@Article{hansen1982large,
  author    = {Hansen, Lars P.},
  year      = {1982},
  title     = {Large Sample Properties of Generalized Method of Moments
          Estimators},
  abstract  = {This paper studies estimators that make sample analogues
          of population orthogonality conditions close to zero.
          Strong consistency and asymptotic normality of such
          estimators is established under the assumption that the
          observable variables are stationary and ergodic. Since many
          linear and nonlinear econometric estimators reside within
          the class of estimators studied in this paper, a convenient
          summary of the large sample properties of these estimators,
          including some whose large sample properties have not
          heretofore been discussed, is provided.},
  doi       = {10.2307/1912775},
  journal   = {Econometrica},
  number    = {4},
  pages     = {1029--1054},
  volume    = {50}
}

@Article{hansen:1982,
  author    = "L. P. Hansen",
  year      = 1982,
  title     = "Large Sample Properties of Generalized Method of Moments
          Estimators",
  journal   = "Econometrica",
  volume    = 12,
  pages     = "347--360"
}

@Article{hansen:1996,
  author    = {Hansen, Bruce E. },
  year      = {1996},
  title     = {Inference When a Nuisance Parameter Is Not Identified
          Under the Null Hypothesis},
  doi       = {10.2307/2171789},
  journal   = {Econometrica},
  number    = {2},
  pages     = {413--430},
  volume    = {64},
  abstract  = {Many econometric testing problems involve nuisance
          parameters which are not identified under the null
          hypotheses. This paper studies the asymptotic distribution
          theory for such tests. The asymptotic distributions of
          standard test statistics are described as functionals of
          chi-square processes. In general, the distributions depend
          upon a large number of unknown parameters. We show that a
          transformation based upon a conditional probability measure
          yields an asymptotic distribution free of nuisance
          parameters, and we show that this transformation can be
          easily approximated via simulation. The theory is applied
          to threshold models, with special attention given to the
          so-called self-exciting threshold autoregressive model.
          Monte Carlo methods are used to assess the finite sample
          distributions. The tests are applied to U.S. GNP growth
          rates, and we find that Potter's (1995) threshold effect in
          this series can be possibly explained by sampling
          variation.}
}

@Article{hansen:2000,
  author    = {Hansen, Bruce E. },
  year      = {2000},
  title     = {Testing for structural change in conditional models},
  doi       = {10.1016/S0304-4076(99)00068-8},
  journal   = {Journal of Econometrics},
  number    = {1},
  pages     = {93--115},
  volume    = {97}
}

@InBook{hansen:2006,
  author    = {Hansen, Bruce},
  editor    = {Corbade, Dean and Durlauf, Steven N. and Hancen, Bruce
          E.},
  year      = {2006},
  title     = {Edgeworth expansions for the Wald and GMM statistics for
          nonlinear regressions},
  abstract  = {The Edgeworth expansion is derived for the GMM distance
          statistic for a real-valued nonlinear restriction on a
          normal linear regression. A refinement of the Edgeworth
          expansion for the Wald statistic (Park and Phillips 1988)
          is provided. The leading coefficients are shown to be the
          same in these two expansions. This establishes that, to the
          order of approximation of the Edgeworth expansion, the GMM
          distance statistic has a better approximation to the
          chi-square distribution than does the Wald statistic.

          The Monte Carlo simulation of Gregory and Veall (1985) is
          updated to include both heteroskedasticity-robust
          covariance matrix estimation and the GMM distance
          statistic. If the robust covariance matrix is calculated
          under the null, the GMM has near perfect finite sample type
          I error even in sample sizes as small as 20.},
  address   = {New York},
  booktitle = {Econometric Theory and Practice},
  chapter   = {1},
  publisher = {Cambridge University Press}
}

@Article{hansen:heaton:yaron:1996,
  author    = {Hansen, Lars P. and Heaton, John and Yaron, Amir },
  year      = {1996},
  title     = {Finite-Sample Properties of Some Alternative {GMM}
          Estimators},
  doi       = {10.2307/1392442},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {3},
  pages     = {262--280},
  volume    = {14},
  abstract  = {We investigate the small-sample properties of three
          alternative generalized method of moments (GMM) estimators
          of asset-pricing models. The estimators that we consider
          include ones in which the weighting matrix is iterated to
          convergence and ones in which the weighting matrix is
          changed with each choice of the parameters. Particular
          attention is devoted to assessing the performance of the
          asymptotic theory for making inferences based directly on
          the deterioration of GMM criterion functions.}
}

@Book{hansen:hurwitz:madow:1953,
  author    = "Morris Hansen and William N. Hurwitz and William G. Madow",
  year      = 1953,
  title     = "Sample Survey Methods and Theory",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{hardin:2002,
  author    = {Hardin, James},
  year      = {2002},
  title     = {The robust variance estimator for two-stage models},
  abstract  = {This article discusses estimates of variance for two-stage
          models. We present the sandwich estimate of variance as an
          alternative to the Murphy–Topel estimate. The sandwich
          estimator has a simple formula that is similar to the
          formula for the Murphy–Topel estimator, and the two
          estimators are asymptotically equal when the assumed model
          distributions are true. The advantages of the sandwich
          estimate of variance are that it may be calculated for the
          complete parameter vector, and that it requires estimating
          equations instead of fully specified log likelihoods.},
  journal   = {The Stata Journal},
  number    = {3},
  pages     = {253--266},
  volume    = {2}
}

@InCollection{hardin:2003,
  author    = {James W. Hardin},
  editor    = {Thomas B. Fomby and R. Carter Hill},
  year      = {2003},
  title     = {The Sandwich Estimator of Variance},
  booktitle = {Maximum Likelihood Estimation of Misspecified Models:
          Twenty Years Later},
  publisher = {Elsevier},
  address   = {New York}
}

@Book{harding:1996,
  author    = "Ann Harding",
  editor    = "",
  year      = "1996",
  title     = "Microsimulation and Public Policy",
  publisher = "North Holland Elsevier",
  address   = "Amsterdam"
}

@Book{hardle:1990,
  author    = {H{\"a}rdle, Wolfgang},
  year      = {1990},
  title     = {Applied Nonparametric Regression},
  publisher = {Cambridge University Press},
  address   = {Cambridge}
}

@Article{hardle:mammen:1993,
  author    = {H{\"a}rdle, W. and Mammen, E. },
  year      = {1993},
  title     = {Comparing Nonparametric Versus Parametric Regression
          Fits},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {1926--1947},
  volume    = {21}
}

@Book{harr:2002,
  author    = "Harrell, F.",
  year      = "2002",
  title     = "Regression Modeling Strategies",
  publisher = "Springer-Verlag",
  address   = "New York"
}

@Book{harrell:2010,
  author    = "Harrell, F.",
  year      = "2010",
  title     = "Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis",
  publisher = "Springer-Verlag",
  address   = "New York",
  edition   = "2nd"
}

@Article{harris:1997,
  author    = "David Harris",
  year      = 1997,
  title     = "Principal Component Analysis of Cointegrated Time Series",
  journal   = "Econometric Theory",
  volume    = 13,
  issue     = 4,
  pages     = "529--557"
}

@InProceedings{hartigan:1985,
  author    = {J. A. Hartigan},
  year      = {1985},
  title     = {A failture of likelihood asymptotics for normal mixtures},
  booktitle = {Proceedings of Berkeley Conference in Honor of J. Neyman
          and J. Kiefer},
  volume    = {2},
  pages     = {807--810},
  address   = {Monterey, CA},
  publisher = {Wadsworth}
}

@Article{hartley:rao:1968,
  author    = {H. O. Hartley and J. N. K. Rao},
  year      = {1968},
  title     = {A new estimation theory for sample surveys},
  journal   = {Biometrika},
  volume    = {55},
  pages     = {547--557}
}

@article{hartley:rao:kiefer:1969,
    abstract =
        {A new solution to the problem of variance estimation with one unit
        per stratum is presented. This method may lead to smaller bias in
        variance estimation, in many situations, than the methods of
        ?collapsed strata?. It requires that we can associate with the strata
        concomitant variables which are correlated with the strata means.
        Several numerical examples with one or two concomitant variables are
        considered.},
    author = {Hartley, H. O. and Rao, J. N. K. and Kiefer, Grace},
    doi = {10.1080/01621459.1969.10501016},
    journal = {Journal of the American Statistical Association},
    number = {327},
    pages = {841--851},
    title = {Variance Estimation with One Unit per Stratum},
    volume = {64},
    year = {1969}
}

@Article{harvey:pierse:1984,
  author    = "A. C. Harvey and R. G. Pierse",
  year      = 1984,
  title     = "Estimating Missing Observations in Economic Time Series",
  journal   = "Journal of the American Statistical Association",
  volume    = 79,
  number    = 385,
  pages     = "125--131"
}

@article{harville:1977,
    author = {Harville, David A.},
    doi = {10.1080/01621459.1977.10480998},
    journal = {Journal of the American Statistical Association},
    number = {358},
    pages = {320--338},
    title = {Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems},
    volume = {72},
    year = {1977},
    abstract = {
        Recent developments promise to increase greatly the popularity of
        maximum likelihood (ml) as a technique for estimating variance
        components. Patterson and Thompson (1971) proposed a restricted
        maximum likelihood (reml) approach which takes into account the loss
        in degrees of freedom resulting from estimating fixed effects. Miller
        (1973) developed a satisfactory asymptotic theory for ml estimators
        of variance components. There are many iterative algorithms that can
        be considered for computing the ml or reml estimates. The
        computations on each iteration of these algorithms are those
        associated with computing estimates of fixed and random effects for
        given values of the variance components.},
}

@Book{harville:1997,
  author    = {David A. Harville},
  year      = {1997},
  title     = {Matrix Algebra from a Statistician's Perspective},
  publisher = {Springer},
  address   = {New York}
}

@Article{hastie:stuetzle:1989,
  author    = {Trevor Hastie and Werner Stuetzle},
  year      = {1989},
  title     = {Principal Curves},
  journal   = {Journal of the American Statistical Association},
  volume    = {84},
  number    = {406},
  pages     = {502--516}
}

@Book{hastie:tibs:1999,
  author    = "Trevor Hastie and Robert Tibshirani",
  year      = 1990,
  title     = "Generalized additive models",
  publisher = "Chapman \& Hall/CRC"
}

@Book{hastie:tibs:fried:2001,
  author    = "Trevor Hastie and Robert Tibshirani and Jerome Friedman",
  year      = 2001,
  title     = "The Elements of Statistical Learning",
  publisher = "Springer-Verlag",
  address   = "New York"
}

@Article{hausman:1978,
  author    = {J. Hausman},
  year      = {1978},
  title     = {Specification tests in econmetrics},
  journal   = {Econometrica},
  volume    = {46},
  pages     = {1251--1271}
}

@Book{hayashi:2000,
  author    = {Fumio Hayashi},
  year      = {2000},
  title     = {Econometrics},
  publisher = {Princeton University Press},
  address   = {Princeton, NJ}
}

@Article{hayashi:bentler:yuan:2007,
  author    = {Hayashi, Kentaro and Bentler, Peter M. and Yuan, Ke-Hai },
  year      = {2007},
  title     = {On the Likelihood Ratio Test for the Number of Factors in
          Exploratory Factor Analysis},
  doi       = {10.1080/10705510701301891},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {3},
  volume    = {14},
  abstract  = {In the exploratory factor analysis, when the number of
          factors exceeds the true number of factors, the likelihood
          ratio test statistic no longer follows the chi-square
          distribution due to a problem of rank deficiency and
          nonidentifiability of model parameters. As a result,
          decisions regarding the number of factors may be incorrect.
          Several researchers have pointed out this phenomenon, but
          it is not well known among applied researchers who use
          exploratory factor analysis. We demonstrate that
          overfactoring is one cause for the well-known fact that the
          likelihood ratio test tends to find too many factors.}
}

@Article{haziza:rao:2006,
  author    = {David Haziza and Jon N.K. Rao},
  year      = {2006},
  title     = {A Nonresponse Model Approach to Inference Under Imputation
          for Missing Survey Data},
  journal   = {Survey Methodology},
  publisher = {Statistics Canada},
  volume    = {32},
  number    = {1},
  pages     = {53--64}
}

@Article{he:ragh:2006,
  author    = {He, Yulei and Raghunathan, Trivellore, E.},
  year      = {2006},
  title     = {Tukey's $gh$ Distribution for Multiple Imputation},
  comment   = {A cute use of a flexible parametric family to draw
          multiple imputations, where the imputation model is
          estimated from and applied to the bootstrap subsamples.},
  doi       = {10.1198/000313006X126819},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {3},
  pages     = {251--256},
  publisher = {American Statistical Association},
  volume    = {60}
}

@Article{headrick:sawilow:1999,
  author    = {Headrick, Todd and Sawilowsky, Shlomo},
  year      = {1999},
  title     = {Simulating correlated multivariate nonnormal
          distributions: Extending the {F}leishman power method},
  abstract  = {A procedure for generating
          multivariate nonnormal distributions is proposed. Our
          procedure generates average values of intercorrelations
          much closer to population parameters than competing
          procedures for skewed and/or heavy tailed distributions and
          for small sample sizes. Also, it eliminates the necessity
          of conducting a factorization procedure on the population
          correlation matrix that underlies the random deviates, and
          it is simpler to code in a programming language (e.g.,
          FORTRAN). Numerical examples demonstrating the procedures
          are given. Monte Carlo results indicate our procedure
          yields excellent agreement between population parameters
          and average values of intercorrelation, skew, and kurtosis.},
  doi       = {10.1007/BF02294317},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {25--35},
  volume    = {64}
}

@Article{heck:hotz:1989,
  author    = {J. Heckman and J. Hotz},
  year      = {1989},
  title     = {Alternative Methods for Evaluating the Impact of Training
          Programs},
  journal   = {Journal of the American Statistical Association},
  volume    = {84},
  number    = 408,
  pages     = {862--880}
}

@Article{heck:ichi:todd:1997,
  author    = {Heckman, J. and H. Ichimura and P. Todd},
  year      = {1997},
  title     = {Matching as an Econometric Evaluation Estimator: Evidence
          from Evaluating a Job Training Program},
  journal   = {Review of Economic Studies},
  volume    = {64},
  pages     = {605--654}
}

@Article{heckman1996identification,
  author    = {Heckman, James J.},
  year      = {1996},
  title     = {Identification of Causal Effects Using Instrumental
          Variables: Comment},
  doi       = {10.2307/2291631},
  journal   = {Journal of the American Statistical Association},
  number    = {434},
  pages     = {459--462},
  volume    = {91}
}

@Book{heda:sloa:stuf:1999,
  author    = {A.S. Hedayat and Neil J. A. Sloane and John Stufken},
  year      = {1999},
  title     = {Orthogonal Arrays: Theory and Applications},
  publisher = {Springer-Verlag},
  series    = {Springer Series in Statistics},
  address   = {New York}
}

@article{hedeker:nordgren:2013,
    author = {Hedeker, Donald and Nordgren, Rachel},
    doi = {10.18637/jss.v052.i12},
    journal = {Journal of Statistical Software},
    number = {12},
    title = {{MIXREGLS}: A Program for Mixed-Effects Location Scale Analysis},
    volume = {52},
    year = {2013}
}

@book{heeringa:west:berglund:2010,
    author = {Heeringa, Steven G. and West, Brady T. and Berglund, Patricia A.},
    isbn = {1420080660},
    publisher = {Chapman and Hall/CRC},
    title = {Applied Survey Data Analysis},
    series = {Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
    year = {2010}
}

@book{heeringa:west:berglund:2017,
    author = {Heeringa, Steven G. and West, Brady T. and Berglund, Patricia A.},
    isbn = {1420080660},
    publisher = {Chapman and Hall/CRC},
    title = {Applied Survey Data Analysis},
    series = {Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
    year = {2017},
    edition = {2nd}
}

@ARTICLE{henry:valliant:2015,
  author =       {Kimberly A. Henry and Richard Valliant},
  title =        {A design effect measure for calibration weighting in single stage samples},
  journal =      {Survey Methodology Journal},
  year =         {2015},
  volume =       {41},
  number =       {2},
  pages =        {315--331},
}


@TechReport{hent:lanj:1996,
  author    = {J. Hentschel and P. Lanjouw},
  year      = {1996},
  title     = {Constructing an Indicator of Consumption for the Analysis
          of Poverty: Principles and Illustrations with Reference to
          {E}cuador},
  institution   = {The World Bank},
  type      = {Living Standards Measurement Study Working Paper},
  number    = {124},
  address   = {Washington, DC, USA}
}

@Conference{hess:polak:daly:2003,
  author    = {S. Hess and J.W. Polak and A. Daly},
  year      = 2003,
  title     = {On the performance of shuffled {H}alton sequences in the
          estimation of discrete choice models},
  organization  = {European Transport Conference},
  address   = {Strasbourg}
}

@Article{hett:rand:1992,
  author    = {Hettmansperger, Thomas P. and Randles, Ronald H.},
  year      = {2002},
  title     = {A Practical Affine Equivariant Multivariate Median},
  abstract  = {A robust affine equivariant estimator of location for
          multivariate data is proposed which becomes the univariate
          median for data of dimension one. The estimator is robust
          in the sense that it has a bounded influence function, a
          positive breakdown value and has high efficiency compared
          to the sample mean for heavy-tailed distributions. Perhaps
          its greatest strength is that, unlike other affine
          equivariant multivariate medians, it is easily computed for
          data in any practical dimension.},
  journal   = {Biometrika},
  number    = {4},
  pages     = {851--860},
  volume    = {89}
}

@article{hessen:dolan:2010,
    author = {Hessen, David J. and Dolan, Conor V.},
    doi = {10.1348/000711007X248884},
    issn = {0007-1102},
    journal = {British Journal of Mathematical and Statistical Psychology},
    number = {1},
    pages = {57--77},
    publisher = {British Psychological Society},
    title = {Heteroscedastic one-factor models and marginal maximum likelihood estimation},
    volume = {62},
    year = {2009},
}

@article{hewitt:2002,
  title={Attitudes toward Interview Mode and Comparability of Reporting
    Sexual Behavior by Personal Interview and Audio Computer-assisted
    Self-interviewing Analyses of the 1995 {N}ational {S}urvey of {F}amily {G}rowth},
  author={Hewitt, Maria},
  journal={Sociological Methods \& Research},
  volume={31},
  number={1},
  pages={3--26},
  year={2002},
}


@Article{heywood:1931,
  author    = {Heywood, H. B. },
  year      = {1931},
  title     = {On Finite Sequences of Real Numbers},
  journal   = {Proceedings of the Royal Society of London. Series A,
          Containing Papers of a Mathematical and Physical
          Character},
  number    = {824},
  pages     = {486--501},
  volume    = {134}
}

@article{hidiroglou:2001,
  title={Double sampling},
  author={Hidiroglou, MA},
  journal={Survey methodology},
  volume={27},
  number={2},
  pages={143--154},
  year={2001},
  note={Available from https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2001002/article/6091-eng.pdf}
}

@Article{hidi:patak:2004,
  author    = {Michael A. Hidiroglou and Zdenek Patak},
  year      = {2004},
  title     = {Domain Estimation Using Linear Regression},
  journal   = {Survey Methodology},
  volume    = {30},
  issue     = {1},
  pages     = {67--78},
  abstract  = {One of the main objectives of a sample survey is the
          computation of estimates of means and totals for specific
          domains of interest. Domains are determined either before
          the survey is carried out (primary domains) or after it has
          been carried out (secondary domains). The reliability of
          the associated estimates depends on the variability of the
          sample size as well as on the y-variables of interest. This
          variability cannot be controlled in the absence of
          auxiliary information for subgroups of the population.
          However, if auxiliary information is available, the
          estimated reliability of the resulting estimates can be
          controlled to some extent. In this paper, we study the
          potential improvements in terms of the reliability of
          domain estimates that use auxiliary information. The
          properties (bias, coverage, efficiency) of various
          estimators that use auxiliary information are compared
          using a conditional approach.}
}

@Article{higham1988computing,
  author    = {Higham, Nicholas J.},
  year      = {1988},
  title     = {Computing a nearest symmetric positive semidefinite
          matrix},
  abstract  = {The nearest symmetric positive semidefinite matrix in the
          Frobenius norm to an arbitrary real matrix A is shown to be
          (B + H)/2, where H is the symmetric polar factor of B=(A +
          AT)/2. In the 2-norm a nearest symmetric positive
          semidefinite matrix, and its distance [delta]2(A) from A,
          are given by a computationally challenging formula due to
          Halmos. We show how the bisection method can be applied to
          this formula to compute upper and lower bounds for
          [delta]2(A) differing by no more than a given amount. A key
          ingredient is a stable and efficient test for positive
          definiteness, based on an attempted Choleski decomposition.
          For accurate computation of [delta]2(A) we formulate the
          problem as one of zero finding and apply a hybrid
          Newton-bisection algorithm. Some numerical difficulties are
          discussed and illustrated by example.},
  comment   = {A useful result that the best psd approximation in
          Frobenius norm is through the spectral decomposition. The
          approximation in L\_2 norm is more complicated, and the
          algorithm is also given.},
  doi       = {10.1016/0024-3795(88)90223-6},
  journal   = {Linear Algebra and its Applications},
  pages     = {103--118},
  volume    = {103}
}

@Article{hilbe:2005:stata,
  author    = {Hilbe, Joseph M. },
  year      = {2005},
  title     = {A Review of {Stata} 9.0},
  doi       = {10.1198/000313005X70984},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {335--348},
  publisher = {American Statistical Association},
  volume    = {59}
}

@Article{hinkley:1977,
  author    = {Hinkley, David V. },
  year      = {1977},
  title     = {Jackknifing in Unbalanced Situations},
  comment   = {doi:10.2307/1267698},
  journal   = {Technometrics},
  number    = {3},
  pages     = {285--292},
  volume    = {19}
}

@Article{hipp:bauer:bollen:2005,
  author    = {Hipp, John R. and Bauer, Daniel J. and Bollen, Kenneth
          A.},
  year      = {2005},
  title     = {Conducting Tetrad Tests of Model Fit and Contrasts of
          Tetrad-Nested Models: A New {SAS} Macro},
  doi       = {10.1207/s15328007sem1201_4},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {1},
  pages     = {76--93},
  volume    = {12},
  abstract  = {This article describes a SAS macro to assess model fit of
          structural equation models by employing a test of the
          model-implied vanishing tetrads. Use of this test has been
          limited in the past, in part due to the lack of software
          that fully automates the test in a user-friendly way. The
          current SAS macro provides a straightforward method for
          researchers to use the vanishing tetrads implied by models
          to assess the fit of (a) structural equation models
          containing continuous endogenous variables; (b) structural
          equation models containing continuous endogenous variables
          nested for vanishing tetrads; and (c) structural equation
          models containing dichotomous, ordinal, or censored
          endogenous variables. Besides providing an alternative
          assessment of model fit to the usual likelihood-ratio test
          (LRT), the vanishing tetrads test occasionally provides a
          statistical assessment of competing models nested for
          vanishing tetrads but not nested for the LRT. The macro
          permits formal comparisons between tetrad-nested structural
          equation models containing dichotomous, ordinal, or
          censored endogenous variables.}
}

@Article{hirsh:shum:2003,
  author    = {David Hirshleifer and Tyler Shumway},
  year      = 2003,
  title     = {Good Day Sunshine: Stock Returns and the Weather},
  journal   = {Journal of Finance},
  volume    = {58},
  number    = {3},
  pages     = {1009--1032}
}

@article{hjort:mckeague:keilegom:2009,
    author = {Hjort, Nils Lid and Mc{K}eague, Ian W. and {Van Keilegom}, Ingrid},
    journal = {Annals of Statistics},
    number = {3},
    pages = {1079--1111},
    title = {Extending the scope of empirical likelihood},
    volume = {37},
    year = {2009},
    abstract =
       {This article extends the scope of empirical likelihood
       methodology in three directions: to allow for plug-in
       estimates of nuisance parameters in estimating equations,
       slower than $\sqrt{n}$-rates of convergence, and settings
       in which there are a relatively large number of estimating
       equations compared to the sample size. Calibrating empirical
       likelihood confidence regions with plug-in is sometimes
       intractable due to the complexity of the asymptotics, so we
       introduce a bootstrap approximation that can be used in such
       situations. We provide a range of examples from survival
       analysis and nonparametric statistics to illustrate the main
       results.},
}

@Article{hoar:mill:nych:wikle:berl:2003,
  author    = "Timothy J. Hoar and Ralph E. Milliff and Douglas Nychka
          and Christopher K. Wikle and L. Mark Berliner",
  year      = 2003,
  title     = "Winds from a {B}ayesian Hierarchical Model: Computation
          for Atmosphere-Ocean Research",
  journal   = "Journal of Computational and Graphical Statistics",
  volume    = 12,
  issue     = 4,
  pages     = "781--807"
}

@article{hobert:jones:2004,
    author = {Hobert, James P. and Jones, Galin L.},
    doi = {10.1214/009053604000000184},
    journal = {The Annals of Statistics},
    number = {2},
    pages = {784--817},
    title = {Sufficient burn-in for {Gibbs} samplers for a hierarchical random effects model},
    volume = {32},
    year = {2004}
}

@Article{hoef:1950s,
  author    = "Wassily Hoeffding",
  year      = "1948",
  title     = "A class of statistics with asymptotically normal
          distribution",
  journal   = "Annals of Mathematical Statistics",
  pages     = "293--325",
  volume    = 19
}

@Article{hoff2005bilinear,
  author    = {Hoff, Peter D.},
  year      = {2005},
  title     = {Bilinear Mixed-Effects Models for Dyadic Data},
  doi       = {10.1198/016214504000001015},
  issn      = {0162-1459},
  journal   = {Journal of the American Statistical Association},
  number    = {469},
  pages     = {286--295},
  publisher = {American Statistical Association},
  volume    = {100}
}

@BOOK{hoffman:2015,
  AUTHOR =       {Lesa Hoffman},
  TITLE =        {Longitudinal Analysis: Modeling Within-Person Fluctuation and Change},
  PUBLISHER =    {Rutledge Press (Taylor \& Francis)},
  YEAR =         {2015},
  address =      {New York},
}

@Article{hogan:laird:1998,
  author    = {Hogan, Joseph W. and Laird, Nan M.},
  year      = {1997},
  title     = {MODEL-BASED APPROACHES TO ANALYSING INCOMPLETE
          LONGITUDINAL AND FAILURE TIME DATA},
  abstract  = {Since Wu and Carroll (Biometrics 44, 175-188) proposed a
          model for longitudinal progression in the presence of
          informative dropout, several researchers have developed and
          studied models for situations where both a vector of
          repeated outcomes and an event time is available for each
          subject. These models have been developed for either
          longitudinal studies with dropout or for survival studies
          in which a random, time-varying covariate is measured
          repeatedly across time. When inference about the
          longitudinal variable is of interest, event times are
          treated as covariates and are often incomplete due to
          censoring. If survival or event time is the primary
          endpoint, repeated outcomes observed prior to the event are
          viewed as covariates; this covariate process is often
          incomplete, measured with error, or observed at unscheduled
          times during the study. We review several models which are
          used to handle incomplete response and covariate data in
          both survival and longitudinal studies. {\copyright} 1997
          by John Wiley \& Sons, Ltd.},
  address   = {Center for Statistical Sciences, Brown University, Box
          G-H, Providence, RI 02192, U.S.A.; Department of
          Biostatistics, Harvard School of Public Health, 677
          Huntington Avenue Boston, MA 02115, U.S.A.},
  doi       = {10.1002/(SICI)1097-0258(19970215)16:3\%3C259::AID-SIM484\%3E3.0.CO;2-S}
          ,
  journal   = {Statistics in Medicine},
  number    = {3},
  pages     = {259--272},
  volume    = {16}
}

@Article{hogan:lin:herman:2004,
  author    = {Hogan, Joseph W. and Lin, Xihong and Herman, Benjamin},
  title     = {Mixtures of Varying Coefficient Models for Longitudinal
          Data with Discrete or Continuous Nonignorable Dropout},
  doi       = {10.1111/j.0006-341X.2004.00240.x},
  issn      = {0006-341X},
  journal   = {Biometrics},
  number    = {4},
  pages     = {854+},
  publisher = {Blackwell Publishing},
  volume    = {60}
}

@article{holb:cho:john:2006,
    abstract =
        {Behavior coding is one technique researchers use to detect problems
        in survey questions, but it has been primarily explored as a
        practical tool rather than a source of insight into the theoretical
        understanding of the cognitive processes by which respondents answer
        survey questions. The latter is the focus of the current
        investigation. Using data from a large study in which face-to-face
        interviews were taped and extensive behavior coding was done, we
        tested whether sets of respondent behavior codes could be used to
        distinguish respondent difficulties with comprehension of the
        question from difficulties associated with mapping a judgment onto
        the response format provided, and whether characteristics of the
        survey questions and respondents could be used to predict when and
        for whom such difficulties would occur. Sets of behavior codes were
        identified that reflected comprehension and mapping difficulties, and
        these two types of difficulties were associated with different
        question and respondent characteristics. This evidence suggests that
        behavior coding shows promise as a tool for researchers studying the
        cognitive processes involved in answering survey questions.},
    author = {Holbrook, Allyson and Cho, Young I. K. and Johnson, Timothy},
    doi = {10.1093/poq/nfl027},
    journal = {Public Opinion Quarterly},
    number = {4},
    pages = {565--595},
    title = {The Impact of Question and Respondent Characteristics on Comprehension and Mapping Difficulties},
    volume = {70},
    year = {2006}
}

@Article{holl:deol:cox:smith:2000,
  author    = "Holland, D.~M. and De Oliveira, V. and Cox, L.~H. and
          Smith, R.~L.",
  year      = 2000,
  title     = "Estimation of regional trends in sulfur dioxide over the
          eastern United States",
  journal   = "Environmetrics",
  volume    = 11,
  pages     = "373--393"
}

@Book{holl:wolfe:1999,
  author    = {Hollander, Myles and Wolfe, Douglas A. },
  year      = {1999},
  title     = {Nonparametric Statistical Methods, 2nd Edition},
  isbn      = {0471190454},
  publisher = {Wiley-Interscience},
  address   = {New York}
}

@Article{holt:smith:1979,
  author    = {Holt, D. and Smith, T. M. F. },
  year      = {1979},
  title     = {Post Stratification},
  journal   = {Journal of the Royal Statistical Society, Series A},
  number    = {1},
  pages     = {33--46},
  volume    = {142},
  abstract  = {Post stratification is usually judged in the context of
          the variance of the post stratification estimator taken
          over all possible sample configurations appropriately
          weighted by the probability of occurrence. The generally
          accepted view is that, in most cases, the technique has
          little to offer over using the sample mean. This basis for
          evaluating the technique is questioned and the appropriate
          framework for statistical inference discussed. It is argued
          that inferences should be made conditional on the achieved
          sample configuration. The theory developed shows that
          neither the post stratification estimator nor the sample
          mean is uniformly best in all situations but empirical
          investigations indicate that post stratification offers
          protection against unfavourable sample configurations and
          should be viewed as a robust technique.}
}

@TechReport{holz:swin:1939,
  author    = {Holzinger, K. J. and Swineford, F.},
  year      = {1939},
  title     = {A study in factor analysis: The stability of a bi-factor
          solution},
  institution   = {University of Chicago},
  number    = {48},
  address   = {Chicago, IL}
}

@Article{hong:hong:2007,
  author    = {R. Hong and R. Hong},
  year      = {2007},
  title     = {Economic inequality and undernutrition in women:
          Multilevel analysis of individual, household, and community
          levels in {C}ambodia},
  journal   = {Food and Nutrition Bulletin},
  volume    = {28},
  number    = {1},
  pages     = {59--66}
}

@Article{horn:1965,
  author    = {J. K. Horn},
  year      = {1965},
  title     = {A Rationale and Test for the Number of Factors in Factor
          Analysis},
  journal   = {Psychometrika},
  volume    = {30},
  pages     = {179--186},
  source    = {lattin:carroll:green:2003}
}

@Book{horn:john:1990,
  author    = {Roger A. Horn and Charles R. Johnson},
  year      = {1990},
  title     = {Matrix Analysis},
  publisher = {Cambridge University Press},
  address   = {New York},
  edition   = {reprint}
}

@Book{horn:johnson:1990,
  author    = "Roger A. Horn and Charles R. Johnson",
  year      = 1990,
  title     = "Matrix Analysis",
  publisher = "Cambridge University Press",
  address   = "Cambridge, UK"
}

@TECHREPORT{horn:1998,
  AUTHOR =       {Horn, L.},
  TITLE =        {Stopouts or stayouts? Undergraduates who leave college in their first year},
  YEAR =         {1998},
  INSTITUTION =  {US Department of Education, National Center for Education Statistics},
  number =       {1999-087},
  source =       {Isabella Zaniletti},
}



@Article{horvitz:thompson:1952,
  author    = {Horvitz, D. G. and Thompson, D. J. },
  year      = {1952},
  title     = {A Generalization of Sampling Without Replacement From a
          Finite Universe},
  journal   = {Journal of the American Statistical Association},
  number    = {260},
  pages     = {663--685},
  volume    = {47},
  doi       = {10.2307/2280784},
  abstract  = {This paper presents a general technique for the treatment
          of samples drawn without replacement from finite universes
          when unequal selection probabilities are used. Two sampling
          schemes are discussed in connection with the problem of
          determining optimum selection probabilities according to
          the information available in a supplementary variable.
          Admittedly, these two schemes have limited application.
          They should prove useful, however, for the first stage of
          sampling with multi-stage designs, since both permit
          unbiased estimation of the sampling variance without
          resorting to additional assumptions.}
}

@book{hosmer:lemeshow:sturdivant:2013,
    author = {Hosmer, David W. and Lemeshow, Stanley and Sturdivant, Rodney X.},
    edition = {3},
    howpublished = {Hardcover},
    isbn = {0470582472},
    publisher = {Wiley},
    title = {Applied Logistic Regression (Wiley Series in Probability and Statistics)},
    year = {2013}
}

@Article{hotelling:pca,
  author    = "Harold Hotelling",
  year      = "1933",
  title     = "Analysis of a complex of statistical variables into
          principal components",
  journal   = "Journal of Educational Psychology",
  pages     = "417-441, 498--520",
  volume    = "24"
}

@Article{hothorn2006lego,
  author    = {Hothorn and Torsten and Hornik and Kurt and de Wiel, Van
          and Mark, A. and Zeileis and Achim},
  year      = {2006},
  title     = {A Lego System for Conditional Inference},
  comment   = {The authors argue that a family of permutation-type tests
          can be constructed by choosing: different transformations
          of the explanatory variables \$g\$; different influence
          functions of the response variables \$h\$ (nothing to do
          with robust statistics, in fact); different possible
          mappings of the matrix-valued test statistics \$gh'\$ to
          the real line; and different approximations of the
          asymptotic distribution of the latter. Couldn't get all of
          that, but looks interesting.},
  doi       = {10.1198/000313006X118430},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {3},
  pages     = {257--263},
  publisher = {American Statistical Association},
  volume    = {60}
}

@Book{hox:2003,
  author    = "Hox, Joop",
  year      = 2003,
  title     = "Multilevel Analysis: Techniques and Applications",
  publisher = "Lawrence Erlbaum"
}

@Book{hoyle1995structural,
  author    = {Hoyle, Rick H.},
  year      = {1995},
  title     = {Structural Equation Modeling: Concepts, Issues, and
          Applications},
  abstract  = {{<p>Practical and up-to-date, <b>Structural Equation
          Modeling</b> includes chapters on major aspects of the
          structural equation modeling approach to research design
          and data analysis. Written by internationally recognized
          leaders in structural equation modeling, this book targets
          graduate students and seasoned researchers in the social
          and behavioral sciences who wish to understand the basic
          concepts and issues associated with the structural equation
          modeling approach and applications to research problems.
          Though technically sound, the chapters are primarily
          nontechnical in content and stylemaking the volume an
          excellent introduction to the structural equation modeling
          approach for readers studied in traditional inferential
          statistics. Early chapters are devoted to fundamental
          concepts such as estimation, fit, assumptions, power, and
          inference. Later chapters address such practical issues as
          the use of computer programs for applying the approach to
          research questions in the social and behavioral sciences.
          </p>}},
  howpublished  = {Paperback},
  isbn      = {0803953186},
  publisher = {{Sage Publications, Inc}}
}

@Book{hoyle:1995,
  author    = {Hoyle, Rick H. },
  year      = {1995},
  title     = {Structural Equation Modeling: Concepts, Issues, and
          Applications},
  isbn      = {0803953186},
  publisher = {Sage Publications, Inc}
}

@Book{hsiao:hammond:holly:2002,
  editor    = "Cheng Hsiao and Peter Hammond and Alberto Holly",
  year      = 2002,
  title     = "Analysis of Panel Data",
  publisher = "Cambridge University Press",
  edition   = "2nd"
}

@Article{hu:bentler:kano:1992,
  author    = {Hu, L. T. and Bentler, P. M. and Kano, Y. },
  year      = {1992},
  title     = {Can test statistics in covariance structure analysis be
          trusted?},
  journal   = {Psychological Bulletin},
  number    = {2},
  pages     = {351--362},
  volume    = {112},
  abstract  = {Covariance structure analysis uses $\chi_2$
          goodness-of-fit test statistics whose adequacy is not
          known. Scientific conclusions based on models may be
          distorted when researchers violate sample size, variate
          independence, and distributional assumptions. The behavior
          of 6 test statistics is evaluated with a Monte Carlo
          confirmatory factor analysis study. The tests performed
          dramatically differently under 7 distributional conditions
          at 6 sample sizes. Two normal-theory tests worked well
          under some conditions but completely broke down under other
          conditions. A test that permits homogeneous nonzero
          kurtoses performed variably. A test that permits
          heterogeneous marginal kurtoses performed better. A
          distribution-free test performed spectacularly badly in all
          conditions at all but the largest sample sizes. The
          Satorra-Bentler scaled test statistic performed best
          overall.}
}

@InProceedings{huber:1967,
  author    = "Huber, Peter",
  year      = 1967,
  title     = "The behavior of the maximum likelihood estimates under
          nonstandard conditions",
  booktitle = "Proceedings of the Fifth Berkeley Symposium on
          Mathematical Statistics and Probability",
  volume    = 1,
  pages     = "221--233",
  publisher = "University of California Press",
  address   = "Berkeley"
}

@Book{huber:1974,
  author    = "Huber, Peter",
  year      = 1974,
  title     = "Robust Statistics",
  publisher = "Wiley",
  address   = "New York"
}

@Book{huber:2003,
  author    = "Peter J. Huber",
  year      = 2003,
  title     = "Robust Statitsics",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@TechReport{huff:eltinge:gersh:2002,
  author    = {Larry L. Huff and John L. Eltinge and Julie Gershunskaya},
  year      = {2002},
  title     = {Exploratory Analysis of Generalized Variance Function
          Models for the {U.S. Current Employment Survey}},
  institution   = {BLS},
  type      = {technical report},
  number    = {020120},
  note      = {http://www.bls.gov/ore/abstract/st/st020120.htm}
}

@BOOK{hunderpol:etal:2012,
  author =       {Arco Hunderpol and Josep Domingo-Ferrer and Luisa Franconi
                    and Sarah Giessing and Eric Schulte Nordholt and
                    Keith Spicer and Peter-Paul de Wolf},
  title =        {Statistical Disclosure Control},
  publisher =    {Wiley},
  year =         {2012},
  series =       {Wiley Series in Survey Methodology},
  address =      {Hoboken, NJ},
}


@Article{hurd:1999,
  author    = {Hurd, Michael D. },
  year      = {1999},
  title     = {Anchoring and Acquiescence Bias in Measuring Assets in
          Household Surveys},
  doi       = {10.1023/A:1007819225602},
  journal   = {Journal of Risk and Uncertainty},
  number    = {1},
  pages     = {111--136},
  volume    = {19},
  abstract  = {Cognitive psychology has identified and studied
          extensively a number of cognitive anomalies that may be
          important for the assessment of the economic status of
          individuals and households. In particular the use of
          brackets to elicit information about income and assets in
          surveys of households can interact with acquiescence bias
          and anchoring to cause bias in the estimates of the
          distributions of income and assets. This paper uses data
          from the Health and Retirement Study and the Asset and
          Health Dynamics Study to find that, as predicted by
          psychology, bracketing can produce bias in population
          estimates of assets.}
}

@Book{hyva:karh:oja:2001,
  author    = {Aapo Hyv{\"a}rinen and Juha Karhunen and Erkki Oja},
  year      = {2001},
  title     = {Independent Component Analysis},
  publisher = {Wiley-Interscience},
  series    = {Wiley Series on Adaptive and Learning Systems for Signal
          Processing, Communications, and Control},
  address   = {New York}
}

@inproceedings{iannacchione:milne:folsom:1991,
    author = {Iannacchione, V.G. and Milne, J.G. and Folsom, R.E.},
    organization = {Survey Research Methodology Section},
    pages = {637--642},
    publisher = {American Statistical Association},
    title = {Response probability weight adjustments using logistic regression},
    year = {1991},
    booktitle = {Proceedings of the Survey Research Methods Section}
}

@Article{ibra:1990,
  author    = "Ibrahim, Joseph G.",
  year      = 1990,
  title     = "Incomplete Data in Generalized Linear Models",
  journal   = "Journal of the American Statistical Association",
  volume    = 85,
  pages     = "765--769"
}

@article{ibrahim:chen:2000,
  title={Power prior distributions for regression models},
  author={Ibrahim, Joseph G and Chen, Ming-Hui},
  journal={Statistical Science},
  pages={46--60},
  volume = 15,
  year={2000},
}

@Article{ibra:chen:lips:2001,
  author    = {Ibrahim, Joseph G. and Chen, Ming-Hui and Lipsitz, Stuart
          R.},
  year      = 2001,
  title     = {Missing Responses in Generalised Linear Mixed Models When
          the Missing Data Mechanism Is Nonignorable},
  journal   = {Biometrika},
  volume    = 88,
  number    = 2,
  pages     = {551--564}
}

@Book{idr:2005,
  author    = {NAS/NAE/IM},
  year      = {2005},
  title     = {Facilitating Interdisciplinary Research},
  publisher = {The National Academic Press},
  address   = {Washington, DC},
  comment   = {National Academy of Sciences and National Academy of
          Engineering and Institute of Medicine}
}

@Article{imbens:2002,
  author    = {Imbens, G. W. },
  year      = {2002},
  title     = {Generalized Method of Moments and Empirical Likelihood},
  journal   = {Journal of Business and Economic Statistics},
  pages     = {493--506},
  abstract  = {Generalized method of moments (GMM) estimation has become
          an important unifying framework for inference in
          econometrics in the last 20 years. It can be thought of as
          encompassing almost all of the common estimation methods,
          such as maximum likelihood, ordinary least squares,
          instrumental variables, and two-stage least squares, and
          nowadays is an important part of all advanced econometrics
          textbooks. The GMM approach links nicely to economic theory
          where orthogonality conditions that can serve as such
          moment functions often arise from optimizing behavior of
          agents. Much work has been done on these methods since the
          seminal article by Hansen, and much remains in progress.
          This article discusses some of the developments since
          Hansen\&\#039;s original work. In particular, it focuses on
          some of the recent work on empirical likelihood\&\#150;type
          estimators, which circumvent the need for a first step in
          which the optimal weight matrix is estimated and have
          attractive information theoretic interpretations.},
  volume    = 20,
  issue     = 4
}

@InCollection{imbens:spady:2005,
  author    = {Imbens, Guido W. and Spady, Richard H.},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {The performance of Empirical Likelihood and its
          Generalizations},
  abstract  = {We calculate higher-order asymptotic biases and mean
          squared errors (MSE) for a simple model with a sequence of
          moment conditions. In this setup, generalized empirical
          likelihood (GEL) and infeasible optimal GMM (OGMM) have the
          same higher-order biases, with GEL having an MSE that
          exceeds OGMM's by an additional term of order (M-1)/N, i.e.
          the degree of overidentification divided by sample size. In
          contrast, any 2-step GMM estimator has an additional bias
          relative to OGMM of order (M-1)/N and an additional MSE of
          order (M-1)^2/N. Consequently GEL must be expected to
          dominate 2-step GMM. In our simple model all GEL's have
          equivalent next higher order behavior because generalized
          third moments of moment conditions are assumed to be zero;
          we explore, in further analysis and simulations, the
          implications of dropping this assumption.},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {10},
  publisher = {Cambridge University Press}
}

@MANUAL{ipums:2010,
  TITLE =        {Integrated Public Use Microdata Series: Version 5.0 [Machine-readable database]},
  author =       {Steven Ruggles and J. Trent Alexander and Katie Genadek and Ronald Goeken and Matthew B. Schroeder and Matthew Sobek},
  organization = {University of Minnesota},
  address =      {Minneapolis},
  year =         {2010},
}

@MANUAL{ipums:2015:cps,
  TITLE =        {Integrated Public Use Microdata Series, Current Population Survey: Version 4.0. [Machine-readable database]},
  author =       {Sarah Flood and Miriam King and Steven Ruggles and J. Robert Warren},
  organization = {University of Minnesota},
  address =      {Minneapolis},
  year =         {2015},
}

@ARTICLE{isaki:tsay:fuller:2004,
  AUTHOR =       {C. T. Isaki and J. H. Tsay and W. A. Fuller},
  TITLE =        {Weighting sample data subject to independent controls},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2004},
  volume =       {30},
  number =       {1},
  pages =        {35--44},
}

@INPROCEEDINGS{izrael:batt:batt:ball:2017,
  author =       {David Izrael and Michael P. Battaglia and Annabella A. Battaglia and Sarah W. Ball},
  title =        {You Do Not Have To Step On The Same Rake: SAS Raking Macro---Generation IV},
  booktitle =    {{SAS} Global Forum},
  year =         {2017},
  note =         {Available at https://support.sas.com/resources/papers/proceedings17/0470-2017-poster.pdf},
}


@Article{jacquier:polson:rossi:1994,
  author    = "E. Jacquier and N. Polson and P. Rossi",
  year      = 1994,
  title     = "Bayesian analysis of stochastic volatility models",
  journal   = "Journal of Business and Economic Statistics",
  volume    = 12,
  pages     = "371--417"
}

@Article{jacquier:polson:rossi:2004,
  author    = {Jacquier, Eric and Polson, Nicholas G. and Rossi,
          P.E.Peter E.},
  year      = 2004,
  title     = {Bayesian analysis of stochastic volatility models with
          fat-tails and correlated errors},
  journal   = {Journal of Econometrics},
  volume    = {122},
  number    = {1},
  pages     = {185--212}
}

@article{jacq:titm:yalc:2009,
  title={Predicting Systematic Risk: Implications from Growth Options},
  author={Jacquier, Eric and Titman, Sheridan and Yal\c{c}in, Atakan},
  journal={Review of Financial Studies},
  volume={22},
  number={1},
  pages={435--480},
  year={2009}
}


@Book{jahn:1996,
  author    = "Johannes Jahn",
  year      = 1996,
  title     = "Introduction to the Theory of Nonlinear Optimization",
  publisher = "Springer",
  address   = "New York",
  edition   = "2nd revised"
}

@Article{jamsh:bentler:1994,
  author    = {Jamshidian, Mortaza and Bentler, Peter M. },
  year      = {1994},
  title     = {Gramian Matrices in Covariance Structure Models},
  journal   = {Applied Psychological Measurement},
  number    = {1},
  pages     = {79--94},
  volume    = {18},
  abstract  = { A general approach is proposed to avoid improper
          solutions in structural equation models. The constrained
          estimation approach presented, which is based on an
          adaptation of a globally convergent method for nonlinear
          programming, has worked well in all trials. }
}

@Article{jamsh:bentler:1999,
  author    = {Jamshidian, Mortaza and Bentler, Peter M. },
  year      = {1999},
  title     = {{ML} Estimation of Mean and Covariance Structures with
          Missing Data Using Complete Data Routines},
  journal   = {Journal of Educational and Behavioral Statistics},
  number    = {1},
  pages     = {21--41},
  volume    = {24}
}

@ARTICLE{jang:eltinge:2009,
  author =       {Donsig Jang and John L. Eltinge},
  title =        {Use of within-primary-sample-unit variances to assess the stability of a standard design-based variance estimator},
  journal =      {Survey Methodology},
  year =         {2009},
  volume =       {35},
  number =       {2},
  pages =        {235--245},
}


@Article{jann:2005,
  author    = {Ben Jann},
  year      = {2005},
  title     = {Making regression tables from stored estimates},
  journal   = {Stata Journal},
  volume    = {5},
  number    = {3},
  pages     = {288--308}
}

@ARTICLE{jenkins:1995,
  AUTHOR =       {Jenkins, S. P.},
  TITLE =        {Easy estimation methods for discrete-time duration models},
  JOURNAL =      {Oxford Bulletin of Economics and Statistics},
  YEAR =         {1995},
  volume =       {57},
  pages =        {129--138},
}


@Article{jenkins:1999,
  author    = {Stephen P. Jenkins},
  year      = {1999},
  title     = {Analysis of income distributions},
  journal   = {Stata Technical Bulletin},
  volume    = {48},
  pages     = {4--18}
}

@Article{jenkins:cowell:1994,
  author    = {Jenkins, Stephen P. and Cowell, Frank A.},
  year      = {1994},
  title     = {Parametric Equivalence Scales and Scale Relativities},
  abstract  = {We respond to Banks and Johnson's (1994) Comment on
          Coulter et al. (1992) drawing on a more general discussion
          of parametric equivalence scale and scale relativity issues
          and new empirical results. We show that criticisms of our
          earlier work are unfounded. When the McClements scale is
          properly characterised, the scale does indeed provide lower
          estimates of poverty and inequality levels than most other
          scales. We reiterate our conclusion that relationships
          between scale relativities and inequality and poverty
          indices may be index-specific. Moreover the picture about
          distributional trends may differ from that about levels.},
  doi       = {10.2307/2234983},
  issn      = {00130133},
  journal   = {The Economic Journal},
  number    = {425},
  pages     = {891--900},
  publisher = {Blackwell Publishing for the Royal Economic Society},
  volume    = {104}
}

@Article{jenkins:2009,
  author={Stephen P. Jenkins},
  title={Distributionally-Sensitive Inequality Indices And The {GB2} Income Distribution},
  journal={Review of Income and Wealth},
  year=2009,
  volume={55},
  number={2},
  pages={392-398},
  month={06},
}

@Article{jennrich:2008,
  author    = {Jennrich, Robert },
  year      = 2008,
  title     = {Nonparametric Estimation of Standard Errors in Covariance
          Analysis Using the Infinitesimal Jackknife},
  doi       = {10.1007/s11336-008-9083-y},
  journal   = {Psychometrika},
  volume    = 73,
  number    = 4,
  pages     = {579--594},
  abstract  = {The infinitesimal jackknife provides a simple general
          method for estimating standard errors in covariance
          structure analysis. Beyond its simplicity and generality
          what makes the infinitesimal jackknife method attractive is
          that essentially no assumptions are required to produce
          consistent standard error estimates, not even the
          requirement that the population sampled has the covariance
          structure assumed. Commonly used covariance structure
          analysis software uses parametric methods for estimating
          parameters and standard errors. When the population sampled
          has the covariance structure assumed, but fails to have the
          distributional form assumed, the parameter estimates
          usually remain consistent, but the standard error estimates
          do not. This has motivated the introduction of a variety of
          nonparametric standard error estimates that are consistent
          when the population sampled fails to have the
          distributional form assumed. The only distributional
          assumption these require is that the covariance structure
          be correctly specified. As noted, even this assumption is
          not required for the infinitesimal jackknife. The relation
          between the infinitesimal jackknife and other nonparametric
          standard error estimators is discussed. An advantage of the
          infinitesimal jackknife over the jackknife and the
          bootstrap is that it requires only one analysis to produce
          standard error estimates rather than one for every
          jackknife or bootstrap sample.}
}

@Article{jessen:1942,
  author    = "R. J. Jessen",
  year      = "1942",
  title     = "Statistical Investigaion of a Farm Survey for Obtaining
          Farm Facts",
  journal   = "Iowa Agricultural Station Research Bulletin",
  volume    = "304",
  pages     = "54--59"
}

@TechReport{jian:sachs:warn:1996,
  author    = "Tianlun Jian and Jeffrey D. Sachs and Andrew M. Warner",
  year      = 1996,
  title     = "Trends in Regional Inequality in {China}",
  institution   = "NBER",
  type      = "Working Paper",
  number    = "W5412"
}

@article{jiang:lahiri:2001,
    abstract =
        {The paper introduces a frequentist's alternative to the recently
        developed hierarchical Bayes methods for small area estimation with
        binary data. Specifically, the best predictor ({BP}) and empirical
        best predictor ({EBP}) of small area specific random effect are
        developed in the context of a mixed logistic model and different
        asymptotic properties of the proposed {BP} and {EBP} are studied. An
        approximation to the mean squared error ({MSE}) of the proposed {EBP}
        correct up to the order o(m-1) is obtained, where m denotes the
        number of small areas. The asymptotic behavior of the relative
        savings loss ({RSL}) demonstrates the superiority of the proposed
        {EBP} over the usual small area proportion.},
    author = {Jiang, Jiming and Lahiri, P.},
    journal = {Annals of the Institute of Statistical Mathematics},
    doi = {10.1023/a:1012410420337},
    number = {2},
    pages = {217--243},
    title = {Empirical Best Prediction for Small Area Inference with Binary Data},
    volume = {53},
    year = {2001}
}

@INCOLLECTION{john:sriv:1999:big5,
  author =       {John, O. P. and Srivastava, S. },
  title =        {The {Big} {Five} trait taxonomy: History, measurement, and theoretical perspectives},
  booktitle =    {Handbook of personality: Theory and research},
  publisher =    {Guilford Press},
  year =         {1999},
  editor =       {L. A. Pervin and O. P. John},
  pages =        {102?-138},
  address =      {New York},
}


@Article{john:1978,
  author    = {Johnson, Norman J.},
  year      = {1978},
  title     = "Modified $t$ Tests and Confidence Intervals for
          Asymmetrical Populations",
  journal   = {Journal of the American Statistical Association},
  volume    = {73},
  pages     = {536--544}
}

@Book{john:kotz:bala:1997,
  author    = "Norman L. Johnson and Samuel Kotz and N. Balakrishnan",
  year      = 1997,
  title     = "Discrete Multivariate Distributions",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{johnson:bodner:2007,
  author    = {Johnson, Timothy R. and Bodner, Todd E. },
  year      = {2007},
  title     = {A Note on the Use of Bootstrap Tetrad Tests for Covariance
          Structures},
  doi       = {10.1080/10705510709336739},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {1},
  pages     = {113--124},
  volume    = {14},
  abstract  = {Tetrad tests are a valuable tool for testing covariance
          structures, particularly when other tests such as the
          likelihood ratio test are not appropriate. However the
          implementation of tetrad tests is complicated by the fact
          that there are often multiple sets of nonredundant
          vanishing tetrads, and also by computational limitations,
          particularly for bootstrap tetrad tests for models with a
          large number of indicator variables. We show that a simple
          modification of a bootstrap tetrad test can overcome these
          limitations. Another beneficial result of this modification
          is that it can increase the power of the test.}
}

@Article{johnson:creech:1983,
  author    = "David Richard Johnson and James C. Creech",
  year      = 1983,
  title     = "Ordinal Measures in Mulitple Indicator Models: A
          Simulation Study of Categorization Error",
  journal   = "American Sociological Review",
  volume    = 48,
  pages     = "398--407"
}

@Book{johnson:kotz:bala:1994,
  author    = {Johnson, Norman L. and Kotz, Samuel and Balakrishnan, N.},
  title     = {Continuous Univariate Distributions, Vol. 1 (Wiley Series
          in Probability and Statistics)},
  abstract  = {Volume 1 has been fully revised and updated, and
          significantly expanded to include the latest advances in
          the theory, methodology, computational procedures, and
          application of continuous distributions. DLC: Distribution
          (Probability theory)},
  edition   = {2},
  howpublished  = {Hardcover},
  isbn      = {0471584959},
  publisher = {Wiley-Interscience}
}

@Book{johnson:kotz:bala:1995,
  author    = {Johnson, Norman L. and Kotz, Samuel and Balakrishnan, N.},
  title     = {Continuous Univariate Distributions, Vol. 2 (Wiley Series
          in Probability and Statistics)},
  edition   = {2nd},
  howpublished  = {Hardcover},
  isbn      = {978-0471584940},
  publisher = {Wiley-Interscience}
}

@Book{johnson:wichern:2002,
  author    = {Richard A. Johnson and Dean W. Wichern},
  year      = {2002},
  title     = {Applied Multivariate Statistical Analysis},
  publisher = {Prentice Hall},
  edition   = {5th},
  address   = {Englewood Cliffs, NJ}
}

@book{johnson:2015:handbook,
    address = {Hoboken, NJ},
    editor = {Johnson, Timothy P.},
    publisher = {Wiley},
    series = {Wiley Handbooks in Survey Methodology},
    title = {Handbook of Health Survey Methods},
    year = {2015}
}
@Article{johnstone:2001,
  author    = {Johnstone, Iain M.},
  year      = {2001},
  title     = {On the distribution of the largest eigenvalue in principal
          components analysis},
  abstract  = {Let \$x\_{(1)}\$ denote the square of the largest singular
          value of an n ? p matrix X, all of whose entries are
          independent standard Gaussian variates. Equivalently,
          \$x\_{(1)}\$ is the largest principal component variance of
          the covariance matrix \$X'X\$, or the largest eigenvalue of
          a p-variate Wishart distribution on n degrees of freedom
          with identity covariance.

          Consider the limit of large p and n with \$n/p = \gamma \ge
          1\$. When centered by \$\mu\_p = (\sqrt{n-1} +
          \sqrt{p})^2\$ and scaled by \$\sigma\_p = (\sqrt{n-1} +
          \sqrt{p})(1/\sqrt{n-1} + 1/\sqrt{p}^{1/3}\$, the
          distribution of x(1) approaches the Tracey-Widom law of
          order 1, which is defined in terms of the Painlev\'{e} II
          differential equation and can be numerically evaluated and
          tabulated in software. Simulations show the approximation
          to be informative for n and p as small as 5.

          The limit is derived via a corresponding result for complex
          Wishart matrices using methods from random matrix theory.
          The result suggests that some aspects of large p
          multivariate distribution theory may be easier to apply in
          practice than their fixed p counterparts.},
  doi       = {doi:10.1214/aos/1009210544},
  journal   = {Annals of Statistics},
  number    = {2},
  pages     = {295--327},
  volume    = {29}
}

@Article{johnstone:pca,
  author    = "Iain M. Johnstone",
  year      = "2001",
  title     = "On the distribution of the largest eigenvalue in principal
          component analysis",
  journal   = "Annals of Statistics",
  pages     = "295--327",
  volume    = "29"
}

@Book{jolliffe:2002,
  author    = {I.T. Jolliffe},
  year      = {2002},
  title     = {Principal Component Analysis},
  publisher = {Springer},
  address   = {New York},
  edition   = {2nd}
}

@Article{jones1990serial,
  author    = {Jones, Richard H. and Ackerson, Lynn M.},
  year      = {1990},
  title     = {Serial correlation in unequally spaced longitudinal data},
  abstract  = {Serial correlation in the within subject error structure
          in longitudinal data with unequally spaced observations is
          modelled using continuous time autoregressive moving
          averages. The models considered have both fixed and random
          effects in addition to serially correlated within subject
          errors. Two approaches are presented for calculating the
          exact likelihood for a model when the errors are Gaussian.
          The first calculates the covariance matrices for each
          subject for assumed values of the unknown parameters and
          estimates the fixed parameters by weighted least squares.
          The second uses a state space model and the Kalman filter
          to calculate the exact likelihood. Both methods involve the
          use of complex arithmetic. Nonlinear optimization is used
          to obtain maximum likelihood estimates of the parameters.
          10.1093/biomet/77.4.721},
  doi       = {10.1093/biomet/77.4.721},
  journal   = {Biometrika},
  number    = {4},
  pages     = {721--731},
  volume    = {77}
}

@Book{jones:1993,
  author    = {Jones, Richard H.},
  year      = {1993},
  title     = {Longitudinal Data with Serial Correlation: A State-Space
          Approach (Monographs on Statistics and Applied
          Probability)},
  howpublished  = {Hardcover},
  isbn      = {0412406500},
  publisher = {Chapman \& Hall/CRC}
}

@TechReport{jones:koolman:rice:2005,
  author    = {Jones, Andrew M. and Koolman, Xander and Rice, Nigel},
  year      = {2005},
  title     = {Health-related non-response in BHPS and ECHP: using
          inverse probability weighted estimators in non-linear
          models},
  abstract  = {This paper considers health-related non-response in the
          first eleven waves of the British Household Panel Survey
          (BHPS) and the full eight waves of the European Community
          Household Panel (ECHP) and explores its consequences for
          dynamic models of the association between socioeconomic
          status and self-assessed health (SAH). To address this
          issue we describe the pattern of health-related
          non-response revealed by the BHPS and ECHP data. We both
          test and correct for non-response in empirical models of
          the impact of socioeconomic status on self-assessed health.
          Descriptive evidence shows that there is health-related
          non-response in the data, with those in very poor initial
          health more likely to drop out, and variable addition tests
          provide evidence of non-response bias in the panel data
          models of SAH. Nevertheless a comparison of estimates -
          based on the balanced sample, the unbalanced sample and
          corrected for non-response using inverse probability
          weights (IPW) – shows that, on the whole, there are not
          substantive differences in the average partial effects
          (APE) of the variables of interest. The main differences
          are between unweighted and IPW-2 weighted estimates for the
          APE of income and education in those countries that have
          fewer than eight waves of data. Similar findings have been
          reported concerning the limited influence of non-response
          bias in models of various labour market outcomes; we
          discuss possible explanations for our results.},
  howpublished  = {working paper},
  institution   = {ECuity III},
  number    = {18}
}

@InCollection{jores:1973,
  author    = {J{\"o}reskog, Karl G.},
  editor    = {Goldberger, A. S. and Duncan, O. D.},
  year      = {1973},
  title     = {A general method for estimating a linear structural
          equation system},
  address   = {New York},
  booktitle = {Structural Equation Models in the Social Sciences},
  pages     = {85--112},
  publisher = {Seminar Press}
}

@Article{jores:1978,
  author    = {Karl G. J{\"o}reskog},
  year      = {1978},
  title     = {Structural analysis of covariance and correlation
          matrices},
  journal   = {Psychometrika},
  volume    = {43},
  pages     = {443--477}
}

@Article{joreskog:goldberger:1972,
  author    = {J\"{o}reskog, Karl and Goldberger, Arthur},
  year      = {1972},
  title     = {Factor analysis by generalized least squares},
  abstract  = {Abstract\&nbsp;\&nbsp;Aitken's generalized least squares
          (GLS) principle, with the inverse of the observed
          variance-covariance matrix as a weight matrix, is applied
          to estimate the factor analysis model in the exploratory
          (unrestricted) case. It is shown that the GLS estimates are
          seale free and asymptotically efficient. The estimates are
          computed by a rapidly converging Newton-Raphson procedure.
          A new technique is used to deal with Heywood cases
          effectively.},
  doi       = {10.1007/BF02306782},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {243--260},
  volume    = {37}
}

@Article{joreskog:1967,
  author    = {J{\"o}reskog, Karl G. },
  year      = {1967},
  title     = {Some contributions to maximum likelihood factor analysis},
  doi       = {http://dx.doi.org/10.1007/BF02289658},
  journal   = {Psychometrika},
  pages     = {443--482},
  volume    = {32},
  abstract  = {A new computational method for the maximum likelihood
          solution in factor analysis is presented. This method takes
          into account the fact that the likelihood function may not
          have a maximum in a point of the parameter space where all
          unique variances are positive. Instead, the maximum may be
          attained on the boundary of the parameter space where one
          or more of the unique variances are zero. It is
          demonstrated that suchimproper (Heywood) solutions occur
          more often than is usually expected. A general procedure to
          deal with such improper solutions is proposed. The proposed
          methods are illustrated using two small sets of empirical
          data, and results obtained from the analyses of many other
          sets of data are reported. These analyses verify that the
          new computational method converges rapidly and that the
          maximum likelihood solution can be determined very
          accurately. A by-product obtained by the method is a large
          sample estimate of the variance-covariance matrix of the
          estimated unique variances. This can be used to set up
          approximate confidence intervals for communalities and
          unique variances.}
}

@Article{joreskog:1969,
  author    = {J\"{o}reskog, Karl G. },
  year      = {1969},
  title     = {A general approach to confirmatory maximum likelihood
          factor analysis},
  doi       = {10.1007/BF02289343},
  journal   = {Psychometrika},
  number    = {2},
  pages     = {183--202},
  comment   = {http://dx.doi.org/10.1007/BF02289343},
  volume    = {34},
  abstract  = {We describe a general procedure by which any number of
          parameters of the factor analytic model can be held fixed
          at any values and the remaining free parameters estimated
          by the maximum likelihood method. The generality of the
          approach makes it possible to deal with all kinds of
          solutions: orthogonal, oblique and various mixtures of
          these. By choosing the fixed parameters appropriately,
          factors can be defined to have desired properties and make
          subsequent rotation unnecessary. The goodness of fit of the
          maximum likelihood solution under the hypothesis
          represented by the fixed parameters is tested by a large
          samplex 2 test based on the likelihood ratio technique. A
          by-product of the procedure is an estimate of the
          variance-covariance matrix of the estimated parameters.
          From this, approximate confidence intervals for the
          parameters can be obtained. Several examples illustrating
          the usefulness of the procedure are given.}
}

@InCollection{joreskog:1973,
  author    = {J{\"o}reskog, Karl G.},
  editor    = {A. S. Goldberger and O. D. Duncan},
  year      = {1973},
  title     = {A general method for estimating a linear structural
          equation system},
  booktitle = {Structural Equation Models in the Social Sciences},
  pages     = {85--112},
  publisher = {Academic Press},
  address   = {New York}
}

@Article{joreskog:goldberger:1972,
  author    = {J\"{o}reskog, Karl and Goldberger, Arthur },
  year      = {1972},
  title     = {Factor analysis by generalized least squares},
  doi       = {http://dx.doi.org/10.1007/BF02306782},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {243--260},
  volume    = {37},
  abstract  = {Aitken's generalized least squares (GLS) principle, with
          the inverse of the observed variance-covariance matrix as a
          weight matrix, is applied to estimate the factor analysis
          model in the exploratory (unrestricted) case. It is shown
          that the GLS estimates are scale free and asymptotically
          efficient. The estimates are computed by a rapidly
          converging Newton-Raphson procedure. A new technique is
          used to deal with Heywood cases effectively.}
}

@Manual{joreskog:lisrel:2004,
  author    = "Karl {J\"o}reskog",
  year      = 2004,
  title     = "Structural Equation Modeling With Ordinal Variables using
          {LISREL}",
  note      = "Notes on {LISREL} 8.52.
          http://www.ssicentral.com/lisrel/ordinal.pdf"
}

@Misc{joreskog:samsi:2004,
  author    = "Karl {J\"o}reskog",
  year      = 2004,
  title     = "Structural Equation Modeling With Ordinal Variables",
  misc      = {Presentation at SAMSI Workshop on Latent Variables in
          Social Sciences. URL:
          http://www.samsi.info/200405/socsci/samsi-kjoreskog.pdf}
}

@Manual{joreskog:sorbom:1986:lisrel,
  author    = {Karl J{\"o}reskog and D. S{\"o}rbom},
  year      = {1986},
  title     = {LISREL VI: Analysis of Linear Structural Relationships by
          Maximum Likelihood and Least Squares Methods},
  organization  = {Scientific Software, Inc},
  address   = {Mooresville, IN}
}

@Book{judd:1998,
  author    = "Kenneth L. Judd",
  year      = 1998,
  title     = "Numerical Methods in Economics",
  publisher = "MIT Press",
  address   = "Cambridge, MA"
}

@Book{judge:bock:1978,
  author    = {G. G. Judge and M. E. Bock},
  year      = {1978},
  title     = {The Statistical Implicatinos of Pre-Test and Stein-Rule
          Estimators in Econometrics},
  publisher = {North-Holland},
  address   = {Amsterdam}
}

@Article{judge:mittel:2006,
  author    = {Judge, George G. and Mittelhammer, Ron C. },
  year      = {2007},
  title     = {Estimation and inference in the case of competing sets of
          estimating equations},
  abstract  = {When there is uncertainty concerning the appropriate
          statistical model and corresponding estimators and
          inference methods, we use the Cressie-Read measure of
          divergence to define a semiparametric estimator, , that
          combines plausible estimation problems. This estimation
          procedure identifies, conditional on the data, an optimal
          combination of competing estimators for the unknown
          parameters associated with the alternative plausible
          structural model specifications. The optimization is
          handled internally and avoids the tuning parameters usually
          necessary in problems of this type. To illustrate finite
          sample performance, an extensive sampling experiment is
          conducted to demonstrate the adaptive nature of the
          estimator for an array of data sampling specifications.},
  booktitle = {'Information and Entropy Econometrics' - A Volume in Honor
          of Arnold Zellner},
  doi       = {10.1016/j.jeconom.2006.05.007},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {513--531},
  volume    = {138}
}

@Article{judkins:1990,
  author    = {Judkins, David R. },
  year      = {1990},
  title     = {Fay's Method for Variance Estimation},
  journal   = {Journal of Official Statistics},
  number    = {3},
  pages     = {223--239},
  comment   = {http://www.jos.nu/Articles/abstract.asp?article=63223},
  volume    = {6},
  abstract  = {The standard balanced repeated replication (BRR) method of
          estimating variances involves dividing the sample in each
          stratum into half-samples, selecting a balanced set of half
          samples across all strata, re-computing the statistic of
          interest (generally nonlinear) on each selected
          half-sample, and taking the mean square difference of among
          the replicate estimates as the variance estimate. One
          problem that occasionally arises is that one or more
          replicate estimates will be undefined due to division by
          zero. This is particularly common when ratio estimation has
          been used with very small cell sizes. Robert Fay suggested
          a solution to this problem several years ago: Instead of
          increasing the weights of one half sample by $100\%$ and
          decreasing the weights of the other half sample to zero, he
          recommended perturbing the weights by $\pm x\%$. In this
          article, his suggestion is evaluated with simulation
          techniques. It is shown to be useful when variance
          estimates are needed for both smooth and nonsmooth
          statistics or when there are very few degrees of freedom
          available for variance estimation. The paper also discusses
          further modifications to the technique that are useful for
          variance estimation when only one PSU is selected per
          stratum.}
}

@ARTICLE{kakwani:wagstaff:vandoor:1997,
  AUTHOR =       {Nanak Kakwani and Adam Wagstaff and Eddy {van Doorslaer}},
  TITLE =        {Socioeconomic inequalities in health:
                  Measurement, computation and statistical inference},
  JOURNAL =      {Journal of Econometrics},
  YEAR =         {1997},
  volume =       {77},
  pages =        {87--103},
}

@book{kalb:pren:2002,
    author = {Kalbfleisch, John D. and Prentice, Ross L.},
    edition = {2},
    isbn = {047136357X},
    publisher = {Wiley-Interscience},
    title = {The Statistical Analysis of Failure Time Data (Wiley Series in Probability and Statistics)},
    year = {2002}
}

@article{kalton:anderson:1986,
  title={Sampling rare populations},
  author={Kalton, G. and Anderson, D.W.},
  journal={Journal of the Royal Statistical Society. Series A (General)},
  pages={65--82},
  volume = {149},
  number = {1},
  year={1986},
}

@Article{kalton:kasp:1986,
  author    = {Kalton, G. and Kasprzyk, D.},
  year      = {1986},
  title     = {The Treatment of Missing Survey Data},
  journal   = {Survey Methodology},
  volume    = {12},
  number    = {1},
  pages     = {1--16}
}

@article{kalton:flores:cervantes:2003,
    author = {Kalton, Graham and Flores-Cervantes, Ismael},
    journal = {Journal of Official Statistics},
    number = {2},
    pages = {81--97},
    title = {Weighting Methods},
    volume = {19},
    year = {2003}
}

@ARTICLE{kalton:2009,
  author =       {Graham Kalton},
  title =        {Methods for oversampling rare subpopulations in social surveys},
  journal =      {Survey Methodology},
  year =         {2009},
  volume =       {35},
  number =       {2},
  pages =        {125--141},
  note =         {Available at https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2009002/article/11036-eng.pdf?st=HXLvKAiS},
}


@Article{kanbur:zhang:1999,
  author    = "R. Kanbur and X. Zhang",
  year      = 1999,
  title     = "Which Regional Inequality? {The} Evolution of Rural?Urban
          and Inland?Coastal Inequality in {China} from 1983 to 1995",
  journal   = "Journal of Comparative Economics",
  volume    = 27,
  number    = 4,
  pages     = "686--701"
}

@ARTICLE{kano:berkane:bentler:1990,
  AUTHOR =       {Y. Kane and M. Berkane and P. M. Bentler},
  TITLE =        {Covariance structure analysis with heterogeneous kurtosis parameters},
  JOURNAL =      {Biometrika},
  YEAR =         {1990},
  volume =       {77},
  pages =        {575--585},
}


@Article{kapl:1988,
  author    = {David Kaplan},
  year      = {1988},
  title     = {The Impact of Specification Error on the Estimation,
          Testing, and Improvement of Structural Equation Models},
  journal   = {Multivariate Behavioral Research},
  volume    = {23},
  pages     = {69--86}
}

@Article{kaplan:1988,
  author    = {Kaplan, David},
  year      = {1988},
  title     = {The Impact of Specification Error on the Estimation,
          Testing, and Improvement of Structural Equation Models},
  abstract  = {The purpose of this paper is to assess the impact of
          misspecification on the estimation, testing, and
          improvement of structural equation models. A population
          study is conducted whereby a prototypical latent variable
          model is misspecified in various ways. Measurement model
          and structural model misspecifications are considered
          separately and together. The maximum likelihood estimator
          (ML) is compared to a limited information two-stage least
          squares (2SLS) estimator implemented in LISREL. The ratio
          of chi-square to its degrees of freedom and power of the
          likelihood ratio test is assessed for each
          misspecification. The modification index provided by LISREL
          is also studied. Results indicate that ML and 2SLS
          estimates of measurement and structural parameters are both
          affected by measurement model misspecification. For
          misspecification of the structural part, ML is shown to
          propagate errors throughout the structural parameters
          whereas 2SLS isolates errors only in the parameters of the
          misspecified equation. Results also show that relying on
          the ratio of chi-square to degrees of freedom as an index
          of fit may lead to accepting models with severe parameter
          bias. Finally, the modification index is shown to be an
          unreliable indicator of the location of a specification
          error.},
  doi       = {10.1207/s15327906mbr2301\_4},
  journal   = {Multivariate Behavioral Research},
  number    = {1},
  pages     = {69--86},
  publisher = {Psychology Press},
  volume    = {23}
}

@Book{kaplan:2000,
  author    = {David Kaplan},
  year      = {2000},
  title     = {Structural Equation Modeling: Foundations and Extensions},
  publisher = {SAGE},
  number    = {10},
  series    = {Advanced Quantitative Techniques in Social Sciences},
  address   = {Thousand Oaks, CA}
}

@Book{kaplan:2004,
  editor    = {David Kaplan},
  year      = {2004},
  title     = {The {SAGE} Handbook of Quantitative Methodology for the
          Social Sciences},
  publisher = {SAGE Publications},
  address   = {Thousand Oaks, CA},
  isbn      = {0761923594}
}

@Book{kaplan:2009,
  author    = {David Kaplan},
  year      = {2009},
  title     = {Structural Equation Modeling: Foundations and Extensions},
  publisher = {SAGE},
  number    = {10},
  edition   = {2nd},
  series    = {Advanced Quantitative Techniques in Social Sciences},
  address   = {Thousand Oaks, CA}
}

@Article{kaplan:elliott:1997,
  author    = {Kaplan, David and Elliott, Pamela R. },
  year      = {1997},
  title     = {A Didactic Example of Multilevel Structural Equation
          Modeling Applicable to the Study of Organizations.},
  abstract  = {A didactic example is presented of the application of new
          developments in structural equation modeling that allow for
          the modeling of multilevel data. The method, a synthesis of
          methods developed by B. Muthen, is applied to the problem
          of validating indicators of science education quality in
          the United States.},
  journal   = {Structural Equation Modeling},
  number    = {1},
  pages     = {1--24},
  comment   = {http://eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true\&_\&ERICExtSearch_SearchValue_0=EJ538594\&ERICExtSearch_SearchType_0=eric_accno\&accno=EJ538594}
          ,
  volume    = {4}
}

@Article{kaplan:ferguson:1999,
  author    = {Kaplan, David and Ferguson, Aaron J. },
  year      = {1999},
  title     = {On the Utilization of Sample Weights in Latent Variable
          Models},
  abstract  = {Examines the use of sample weights in latent variable
          models in the case where a simple random sample is drawn
          from a population containing a mixture of strata through a
          bootstrap simulation study. Results show that ignoring
          weights can lead to serious bias in latent variable model
          parameters and reveal the advantages of using sample
          weights.},
  journal   = {Structural Equation Modeling},
  number    = {4},
  pages     = {305--321},
  volume    = {6}
}

@Book{kasp:dunc:kalt:sing:pane:1989,
  editor    = "Kasprzyk, Daniel and Duncan, Greg and Kalton, Graham and
          Singh, M. P.",
  year      = 1989,
  title     = "Panel Surveys",
  publisher = "John Wiley \& Sons",
  address   = "New York"
}

@Article{kauer:carroll:2001,
  author    = {G{\"o}ran Kauermann and Raymond J. Carroll},
  year      = {2001},
  title     = {A Note on the Efficiency of Sandwich Covariance Matrix
          Estimation},
  journal   = {Journal of the American Statistical Association},
  volume    = {96},
  number    = {456},
  pages     = {1387--1396},
  abstract  = { The sandwich estimator, also known as robust covariance
          matrix estimator, heteroscedasticity-consistent covariance
          matrix estimate, or empirical covariance matrix estimator,
          has achieved increasing use in the econometric literature
          as well as with the growing popularity of generalized
          estimating equations. Its virtue is that it provides
          consistent estimates of the covariance matrix for parameter
          estimates even when the . tted parametric model fails to
          hold or is not even speci. ed. Surprisingly though, there
          has been little discussion of properties of the sandwich
          method other than consistency. We investigate the sandwich
          estimator in quasi-likelihood models asymptotically, and in
          the linear case analytically. We show that under certain
          circumstances when the quasi-likelihood model is correct,
          the sandwich estimate is often far more variable than the
          usual parametric variance estimate. The increased variance
          is a . xed feature of the method and the price that one
          pays to obtain consistency even when the parametric model
          fails or when there is heteroscedasticity. We show that the
          additional variability directly affects the coverage
          probability of con. dence intervals constructed from
          sandwich variance estimates. In fact, the use of sandwich
          variance estimates combined with t-distribution quantiles
          gives con. dence intervals with coverage probability
          falling below the nominal value. We propose an adjustment
          to compensate for this fact }
}

@Article{kauermann2001note,
  author    = {Kauermann, G\"{o}ran and Carroll, Raymond J.},
  year      = {2001},
  title     = {A Note on the Efficiency of Sandwich Covariance Matrix
          Estimation},
  abstract  = {The sandwich estimator, also known as robust covariance
          matrix estimator, heteroscedasticity-consistent covariance
          matrix estimate, or empirical covariance matrix estimator,
          has achieved increasing use in the econometric literature
          as well as with the growing popularity of generalized
          estimating equations. Its virtue is that it provides
          consistent estimates of the covariance matrix for parameter
          estimates even when the fitted parametric model fails to
          hold or is not even specified. Surprisingly though, there
          has been little discussion of properties of the sandwich
          method other than consistency. We investigate the sandwich
          estimator in quasi-likelihood models asymptotically, and in
          the linear case analytically. We show that under certain
          circumstances when the quasi-likelihood model is correct,
          the sandwich estimate is often far more variable than the
          usual parametric variance estimate. The increased variance
          is a fixed feature of the method and the price that one
          pays to obtain consistency even when the parametric model
          fails or when there is heteroscedasticity. We show that the
          additional variability directly affects the coverage
          probability of confidence intervals constructed from
          sandwich variance estimates. In fact, the use of sandwich
          variance estimates combined with t-distribution quantiles
          gives confidence intervals with coverage probability
          falling below the nominal value. We propose an adjustment
          to compensate for this fact.},
  journal   = {Journal of the American Statistical Association},
  number    = {456},
  pages     = {1387--1396},
  volume    = {96}
}

@Book{kell:1928,
  author    = {T.L. Kelley},
  year      = {1928},
  title     = {Crossroads in the mind of man: A study of differentiable
          mental abilities},
  journal   = {Stanford University},
  address   = {Stanford, California}
}

@Book{kendall:1955,
  author    = "Maurice G. Kendall",
  year      = "1955",
  title     = "Rank Correlation Methods",
  edition   = "2nd",
  publisher = "Charles Griffin \& Co.",
  address   = "London"
}

@Book{kendall:stuart:ord:1991,
  author    = "Maurice Kendall and Alan Stuart and J. Keith Ord",
  year      = 1991,
  title     = "Advanced Theory of Statistics: Classical Inference and
          Relationship",
  edition   = "6th",
  volume    = 2,
  publisher = "Oxford University Press",
  address   = "Oxford, UK"
}

@Book{kendall:stuart:ord:arnold:1999:2a,
  author    = {Kendall, Maurice and Stuart, Alan and Ord, Keith J. and
          Arnold, Steven },
  year      = {1999},
  title     = {Advanced Theory of Statistics: Classical Inference and and
          the Linear Model},
  edition   = {6},
  isbn      = {0340662301},
  publisher = {A Hodder Arnold Publication},
  series    = {Kendall's Library of Statistics},
  volume    = {2A}
}

@ARTICLE{kelker:1970,
  AUTHOR =       {D. Kelker},
  TITLE =        {Distribution theory of spherical distributions
                  and a location-scale parameter generalizations},
  JOURNAL =      {Sankhya A},
  YEAR =         {1970},
  volume =       {32},
  pages =        {419--430},
}


@Article{kenny:judd:1984,
  author    = {David A. Kenny and Charles M. Judd},
  year      = {1984},
  title     = {Estimating the Nonlinear and Interactive Effects of Latent
          Variables},
  journal   = {Psychological Bulletin},
  volume    = {96},
  number    = {1},
  pages     = {201--210}
}

@Article{kent:1982,
  author    = {Kent, John T.},
  year      = {1982},
  title     = {Robust Properties of Likelihood Ratio Test},
  abstract  = {The usual asymptotic chi-squared distribution for the
          likelihood ratio test statistic is based on the assumptions
          that the data come from the parametric model under
          consideration and that the parameter satisfies the null
          hypothesis. In this paper we examine the distribution of
          the likelihood ratio statistic when the data do not come
          from the parametric model, but when the `nearest' member of
          the parametric family still satisfies the null hypothesis.
          In general, the likelihood ratio statistic no longer
          alternative statistic based on the union-intersection
          approach is proposed.},
  doi       = {10.2307/2335849},
  journal   = {Biometrika},
  number    = {1},
  pages     = {19--27},
  publisher = {Biometrika Trust},
  volume    = {69}
}

@Article{kenward:molenberghs:1999,
  author    = {Kenward, M. G. and Molenberghs, G.},
  year      = {1999},
  title     = {Parametric models for incomplete continuous and
          categorical longitudinal data},
  abstract  = {This paper reviews models for incomplete continuous and
          categorical longitudinal data. In terms of Rubin's
          classification of missing value processes we are
          specifically concerned with the problem of nonrandom
          missingness. A distinction is drawn between the classes of
          selection and pattern-mixture models and, using several
          examples, these approaches are compared and contrasted. The
          central roles of identifiability and sensitivity are
          emphasized throughout. 10.1177/096228029900800105},
  doi       = {10.1177/096228029900800105},
  journal   = {Stat Methods Med Res},
  number    = {1},
  pages     = {51--83},
  volume    = {8}
}

@BOOK{khuri:2003,
  AUTHOR =       {Andr{\'e} I. Khuri},
  TITLE =        {Advanced Calculus with Applications in Statistics},
  PUBLISHER =    {Wiley Interscience},
  YEAR =         {2003},
  address =      {New York},
}


@Article{kim:bentler:2002,
  author    = {Kim, Kevin and Bentler, Peter },
  year      = {2002},
  title     = {Tests of homogeneity of means and covariance matrices for
          multivariate incomplete data},
  doi       = {10.1007/BF02295134},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {609--623},
  volume    = {67},
  abstract  = {Existing test statistics for assessing whether incomplete
          data represent a missing completely at random sample from a
          single population are based on a normal likelihood
          rationale and effectively test for homogeneity of means and
          covariances across missing data patterns. The likelihood
          approach cannot be implemented adequately if a pattern of
          missing data contains very few subjects. A generalized
          least squares rationale is used to develop parallel tests
          that are expected to be more stable in small samples. Three
          factors were varied for a simulation: number of variables,
          percent missing completely at random, and sample size. One
          thousand data sets were simulated for each condition. The
          generalized least squares test of homogeneity of means
          performed close to an ideal Type I error rate for most of
          the conditions. The generalized least squares test of
          homogeneity of covariance matrices and a combined test
          performed quite well also.}
}

@Article{kim:brick:fuller:kalton:2006,
  author    = {Kim, Jae K. and Brick, Michael J. and Fuller, Wayne A. and
          Kalton, Graham },
  year      = {2006},
  title     = {On the bias of the multiple-imputation variance estimator
          in survey sampling},
  comment   = {10.1111/j.1467-9868.2006.00546.x},
  journal   = {Journal of the Royal Statistical Society: Series B
          (Statistical Methodology)},
  number    = {3},
  pages     = {509--521},
  volume    = {68},
  abstract  = {Multiple imputation is a method of estimating the
          variances of estimators that are constructed with some
          imputed data. We give an expression for the bias of the
          multiple-imputation variance estimator for data that are
          collected with a complex sample design. The bias may be
          sizable for certain estimators, such as domain means, when
          a large fraction of the values are imputed. A bias-adjusted
          variance estimator is suggested.}
}

@article{kim:wu:2013,
    abstract =
        {It is routine practice for survey organizations to provide
        replication weights as part of survey data files. These replication
        weights are meant to produce valid and efficient variance estimates
        for a variety of estimators in a simple and systematic manner. Most
        existing methods for constructing replication weights, however, are
        only valid for specific sampling designs and typically require a very
        large number of replicates. In this paper we first show how to
        produce replication weights based on the method outlined in Fay
        (1984) such that the resulting replication variance estimator is
        algebraically equivalent to the fully efficient linearization
        variance estimator for any given sampling design. We then propose a
        novel weight-calibration method to simultaneously achieve efficiency
        and sparsity in the sense that a small number of sets of replication
        weights can produce valid and efficient replication variance
        estimators for key population parameters. Our proposed method can be
        used in conjunction with existing resampling techniques for
        large-scale complex surveys. Validity of the proposed methods and
        extensions to some balanced sampling designs are also discussed.
        Simulation results showed that our proposed variance estimators
        perform very well in tracking coverage probabilities of confidence
        intervals. Our proposed strategies will likely have impact on how
        public-use survey data files are produced and how these data sets are
        analyzed.},
    author = {Kim, Jae K. and Wu, Changbao},
    journal = {Survey Methodology},
    number = {1},
    pages = {91--120},
    title = {Sparse and efficient replication variance estimation for complex surveys},
    volume = {39},
    year = {2013}
}

@book{kim:shao:2013,
    author = {Kim, Jae K. and Shao, Jun},
    publisher = {Chapman \& Hall/CRC Press},
    title = {Statistical Methods for Handling Incomplete Data},
    series = {Texts in Statistical Science},
    year = {2013}
}
@Article{kinal:1980,
  author    = {Kinal, Terrence W. },
  year      = {1980},
  title     = {The Existence of Moments of $k$-Class Estimators},
  doi       = {10.2307/1912027},
  journal   = {Econometrica},
  number    = {1},
  pages     = {241--249},
  volume    = {48}
}

@TECHREPORT{IPUMS:CPS:2010,
  AUTHOR =       {Miriam King and  Steven Ruggles and  J. Trent Alexander and  Sarah Flood and  Katie Genadek and  Matthew B. Schroeder and  Brandon Trampe and  Rebecca Vick},
  TITLE =        {Integrated Public Use Microdata Series, Current Population Survey: Version 3.0},
  INSTITUTION =  {University of Minnesota},
  YEAR =         {2010},
  type =         {Machine-readable database},
  address =      {Minneapolis},
  comment =      {http://cps.ipums.org/cps/citation.shtml},
}

@Article{kirby:bollen:2009,
  author    = {Kirby, James B. and Bollen, Kenneth A.},
  year      = {2009},
  title     = {Using Instrumental Variable Tests to Evaluate Model
          Specification in Latent Variable Structural Equation
          Models},
  doi       = {10.1111/j.1467-9531.2009.01217.x},
  journal   = {Sociological Methodology},
  number    = {1},
  pages     = {327--355},
  volume    = {39},
  abstract  = {Structural equation modeling (SEM) with latent variables
          is a powerful tool for social and behavioral scientists,
          combining many of the strengths of psychometrics and
          econometrics into a single framework. The most common
          estimator for SEM is the full-information maximum
          likelihood (ML) estimator, but there is continuing interest
          in limited information estimators because of their
          distributional robustness and their greater resistance to
          structural specification errors. However, the literature
          discussing model fit for limited information estimators for
          latent variable models is sparse compared with that for
          full-information estimators. We address this shortcoming by
          providing several specification tests based on the 2SLS
          estimator for latent variable structural equation models
          developed by Bollen (1996). We explain how these tests can
          be used not only to identify a misspecified model but to
          help diagnose the source of misspecification within a
          model. We present and discuss results from a Monte Carlo
          experiment designed to evaluate the finite sample
          properties of these tests. Our findings suggest that the
          2SLS tests successfully identify most misspecified models,
          even those with modest misspecification, and that they
          provide researchers with information that can help diagnose
          the source of misspecification.}
}

@Article{kirk:gela:vecc:1983,
  author    = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  year      = {1983},
  title     = {Optimization by Simulated Annealing},
  doi       = {10.2307/1690046},
  journal   = {Science},
  number    = {4598},
  pages     = {671--680},
  volume    = {220},
  abstract  = {There is a deep and useful connection between statistical
          mechanics (the behavior of systems with many degrees of
          freedom in thermal equilibrium at a finite temperature) and
          multivariate or combinatorial optimization (finding the
          minimum of a given function depending on many parameters).
          A detailed analogy with annealing in solids provides a
          framework for optimization of the properties of very large
          and complex systems. This connection to statistical
          mechanics exposes new information and provides an
          unfamiliar perspective on traditional optimization problems
          and methods.}
}

@Article{kirkpatrick1983optimization,
  author    = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  year      = {1983},
  title     = {Optimization by Simulated Annealing},
  abstract  = {There is a deep and useful connection between statistical
          mechanics (the behavior of systems with many degrees of
          freedom in thermal equilibrium at a finite temperature) and
          multivariate or combinatorial optimization (finding the
          minimum of a given function depending on many parameters).
          A detailed analogy with annealing in solids provides a
          framework for optimization of the properties of very large
          and complex systems. This connection to statistical
          mechanics exposes new information and provides an
          unfamiliar perspective on traditional optimization problems
          and methods.},
  doi       = {10.2307/1690046},
  issn      = {00368075},
  journal   = {Science},
  number    = {4598},
  pages     = {671--680},
  publisher = {American Association for the Advancement of Science},
  series    = {New Series},
  volume    = {220}
}

@Article{kish1970balanced,
  author    = {Kish, Leslie and Frankel, Martin R.},
  year      = {1970},
  title     = {Balanced Repeated Replications for Standard Errors},
  journal   = {Journal of the American Statistical Association},
  number    = {331},
  pages     = {1071--1094},
  volume    = {65}
}

@Book{kish:1965,
  author    = "Leslie Kish",
  year      = "1965",
  title     = "Survey Sampling",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Book{kish:1995,
  author    = "Leslie Kish",
  year      = "1995",
  title     = "Survey Sampling",
  publisher = "John Wiley and Sons",
  address   = "New York",
  edition   = "3rd"
}

@Article{kish:frankel:1970,
  author    = {Kish, Leslie and Frankel, Martin R. },
  year      = {1970},
  title     = {Balanced Repeated Replications for Standard Errors},
  journal   = {Journal of the American Statistical Association},
  number    = {331},
  pages     = {1071--1094},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28197009%2965%3A331%3C1071%3ABRRFSE%3E2.0.CO%3B2-V}
          ,
  volume    = {65}
}

@Article{kish:frankel:1974,
  author    = {L. Kish and M. R. Frankel},
  year      = {1974},
  title     = {Inference from complex samples},
  journal   = {Journal of the Royal Statistical Society, Series B},
  volume    = {36},
  pages     = {1--37}
}

@TechReport{kitamura:2006,
  author    = {Kitamura, Yuichi },
  year      = {2006},
  title     = {Empirical Likelihood Methods in Econometrics: Theory and
          Practice},
  number    = {1569},
  series    = {Cowles Foundation Discussion Paper},
  comment   = {http://ideas.repec.org/p/cwl/cwldpp/1569.html}
}

@InCollection{kitamura:2007,
  author    = {Kitamura, Yuichi},
  editor    = {Richard Blundell and Whitney Newey and Torsten Persson},
  year      = {2007},
  title     = {Empirical Likelihood Methods in Econometrics: Theory and
          Practice},
  booktitle = {Advances in Economics and Econometrics, Vol. III},
  publisher = {Cambridge University Press},
  address   = {New York}
}

@Article{kitamura:stutzer:1997,
  author    = {Kitamura, Y., and M. Stutzer},
  year      = {1997},
  title     = {An Information Theoretic Alternative to Generalized Method
          of Moments Estimation},
  journal   = {Econometrica},
  volume    = {65},
  number    = {4},
  pages     = {861--874}
}

@Article{kitamura:tripathi:ahn:2004,
  author    = {Kitamura, Yuichi and Tripathi, Gautam and Ahn, Hyungtaik},
  year      = {2004},
  title     = {Empirical Likelihood-Based Inference in Conditional Moment
          Restriction Models},
  doi       = {10.2307/3598764},
  issn      = {00129682},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1667--1714},
  publisher = {The Econometric Society},
  volume    = {72},
  abstract  = {This paper proposes an asymptotically efficient method for
          estimating models with conditional moment restrictions. Our
          estimator generalizes the maximum empirical likelihood
          estimator (MELE) of Qin and Lawless (1994). Using a kernel
          smoothing method, we efficiently incorporate the
          information implied by the conditional moment restrictions
          into our empirical likelihood-based procedure. This yields
          a one-step estimator which avoids estimating optimal
          instruments. Our likelihood ratio-type statistic for
          parametric restrictions does not require the estimation of
          variance, and achieves asymptotic pivotalness implicitly.
          The estimation and testing procedures we propose are
          normalization invariant. Simulation results suggest that
          our new estimator works remarkably well in finite
          samples.}
}

@Book{kline:2005:2ed,
  author    = {Kline, Rex B. },
  year      = {2005},
  title     = {Principles and Practice of Structural Equation Modeling},
  edition   = {2nd},
  publisher = {Guilford Press}
}

@Article{kloek:dijk:1978,
  author    = "Teun Kloek and Herman K. van Dijk",
  year      = 1978,
  title     = "Efficient estimation of income distribution parameters",
  journal   = "Journal of Econometrics",
  volume    = 8,
  pages     = "61--74"
}

@article{kocherg:he:mu:2005,
    author = {Kocherginsky, Masha and He, Xuming and Mu, Yunming},
    doi = {10.1198/106186005X27563},
    journal = {Journal of Computational and Graphical Statistics},
    number = {1},
    pages = {41--55},
    title = {Practical Confidence Intervals for Regression Quantiles},
    volume = {14},
    year = {2005},
    abstract =
       {Routine applications of quantile regression analysis require
       reliable and practical algorithms for estimating standard
       errors, variance-covariance matrices, as well as confidence
       intervals. Because the asymptotic variance of a quantile
       estimator depends on error densities, some standard
       large-sample approximations have been found to be highly
       sensitive to minor deviations from the iid error assumption.
       In this article we propose a time-saving resampling method
       based on a simple but useful modification of the Markov chain
       marginal bootstrap (MCMB) to construct confidence intervals
       in quantile regression. This method is compared to several
       existing methods with favorable performance in speed,
       accuracy, and reliability. We also make practical
       recommendations based on the quantreg package contributed by
       Roger Koenker and a new package rqmcmb2 developed by the
       first two authors. These recommendations also apply to users
       of the new SAS procedure PROC QUANTREG, available from
       Version 9.2 of SAS.},
}

@Article{kodde:palm:1986,
  author    = {Kodde, David A. and Palm, Franz C. },
  year      = {1986},
  title     = {Wald Criteria for Jointly Testing Equality and Inequality
          Restrictions},
  journal   = {Econometrica},
  number    = {5},
  pages     = {1243--1248},
  volume    = {54}
}

@Book{koen:2005,
  author    = "Roger Koenker",
  year      = 2005,
  title     = "Quantile Regression",
  publisher = "Cambridge University Press",
  address   = "Cambdige, UK"
}

@Article{koen:basset:1978a,
  author    = "Roger Koenker and G. Basset",
  year      = 1978,
  title     = "Regression Quantiles",
  journal   = "Econometrica",
  volume    = 46,
  number    = 1,
  pages     = "33--50"
}

@Article{koen:hall:2001,
  author    = "R. Koenker and K. Hallock",
  year      = 2001,
  title     = "Quantile Regression: An Introduction",
  journal   = "Journal of Economic Perspectives",
  volume    = 15,
  number    = 4,
  pages     = "43--56"
}

@Article{koenker:basset:1982,
  author    = {Koenker, Roger and Bassett, Gilbert},
  year      = {1982},
  title     = {Tests of Linear Hypotheses and L\_1 Estimation},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1577--1584},
  volume    = {50}
}

@book{kohler:kreuter:2008,
    author = {Kohler, Ulrich and Kreuter, Frauke},
    edition = {2},
    publisher = {Stata Press},
    title = {Data Analysis Using Stata, Second Edition},
    year = {2008}
}

@Article{kohn:ansley:1986,
  author    = "Robert Kohn and Craig F. Ansley",
  year      = 1986,
  title     = "Estimation, Prediction, and Interpolation for {ARIMA}
          Models with Missing Data",
  journal   = "Journal of the American Statistical Association",
  volume    = 81,
  number    = 395,
  pages     = "751--761"
}

@Article{kole:2001review,
  author    = "S. Kolenikov",
  year      = 2001,
  title     = "Review of {Stata} 7",
  journal   = "Journal of Applied Econometrics",
  volume    = 16,
  pages     = "637--646"
}

@Article{kole:shor:2005,
  author    = "Stanislav Kolenikov and Anthony F. Shorrocks",
  year      = 2005,
  title     = "A Decomposition Analysis of Regional Poverty in {Russia}",
  journal   = "Review of Development Economics",
  volume    = 9,
  number    = 1,
  pages     = "25--46"
}

@TechReport{kole:shora:wider:2003,
  author    = "Stanislav Kolenikov and Anthony F. Shorrocks",
  year      = 2003,
  title     = "A Decomposition Analysis of Regional Poverty in {Russia}",
  institution   = "World Institute for Development Economics Research",
  type      = "Discussion paper",
  number    = "2003/74",
  address   = "Helsinki"
}

@Article{kolenikov:shorrocks:2005,
  author    = {Kolenikov, Stanislav and Shorrocks, Anthony},
  year      = {2005},
  title     = {A Decomposition Analysis of Regional Poverty in Russia},
  comment   = {We use the recently proposed Shapley decomposition to
          discover the contributions of such factors as growth (mean
          income changes), inequality, and changes in poverty
          standards, to poverty changes in Russian regions.},
  doi       = {10.1111/j.1467-9361.2005.00262.x},
  issn      = {1363-6669},
  journal   = {Review of Development Economics},
  number    = {1},
  pages     = {25+},
  publisher = {Blackwell Publishing},
  volume    = {9}
}

@PhDThesis{kolenikov:2005:disser,
  author    = {Stanislav Kolenikov},
  year      = {2005},
  title     = {A modification of the {EM} algorithm with applications to
          spatio-temporal modeling},
  school    = {University of North Carolina, Chapel Hill},
  abstract  = {This dissertation outlines the use of the maximum
          likelihood procedures for estimation of the parameters of
          spatial and spatio-temporal processes when some
          observations are missing, and reviews the realizations of
          ML and EM procedures in the presence of missing data when
          the data are correlated. A version of the EM algorithm is
          suggested that has a promise of being computationally more
          efficient due to reduction of the number of matrix
          inversions, although at a price of the loss of the
          estimator's consistency and asymptotic efficiency.
          Corrections that restore unbiasedness of the estimating
          equations implied by the EM algorithm are proposed.
          Asymptotic properties (consistency and normality) of the
          resulting estimators are established. Applications of the
          new procedure are considered: an analytically tractable
          case of AR(1) process, and an application to real data on
          PM$_{2.5}$ measurements.}
}

@InProceedings{kolenikov:2007:qmc,
  author    = {Stanislav Kolenikov},
  year      = {2007},
  title     = {Applications of quasi-{M}onte {C}arlo methods in inference
          for complex survey data},
  booktitle = {Proceedings of the Survey Research Methods Section of
          ASA},
  pages     = {2830-2837}
}

@Article{kolenikov:2009,
  author    = "Kolenikov, S.",
  year      = "2009",
  title     = "Confirmatory factor analysis using confa",
  journal   = "Stata Journal",
  publisher = "Stata Press",
  address   = "College Station, TX",
  volume    = "9",
  number    = "3",
  pages     = "329--373",
  comment   = "http://www.stata-journal.com/article.html?article=st0169"
}

@article{kolenikov:2011,
  author    = {Stanislav Kolenikov},
  year      = {2011},
  title     = {Biases of parameter estimates in misspecified structural
          equation models},
  journal   = {Sociological Methodology},
  volume    = {41},
  number    = {1},
  pages     = {119--157}
}


@article{kolenikov:2010,
  author    = {Stanislav Kolenikov},
  year      = {2010},
  title     = {Resampling inference with complex survey data},
  journal   = {The Stata Journal},
  volume    = {10},
  issue     = {2},
  pages     = {165--199}
}

@Article{kolenikov:2009:confa,
  author    = {Stanislav Kolenikov},
  year      = {2009},
  title     = {Confirmatory factor analysis with \texttt{confa}},
  journal   = {The Stata Journal},
  volume    = {9},
  number    = {3},
  pages     = {329--373}
}

@book{kolenikov:steinley:thombs:2010,
  year      = {2010},
  editor    = {Kolenikov, Stanislav and Steinley, Douglas and Thombs, Lori},
  booktitle = {Statistics in the Social Sciences: Current Methodological Developments},
  publisher = {John Wiley \& Sons},
  address   = {Hoboken, NJ}
}


@TechReport{kolenikov:angeles:2004,
  author    = "Stanislav Kolenikov and Gustavo Angeles",
  year      = 2004,
  title     = "The Use of Discrete Data in {PCA}: Theory, Simulations,
          and Applications to Socioeconomic Indices",
  institution   = "MEASURE/Evaluation project, Carolina Population Center,
          University of North Carolina",
  number    = "WP-04-85",
  address   = "Chapel Hill"
}

@article{kolenikov:angeles:2011,
  author    = {Stanislav Kolenikov and Gustavo Angeles},
  year      = {2011},
  title     = {Cost efficiency of repeated cluster surveys},
  journal   = {Survey Methodology},
  volume    = {37},
  issue     = {1},
  pages     = {75--94}
}

@Article{kolenikov:angeles:2009,
  author    = "Stanislav Kolenikov and Gustavo Angeles",
  year      = 2009,
  title     = "Socioeconomic status measurement with discrete proxy
          variables: Is principal component analysis a reliable
          answer?",
  doi       = {10.1111/j.1475-4991.2008.00309.x},
  issn      = {0034-6586},
  journal   = {The Review of Income and Wealth},
  number    = {1},
  pages     = {128--165},
  volume    = {55}
}

@TechReport{kolenikov:angeles:clustered:2005,
  author    = {Stanislav Kolenikov and Gustavo Angeles},
  year      = {2005},
  title     = {On reuse of clusters in repeated studies},
  institution   = {Carolina Population Center, University of North Carolina},
  type      = {Working Paper},
  address   = {Chapel Hill},
  number    = {WP 05-87}
}

@Unpublished{kolenikov:bollen:2008,
  author    = {Stansilav Kolenikov and Kenneth A Bollen},
  year      = {2010},
  title     = {Testing Negative Error Variances: Is a {H}eywood Case a
          Symptom of Misspecification?},
  note      = {R\&R in \textit{Sociological Methods and Research}}
}

@Unpublished{kolenikov:bollen:2011,
  author    = {Stansilav Kolenikov and Kenneth A Bollen},
  year      = {2011},
  title     = {Testing Negative Error Variances: Is a {H}eywood Case a
          Symptom of Misspecification?},
  note      = {Conditionally accepted to \textit{Sociological Methods and Research}}
}

@article{kolenikov:bollen:2012,
  author    = {Stansilav Kolenikov and Kenneth A Bollen},
  year      = {2012},
  title     = {Testing Negative Error Variances: Is a {H}eywood Case a
          Symptom of Misspecification?},
  journal   = {Sociological Methods and Research},
  volume    = {??},
  number    = {??},
  pages     = {??}
}


@Misc{kolenikov:bollen:2010:miiv,
  author    = {Stanislav Kolenikov and Kenneth A Bollen},
  year      = {2010},
  title     = {Unified treatment of the least squares and instrumental
          variable estimation methods for structural equation models
          by generalized method of moments},
  howpublished  = {under review}
}

@InProceedings{kolenikov:bollen:savalei:2006,
  author    = {Stanislav Kolenikov and Kenneth A. Bollen and Victoria
          Savalei},
  year      = {2006},
  title     = {Specification Tests with {Heywood} Cases},
  booktitle = {Proceedings of American Statistical Association},
  address   = {Alexandria, VA},
  howpublished  = {[CD-ROM]}
}

@Unpublished{kolenikov:li:bollen:2008,
  author    = {Stansilav Kolenikov and Ni Li and Kenneth A Bollen},
  year      = {2008},
  title     = {Estimating equations, sandwich variance estimator and
          parameter biases in misspecified structural equation
          models},
  note      = {work in progress}
}

@Unpublished{kolenikov:savalei:2008,
  author    = {Stansilav Kolenikov and Victoria Savalei},
  year      = {2008},
  title     = {How many factors? Underidentification issues in factor
          analysis},
  note      = {work in progress}
}

@Unpublished{kolenikov:yuan:2009,
  author    = {Stanislav Kolenikov and Yiyong Yuan},
  year      = {2009},
  title     = {Empirical Likelihood Estimation and Testing in Covariance
          Structure Models},
  note      = {Under review}
}

@ARTICLE{kolenikov:2012:scrhalton,
  AUTHOR =       {Stanislav Kolenikov},
  TITLE =        {Scrambled Halton sequences in Mata},
  JOURNAL =      {The Stata Jounral},
  YEAR =         {2012},
  volume =       {12},
  number =       {1},
  pages =        {29--44},
}

@ARTICLE{kolenikov:kennedy:2014,
  AUTHOR =       {Stanislav Kolenikov and Courtney Kennedy},
  TITLE =        {Evaluating Three Approaches to Statistically Adjust for Mode Effects},
  JOURNAL =      {The Journal of Survey Statistics and Methodology},
  YEAR =         {2014},
  volume =       {2},
  number =       {2},
  pages =        {126--158},
  doi =          {10.1093/jssam/smu004}
}

@ARTICLE{kolenikov:2014,
  AUTHOR =       {Stanislav Kolenikov},
  TITLE =        {Calibrating survey data using iterative proportional fitting},
  JOURNAL =      {The Stata Journal},
  YEAR =         {2014},
  volume =       {14},
  number =       {1},
  pages =        {22--59},
}

@incollection{kolenikov:pitblado:2014,
    address = {Hoboken, NJ},
    author = {Kolenikov, Stanislav and Pitblado, Jeff},
    booktitle = {Handbook of Health Survey Methods},
    chapter = {29},
    editor = {Johnson, Timothy P.},
    publisher = {Wiley},
    title = {Analysis of complex health survey data},
    year = {2014}
}

@INPROCEEDINGS{kole:mcge:keet:math:merc:kenn:2015,
  author =       {Stanislav Kolenikov
             and Kyley McGeeney
             and Scott Keeter
             and Nancy Mathiowetz
             and Andrew Mercer
             and Courtney Kennedy
             },
  title =        {Mode Effects in American Trends Panel: A Closer Look
    at the Person-Level and Item-Level Characteristics},
  booktitle =    {Proceedings of the Survey Research Methods Section},
  year =         {2015},
  address =      {Alexandria, VA},
  organization = {American Statistical Association},
  author =       {Kolenikov, S. and Mc{G}eeney, K. and Keeter, S. and Mathiowetz, N. and Mercer, A. and Kennedy, C.}
}

@ARTICLE{kolenikov:hammer:2015,
  author =       {Kolenikov, S. and Hammer, H.},
  title =        {Simultaneous Raking of Survey Weights at Multiple Levels},
  journal =      {Survey Methods: Insights from the Field},
  year =         {2015},
  note =         {Special issue on Weighting: Practical Issues and ?How to? Approach. Retrieved from https://surveyinsights.org/?p=5099},
}


@ARTICLE{kolenikov:2016,
  author =       {Stanislav Kolenikov},
  title =        {Post-stratification or non-response adjustment?},
  journal =      {Survey Practice},
  year =         {2016},
  volume =       {9},
  number =       {3},
  note =         {available at http://www.surveypractice.org/index.php/SurveyPractice/article/view/315}
}

@Book{kopka:daly:latex:4ed,
  author    = "Helmut Kopka and Patrick W. Daly",
  year      = 2003,
  title     = "Guide to \LaTeX",
  edition   = "4th",
  publisher = "Addison-Wesley"
}

@Article{korn1995analysis,
  author    = {Korn, Edward L. and Graubard, Barry I.},
  year      = {1995},
  title     = {Analysis of Large Health Surveys: Accounting for the
          Sampling Design},
  abstract  = {Large scale health surveys offer an opportunity to study
          associations between risk factors and outcomes in a
          population-based setting. Their complicated multistage
          sampling designs with differential probabilities of
          sampling individuals can make their analysis
          unstraightforward. Classical 'design-based' methods that
          yield approximately unbiased estimators of associations and
          standard errors can be highly inefficient. Model-based
          methods require assumptions which, if wrong, can lead to
          biased estimators of associations and standard errors. This
          paper examines the implications of utilizing the sample
          clustering and sample weights in the analysis of survey
          data. The approach is to estimate the inefficiency of using
          these aspects of the sampling design in a design-based
          analysis when actually it was unnecessary to do so. If the
          inefficiency is small, then that aspect of the design is
          used in a design-based fashion. Otherwise, additional
          modelling assumptions are incorporated into the analysis.
          By focusing attention on risk factor-outcome associations
          in large health surveys, specific recommendations for
          practitioners are given. The issues are demonstrated with
          real survey data including two controversial analyses
          previously published in medical references.},
  doi       = {doi:10.2307/2983292},
  journal   = {Journal of the Royal Statistical Society. Series A},
  number    = {2},
  pages     = {263--295},
  volume    = {158}
}

@Article{korn1995examples,
  author    = {Korn, Edward L. and Graubard, Barry I.},
  year      = {1995},
  title     = {Examples of Differing Weighted and Unweighted Estimates
          from a Sample Survey},
  abstract  = {Unweighted estimators using data collected in a sample
          survey can be badly biased, whereas weighted estimators are
          approximately unbiased for population parameters. We
          present four examples using data from the 1988 National
          Maternal and Infant Health Survey to demonstrate that
          weighted and unweighted estimators can be quite different,
          and to show the underlying causes of such differences.},
  journal   = {The American Statistician},
  number    = {3},
  pages     = {291--295},
  volume    = {49}
}

@Article{korn:graubard:1995,
  author    = {Edward L. Korn and Barry I. Graubard},
  year      = {1995},
  title     = {Analysis of Large Health Surveys: Accounting for the
          Sampling Design},
  journal   = {Journal of the Royal Statistical Society, Series A},
  volume    = {158},
  number    = {2},
  pages     = {263--295}
}

@Book{korn:graubard:1999,
  author    = "Edward L. Korn and Barry I. Graubard",
  year      = 1999,
  title     = "Analysis of Health Surveys",
  publisher = "John Wiley and Sons"
}

@Article{kott:2001,
  author    = {Kott, Phillip S.},
  year      = {2001},
  title     = {The Delete-a-Group Jackknife},
  journal   = {Journal of Official Statistics},
  number    = {4},
  pages     = {521--526},
  volume    = {17},
  abstract  = {The National Agricultural Statistics Service of the U.S.
          Department of Agriculture has been using the delete-a-group
          jackknife (DAGJK) in an increasing number of its surveys.
          This article discusses the theory behind the DAGJK when the
          first-phase of sampling is stratified and there are a large
          number of sampled units per stratum, which is the case for
          many list-based surveys. It goes on to propose an extension
          of the DAGJK for use when the number of sampled units per
          stratum is less than the number of jackknife replicates.}
}

@article{kott:2006,
    author = {Kott, Phillip S.},
    journal = {Survey Methodology},
    number = {2},
    pages = {133--142},
    title = {Using Calibration Weighting to Adjust for Nonresponse and Coverage Errors},
    volume = {32},
    year = {2006}
}

@article{kott:2011,
    author = {Kott, Phillip S.},
    journal = {Pakistani Journal of Statistics},
    number = {4},
    pages = {391--396},
    title = {A nearly pseudo-optimal method for keeping calibration weights from falling below unity
             in the absence of nonreponse or frame errors},
    volume = {27},
    year = {2011}
}

@InCollection{kott:2009,
  author    = {Kott, Phillip S.},
  editor    = {D. Pfeffermann and C. R. Rao},
  year      = {2009},
  title     = {Calibration Weighting: Combining Probability Samples and Linear Prediction Models},
  chapter   = {25},
  booktitle = {Sample Surveys: Inference and Analysis},
  publisher = {Elsevier},
  address   = {Oxford, UK},
  series    = {Handbook of Statistics},
  volume    = {29B}
}


@Article{kovacevic:binder:1997,
  author    = {Kovacevic, Milorad S. and Binder, David A. },
  year      = {1997},
  title     = {Variance Estimation for Measures of Income Inequality and
          Polarization -- The Estimating Equations Approach},
  journal   = {Journal of Official Statistics},
  number    = {1},
  pages     = {41--58},
  comment   = {http://www.jos.nu/Articles/abstract.asp?article=13141},
  volume    = {13},
  abstract  = {The estimating equations technique for variance estimation
          is demonstrated on a variety of income inequality and
          polarization measures when data are obtained in a complex
          survey. This method, based on the Taylor linearization, is
          computationally nonintensive and easy to implement. Six
          different measures are considered. An example based on data
          from the Canadian Survey of Consumer Finance is given.}
}

@Article{kovacevic:rai:2003,
  author    = {Kovacevic, Milorad S. and Rai, Shesh N.},
  year      = {2003},
  title     = {A Pseudo Maximum Likelihood Approach to Multilevel
          Modelling of Survey Data},
  abstract  = {An application of the pseudo maximum likelihood method to
          estimation of a multilevel linear model fitted to the
          dependent observations coming from a finite population is
          demonstrated. The proposed approach provides a closed form
          solution for estimating of the model parameters. It is
          computationally simpler than the iterative procedures
          suggested in the literature (e.g., the iterative
          probability weighted least squares method of Pfeffermann et
          al. (Pfeffermann, D., Skinner, C.J., Holmes, D.J.,
          Goldstein, H., Rasbash, J. (1998). Weighting for unequal
          selection probabilities in multilevel models. <i>Journal of
          Royal Statistical Society</i> B 60:2340)). Issues related
          to model and sample design hierarchies and their impact on
          estimation are discussed. A problem of weighting at
          different levels is addressed. A small simulation study
          showed that the proposed procedure is efficient even for
          small within group sample sizes.},
  doi       = {10.1081/STA-120017802},
  journal   = {Communications in Statistics - Theory and Methods},
  number    = {1},
  pages     = {103--121},
  publisher = {Taylor \& Francis},
  volume    = {32}
}

@Article{kovar:rao:wu:1988,
  author    = {Kovar, J G and Rao, J N K and Wu, C F J},
  year      = {1988},
  title     = {Bootstrap and other methods to measure errors in survey
          estimates},
  journal   = {Canadian Journal of Statistics},
  volume    = {16},
  pages     = {25--45},
  howpublished  = {supplement}
}

@TechReport{krelle:1997,
  author    = "Wilhelm Krelle",
  year      = 1997,
  title     = "How to Deal with Unobservable Variables in Economics",
  institution   = "Universitat Bonn",
  number    = "B/414",
  type      = "Discussion Paper"
}

@article{kreuter:presser:tourangeau:2008,
    abstract =
        {Although it is well established that self-administered
        questionnaires tend to yield fewer reports in the socially desirable
        direction than do interviewer-administered questionnaires, less is
        known about whether different modes of self-administration vary in
        their effects on socially desirable responding. In addition, most
        mode comparison studies lack validation data and thus cannot separate
        the effects of differential nonresponse bias from the effects of
        differences in measurement error. This paper uses survey and record
        data to examine mode effects on the reporting of potentially
        sensitive information by a sample of recent university graduates.
        Respondents were randomly assigned to one of three modes of data
        collection?conventional computer-assisted telephone interviewing
        ({CATI}), interactive voice recognition ({IVR}), and the Web?and were
        asked about both desirable and undesirable attributes of their
        academic experiences. University records were used to evaluate the
        accuracy of the answers and to examine differences in nonresponse
        bias by mode. Web administration increased the level of reporting of
        sensitive information and reporting accuracy relative to conventional
        {CATI}, with {IVR} intermediate between the other two modes. Both
        mode of data collection and the actual status of the respondent
        influenced whether respondents found an item sensitive.},
    author = {Kreuter, Frauke and Presser, Stanley and Tourangeau, Roger},
    doi = {10.1093/poq/nfn063},
    journal = {Public Opinion Quarterly},
    number = {5},
    pages = {847--865},
    title = {Social Desirability Bias in {CATI}, {IVR}, and Web Surveys: The Effects of Mode and Question Sensitivity},
    volume = {72},
    year = {2008}
}

@Article{krewski1981inference,
  author    = {Krewski, D. and Rao, J. N. K.},
  year      = {1981},
  title     = {Inference From Stratified Samples: Properties of the
          Linearization, Jackknife and Balanced Repeated Replication
          Methods},
  abstract  = {The asymptotic normality of both linear and nonlinear
          statistics and the consistency of the variance estimators
          obtained using the linearization, jackknife and balanced
          repeated replication (BRR) methods in stratified samples
          are established. The results are obtained as L ? ?
          within the context of a sequence of finite populations
          {?L} with L strata in ?L and are valid for any stratified
          multistage design in which the primary sampling units
          (psu's) are selected with replacement and in which
          independent subsamples are taken within those psu's
          selected more than once. In addition, some exact analytical
          results on the bias and stability of these alternative
          variance estimators in the case of ratio estimation are
          obtained for small L under a general linear regression
          model.},
  doi       = {10.2307/2240615},
  issn      = {00905364},
  journal   = {The Annals of Statistics},
  number    = {5},
  pages     = {1010--1019},
  publisher = {Institute of Mathematical Statistics},
  volume    = {9}
}

@Article{krewski:1978,
  author    = {Krewski, Daniel },
  year      = {1978},
  title     = {Jackknifing $U$-statistics in finite populations},
  comment   = {10.1080/03610927808827598},
  journal   = {Communications in Statistics - Theory and Methods},
  number    = {1},
  pages     = {1--12},
  volume    = {7},
  abstract  = {Arvesen (1969) and Arvesen \& Layard (1975) have
          established the asymptotic normality of the jackknife
          statistic and the consistencv of the jackknife variance
          estimator in the case of independent random variables where
          the unjackknifed statistic may be expressed as a function
          of several U-statistics. Similar results are obtained in
          the case of sampling without replacement from a finite
          population where the observations are no longer
          independent.}
}

@Article{krewski:rao:1981,
  author    = {Krewski, D. and Rao, J. N. K. },
  year      = {1981},
  title     = {Inference From Stratified Samples: Properties of the
          Linearization, Jackknife and Balanced Repeated Replication
          Methods},
  journal   = {The Annals of Statistics},
  number    = {5},
  pages     = {1010--1019},
  comment   = {http://links.jstor.org/sici?sici=0090-5364%28198109%299%3A5%3C1010%3AIFSSPO%3E2.0.CO%3B2-Y}
          ,
  volume    = {9}
}

@Article{kudo:1963,
  author    = "Akio K\^udo",
  year      = "1963",
  title     = "A Multivariate Analogue of the One-Sided Test",
  journal   = "Biometrika",
  volume    = "50",
  number    = "3 and 4",
  pages     = "403--418"
}

@Book{kuhn1996structure,
  author    = {Kuhn, Thomas S.},
  year      = {1996},
  title     = {The Structure of Scientific Revolutions},
  abstract  = {{There's a "Frank \& Ernest" comic strip showing a chick
          breaking out of its shell, looking around, and saying, "Oh,
          wow! Paradigm shift!" Blame the late Thomas Kuhn. Few
          indeed are the philosophers or historians influential
          enough to make it into the funny papers, but Kuhn is
          one.<p> <I>The Structure of Scientific Revolutions</I> is
          indeed a paradigmatic work in the history of science.
          Kuhn's use of terms such as "paradigm shift" and "normal
          science," his ideas of how scientists move from disdain
          through doubt to acceptance of a new theory, his stress on
          social and psychological factors in science--all have had
          profound effects on historians, scientists, philosophers,
          critics, writers, business gurus, and even the cartoonist
          in the street.<p> Some scientists (such as Steven Weinberg
          and Ernst Mayr) are profoundly irritated by Kuhn,
          especially by the doubts he casts--or the way his work has
          been used to cast doubt--on the idea of scientific
          progress. Yet it has been said that the acceptance of plate
          tectonics in the 1960s, for instance, was sped by
          geologists' reluctance to be on the downside of a paradigm
          shift. Even Weinberg has said that "<I>Structure</I> has
          had a wider influence than any other book on the history of
          science." As one of Kuhn's obituaries noted, "We all live
          in a post-Kuhnian age." <I>--Mary Ellen Curtin</I> }},
  edition   = {3rd},
  howpublished  = {Paperback},
  isbn      = {0226458083},
  publisher = {University Of Chicago Press}
}

@TechReport{kukuk:elliptic1998,
  author    = "Martin Kukuk",
  year      = 1998,
  title     = "Analyzing Ordered Categorical Data Derived from
          Elliptically Symmetric Distributions",
  address   = "University of T{\"u}bingen, Germany"
}

@Book{kullback:1997,
  author    = "Solomon Kullback",
  year      = 1997,
  title     = "Information Theory and Statistics",
  publisher = "Dover Publications",
  address   = "Mineola, NY"
}

@Article{kunitomo:matsushita:2009,
  author    = {Kunitomo, Naoto and Matsushita, Yukitoshi},
  year      = {2009},
  title     = {Asymptotic expansions and higher order properties of
          semi-parametric estimators in a system of simultaneous
          equations},
  doi       = {10.1016/j.jmva.2009.02.004},
  journal   = {Journal of Multivariate Analysis},
  number    = {8},
  pages     = {1727--1751},
  abstract  = {Asymptotic expansions are made for the distributions of
          the Maximum Empirical Likelihood (MEL) estimator and the
          Estimating Equation (EE) estimator (or the Generalized
          Method of Moments (GMM) in econometrics) for the
          coefficients of a single structural equation in a system of
          linear simultaneous equations, which corresponds to a
          reduced rank regression model. The expansions in terms of
          the sample size, when the non-centrality parameters
          increase proportionally, are carried out to O ($n^{-1}$) .
          Comparisons of the distributions of the MEL and GMM
          estimators are made. Also, we relate the asymptotic
          expansions of the distributions of the MEL and GMM
          estimators to the corresponding expansions for the Limited
          Information Maximum Likelihood (LIML) and the Two-Stage
          Least Squares (TSLS) estimators. We give useful information
          on the higher order properties of alternative estimators
          including the semi-parametric inefficiency factor under the
          homoscedasticity assumption.},
  volume    = {100}
}

@Article{kuonen:1999,
  author    = {Kuonen, D.},
  year      = {1999},
  title     = {Saddlepoint Approximations for Distributions of Quadratic
          Forms in Normal Variables},
  abstract  = {An extensive study and several applications show that a
          saddlepoint approximation for the distribution of quadratic
          forms in normal variates works very well and outperforms
          existing approximate methods in accuracy.},
  doi       = {10.2307/2673596},
  issn      = {00063444},
  journal   = {Biometrika},
  number    = {4},
  pages     = {929--935},
  publisher = {Biometrika Trust},
  volume    = {86}
}

@Article{lafon:white:1986,
  author    = {Lafontaine, F. and White, K. J. },
  year      = {1986},
  title     = {Obtaining any {Wald} statistic you want},
  journal   = {Economics Letters},
  volume    = {21},
  pages     = {35--40}
}

@Article{lahiri:2003,
  author    = {Lahiri, P. },
  year      = {2003},
  title     = {On the Impact of Bootstrap in Survey Sampling and
          Small-Area Estimation},
  journal   = {Statistical Science},
  pages     = {199--210},
  volume    = {18}
}

@Article{lair:1988,
  author    = {Laird, Nan M.},
  year      = 1988,
  title     = {Missing Data in Longitudinal Studies},
  journal   = {Statistics in Medicine},
  volume    = 7,
  pages     = {305--315}
}

@Article{lair:lang:stra:1987,
  author    = {Laird, Nan and Lange, Nicholas and Stram, Daniel},
  year      = 1987,
  title     = {Maximum Likelihood Computations With Repeated Measures:
          {A}pplication of the {EM} Algorithm},
  journal   = {Journal of the American Statistical Association},
  volume    = 82,
  pages     = {97--105}
}

@Article{lair:ware:1982,
  author    = {Laird, Nan M. and Ware, James H.},
  year      = 1982,
  title     = {Random-effects Models for Longitudinal Data},
  journal   = {Biometrics},
  volume    = 38,
  pages     = {963--974}
}

@Book{lamb:2002,
  author    = "Peter Lambert",
  year      = 2002,
  title     = "The Distribution and Redistribution of Income",
  publisher = "Manchester University Press",
  address   = "Manchester, UK",
  edition   = "3rd"
}

@Book{lambert:3rd:2002,
  author    = {Lambert, Peter},
  year      = {2002},
  title     = {The Distribution and Redistribution of Income: Third
          Edition},
  edition   = {3rd},
  howpublished  = {Paperback},
  isbn      = {0719057329},
  publisher = {Manchester University Press}
}

@Article{langford1998outliers,
  author    = {Langford, Ian H. and Lewis, Toby},
  year      = {1998},
  title     = {Outliers in Multilevel Data},
  abstract  = {This paper offers the data analyst a range of practical
          procedures for dealing with outliers in multilevel data. It
          first develops several techniques for data exploration for
          outliers and outlier analysis and then applies these to the
          detailed analysis of outliers in two large scale multilevel
          data sets from educational contexts. The techniques include
          the use of deviance reduction, measures based on residuals,
          leverage values, hierarchical cluster analysis and a
          measure called DFITS. Outlier analysis is more complex in a
          multilevel data set than in, say, a univariate sample or a
          set of regression data, where the concept of an outlying
          value is straightforward. In the multilevel situation one
          has to consider, for example, at what level or levels a
          particular response is outlying, and in respect of which
          explanatory variables; furthermore, the treatment of a
          particular response at one level may affect its status or
          the status of other units at other levels in the model.},
  journal   = {Journal of the Royal Statistical Society A},
  pages     = {121--160},
  volume    = {161}
}

@Book{lattin:carroll:green:2003,
  author    = {James Lattin and J. Douglas Carroll and Paul E. Green},
  year      = {2003},
  title     = {Analyzing Multivariate Data},
  publisher = {Thompson Brooks/Cole},
  series    = {Duxbury Applied Series},
  isbn      = {0534349749}
}

@ARTICLE{lau:2003,
  AUTHOR =       {Lau, L. K.},
  TITLE =        {Institutional factors affecting student retention},
  JOURNAL =      {Education},
  YEAR =         {2003},
  volume =       {124},
  number =       {1},
  pages =        {126--137},
  source =       {Isabella Zaniletti},
}

@INCOLLECTION{lawless:2003,
  AUTHOR =       {J. F. Lawless},
  TITLE =        {Event History Analysis and Longitudinal Surveys},
  BOOKTITLE =    {Analysis of Survey Data},
  PUBLISHER =    {Wiley},
  YEAR =         {2003},
  editor =       {Chambers, R. L. and Skinner, C. J.},
  chapter =      {15},
  pages =        {221--243},
}


@Article{lawley:1940,
  author    = {Lawley, D. N.},
  year      = {1940},
  title     = {The estimation of factor loadings by the method of maximum
          likelihood},
  journal   = {Proceedings of the Royal Society of Edinburgh},
  volume    = {60},
  pages     = {64--82}
}

@Book{lawley:maxwell:1971,
  author    = {Lawley, D. N. and Maxwell, A. E. },
  year      = 1971,
  title     = {Factor Analysis as a Statistical Method},
  publisher = {Butterworths},
  series    = {Butterworths Mathematical Texts}
}

@article{lavalee:beaumont:2015,
    author = {Lavall{\'{e}}e, Pierre and Beaumont, Jean-Fran{s C.}},
    journal = {Survey Methods: Insights from the Field,},
    note = {Available at  http://surveyinsights.org/?p=6255},
    title = {Why We Should Put Some Weight on Weights},
    volume = {Special Issue: Weighting: Practical Issues and 'How to' Approach},
    year = {2015}
}

@Article{lazar:mykland:1999,
  author    = {Lazar, N. A. and Mykland, P. A.},
  year      = {1999},
  title     = {Empirical likelihood in the presence of nuisance
          parameters},
  doi       = {10.1093/biomet/86.1.203},
  journal   = {Biometrika},
  number    = {1},
  pages     = {203--211},
  volume    = {86},
  abstract  = {Empirical likelihood was introduced as a nonparametric
          analogue of ordinary parametric likelihood. It is well
          known that the empirical likelihood ratio statistic
          inherits a number of properties of the parametric
          likelihood ratio statistic, such as the aymptotic
          chi-squared distribution and Bartlett correctability. This
          raises the question of whether or not the same is true in
          the presence of nuisance parameters. Recent work by Qin
          Lawless (1994) indicates that the chi-squared distribution
          is still valid to first order. We show that, when nuisance
          parameters are present, as introduced via a system of
          estimating equations, the asymptotic expansion for the
          signed square root of the empirical likelihood ratio
          statistic has a nonstandard form. This implies that the
          empirical likelihood ratio statistic itself does not permit
          a Bartlett correction. Keywords:Accuracy; Bartlett
          correction; Edgeworth expansion; Empirical likelihood;
          General estimating equation; Likelihood inference;
          Likelihood ratio test; Martingale inference.
          10.1093/biomet/86.1.203}
}

@Book{leba:mori:warw:1984,
  author    = "Ludovic Lebart and Alain Morineau and Kenneth M. Warwick",
  year      = 1984,
  title     = "Multivariate Descriptive Statistical Analysis",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{lee:1990,
  author    = {Lee, Sik-Yum},
  year      = {1990},
  title     = {Multilevel analysis of structural equation models},
  abstract  = {This paper considers the multilevel analysis of structural
          equation models with unbal anced sampling designs. The
          analysis is based on the maximum likelihood and the
          generalized least squares approaches. Basic statistical
          results for inference such as the asymptotic distribution
          of the estimators, goodness-of-fit test statistics for the
          validity of the model and functional constraints are
          developed. Computationally, the applications of the scoring
          algorithm and the Gauss-Newton algorithm are discussed.
          10.1093/biomet/77.4.763},
  doi       = {10.1093/biomet/77.4.763},
  journal   = {Biometrika},
  number    = {4},
  pages     = {763--772},
  volume    = {77}
}


@Article{lee:1972,
  author    = {Lee, Kok-Huat },
  year      = {1972},
  title     = {Partially Balanced Designs for Half Sample Replication
          Method of Variance Estimation},
  journal   = {Journal of the American Statistical Association},
  number    = {338},
  pages     = {324--334},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28197206%2967%3A338%3C324%3APBDFHS%3E2.0.CO%3B2-J},
  volume    = {67},
  abstract  = {Half sample replication has been used as a method for
          estimating variances in stratified sampling whereby two
          primary selections are drawn from each stratum. This
          article considers some procedures of improving the
          precision of the variance estimator computed from a
          partially balanced set of half samples. Both the
          theoretical and empirical findings suggest that the SAOA
          procedure is an effective method to employ, in particular
          with a "2-order partially balanced design".}
}

@Article{lee:1973,
  author    = {Lee, Kok-Huat },
  year      = {1973},
  title     = {Using Partially Balanced Designs for the Half Sample
          Replication Method of Variance Estimation},
  journal   = {Journal of the American Statistical Association},
  number    = {343},
  pages     = {612--614},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28197309%2968%3A343%3C612%3AUPBDFT%3E2.0.CO%3B2-7},
  volume    = {68}
}

@Article{lee:1980,
  author    = {Lee, Sik-Yum },
  year      = {1980},
  title     = {Estimation of covariance structure models with parameters
          subject to functional restraints},
  doi       = {10.1007/BF02293906},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {309--324},
  volume    = {45}
}

@Article{lee:1985,
  author    = {Lee, Sik-Yum },
  year      = {1985},
  title     = {On testing functional constraints in structural equation
          models},
  abstract  = { In the context of a robust generalized least squares
          approach, a new statistic for testing the validity of
          constraints in structural equation models is proposed. It
          is shown that the new test requires significantly less
          computational effort than the traditional one. Extension to
          models in several populations is also considered.
          Illustrative applications based on real data are given.
          10.1093/biomet/72.1.125 },
  doi       = {10.1093/biomet/72.1.125},
  journal   = {Biometrika},
  number    = {1},
  pages     = {125--131},
  volume    = {72}
}

@Article{lee:1990,
  author    = {Lee, Sik-Yum },
  year      = {1990},
  title     = {Multilevel analysis of structural equation models},
  comment   = {10.1093/biomet/77.4.763},
  journal   = {Biometrika},
  number    = {4},
  pages     = {763--772},
  volume    = {77},
  abstract  = {This paper considers the multilevel analysis of structural
          equation models with unbal anced sampling designs. The
          analysis is based on the maximum likelihood and the
          generalized least squares approaches. Basic statistical
          results for inference such as the asymptotic distribution
          of the estimators, goodness-of-fit test statistics for the
          validity of the model and functional constraints are
          developed. Computationally, the applications of the scoring
          algorithm and the Gauss-Newton algorithm are discussed.
          10.1093/biomet/77.4.763}
}

@article{lee:2006:propensity,
  title={Propensity score adjustment as a weighting scheme for volunteer panel web surveys},
  author={Lee, Sunghee},
  journal={Journal of official statistics},
  volume={22},
  number={2},
  pages={329},
  year={2006},
  publisher={Statistics Sweden (SCB)}
}

@Book{lee:2007,
  author    = {Lee, Sik-Yum},
  year      = {2007},
  title     = {Structural Equation Modelling: A {Bayesian} Approach},
  isbn      = {0470024232},
  publisher = {John Wiley and Sons},
  address   = {New York},
  series    = {Wiley Series in Probability and Statistics}
}

@Article{lee:chesher:1986,
  author    = {Lee, Lung-Fei and Chesher, Andrew },
  year      = {1986},
  title     = {Specification testing when score test statistics are
          identically zero},
  doi       = {10.1016/0304-4076(86)90045-X},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {121--149},
  volume    = {31},
  abstract  = {We investigate the problem of specification testing when
          the score vector evaluated at the restricted MLE is
          identically zero. Several econometric examples are
          provided. A general test procedure which generalizes the
          geometric principle of the score test is proposed. The Wald
          and the likelihood ratio tests are also analyzed. Even
          under such irregularities, the usual asymptotic
          distribution of the likelihood ratio statistics is still
          valid. However, the Wald-type statistics need to be
          modified. The generalized score, the likelihood ratio and
          the modified Wald tests are shown to be asymptotically
          equivalent. The asymptotic efficiency of the tests is
          derived.}
}

@Article{lee:hersh:1990,
  author    = {Lee, Soonmook and Hershberger, Scott},
  year      = {1990},
  title     = {A Simple Rule for Generating Equivalent Models in
          Covariance Structure Modeling},
  abstract  = {This study introduces the replacing rule as a
          simplification of Stelzl's (1986) four rules for the
          generation of recursive equivalent models. The replacing
          rule is applicable to nonrecursive as well as recursive
          models, and generates equivalent models through the
          replacement of direct paths with residual correlations,
          through the replacement of residual correlations with
          direct paths, or through the inversion of path directions.
          Examples of the use of the replacing rule are provided, and
          its advantages over Stelzl's four rules are discussed.},
  doi       = {10.1207/s15327906mbr2503\_4},
  journal   = {Multivariate Behavioral Research},
  number    = {3},
  pages     = {313--334},
  publisher = {Psychology Press},
  volume    = {25}
}

@Article{lee:jennrich:1979,
  author    = {Lee, S. and Jennrich, R. },
  year      = {1979},
  title     = {A study of algorithms for covariance structure analysis
          with specific comparisons using factor analysis},
  doi       = {10.1007/BF02293789},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {99--113},
  volume    = {44},
  abstract  = {Several algorithms for covariance structure analysis are
          considered in addition to the Fletcher-Powell algorithm.
          These include the Gauss-Newton, Newton-Raphson, Fisher
          Scoring, and Fletcher-Reeves algorithms. Two methods of
          estimation are considered, maximum likelihood and weighted
          least squares. It is shown that the Gauss-Newton algorithm
          which in standard form produces weighted least squares
          estimates can, in iteratively reweighted form, produce
          maximum likelihood estimates as well. Previously
          unavailable standard error estimates to be used in
          conjunction with the Fletcher-Reeves algorithm are derived.
          Finally all the algorithms are applied to a number of
          maximum likelihood and weighted least squares factor
          analysis problems to compare the estimates and the standard
          errors produced. The algorithms appear to give satisfactory
          estimates but there are serious discrepancies in the
          standard errors. Because it is robust to poor starting
          values, converges rapidly and conveniently produces
          consistent standard errors for both maximum likelihood and
          weighted least squares problems, the Gauss-Newton algorithm
          represents an attractive alternative for at least some
          covariance structure analyses.}
}

@Article{lee:poon:bentler:1992,
  author    = {Lee, Sik-Yum and Poon, Wai-Yin and Bentler, P. M. },
  year      = {1992},
  title     = {Structural equation models with continuous and polytomous
          variables},
  abstract  = {A two-stage procedure is developed for analyzing
          structural equation models with continuous and polytomous
          variables. At the first stage, the maximum likelihood
          estimates of the thresholds, polychoric covariances and
          variances, and polyserial covariances are simultaneously
          obtained with the help of an appropriate transformation
          that significantly simplifies the computation. An
          asymptotic covariance matrix of the estiates is also
          computed. At the second stage, the parameters in the
          structural covariance model are obtained via the
          generalized least squares approach. Basic statistical
          properties of the estimates are derived and some
          illustrative examples and a small simulation study are
          reported.},
  doi       = {10.1007/BF02294660},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {89--105},
  volume    = {57}
}

@ARTICLE{lee:wang:1996,
  AUTHOR =       {Sik-Yum Lee and S. J. Wang},
  TITLE =        {Sensitivity Analysis of Structural Equation Models},
  JOURNAL =      {Psychometrika},
  YEAR =         {1996},
  volume =       {61},
  number =       {1},
  pages =        {93--108},
  doi =          {10.1007/BF02296960}
}


@Book{lee:2007,
  author    = {Lee, Sik-Yum},
  year      = {2007},
  title     = {Structural Equation Modelling: A {Bayesian} Approach (Wiley
          Series in Probability and Statistics)},
  howpublished  = {Hardcover},
  isbn      = {0470024232},
  publisher = {Wiley}
}

@TechReport{lee:mas:2009,
  author    = {Lee, David and Mas, Alexandre},
  year      = {2009},
  title     = {Long-Run Impacts Of Unions On Firms: New Evidence From Financial Markets, 1961-1999},
  institution   = {National Bureau of Economic Research},
  number    = {14709},
  series    = {NBER Working Paper Series},
  type      = {Working Paper}
}

@Article{leger:macgibbon:2006,
  author    = {Leger, Christian and Mac{g}ibbon, Brenda},
  year      = {2006},
  title     = {On the bootstrap in cube root asymptotics},
  issn      = {0319-5724},
  journal   = {The Canadian Journal of Statistics/La revue canadienne de
          statistique},
  number    = {1},
  pages     = {29--44},
  publisher = {Statistical Society of Canada/Societe statistique du
          Canada},
  volume    = {34}
}

@Book{lehmann:2004,
  author    = {E. L. Lehmann},
  year      = {2004},
  title     = {Elements of Large Sample Theory},
  publisher = {Springer},
  address   = {New York}
}

@Book{lehmann:romano:2005,
  author    = {E. L. Lehmann and Joseph P. Romano},
  year      = {2005},
  title     = {Testing Statistical Hypothesis},
  publisher = {Springer},
  address   = {New York},
  edition   = {3rd}
}

@Book{leht:pahk:2004,
  author    = {Lehtonen, Risto and Pahkinen, Erkki },
  year      = {2004},
  title     = {Practical Methods for Design and Analysis of Complex
          Surveys},
  isbn      = {0470847697},
  publisher = {John Wiley \& Sons},
  series    = {Statistics in Practice},
  address   = {New York},
  edition   = {2nd}
}

@Article{leht:sarn:veij:2003,
  author    = {Lehtonen, Risto and S{\"a}rndal, Carl-Erik and Veijanen, Ari },
  year      = {2003},
  title     = {The effect of model choice in estimation for domains, including small domains},
  journal   = {Survey Methodology},
  number    = {1},
  pages     = {33--44},
  comment   = {http://www.statcan.ca/bsolc/english/bsolc?catno=12-001-X20030016605},
  volume    = {29}
}

@INCOLLECTION{lehto:veija:2009,
  AUTHOR =       {R. Lehtonen and A. Veijanen},
  TITLE =        {Design-based methods for domains and small areas},
  BOOKTITLE =    {Sample Surveys: Inference and Analysis},
  PUBLISHER =    {North Holland, Amsterdam},
  YEAR =         {2009},
  editor =       {D. Pfeffermann and C. R. Rao},
  volume =       {29B},
  series =       {Handbook of Statistics},
  pages =        {219--249},
}


@BOOK{lemieux:2009,
  AUTHOR =       {Christiane Lemieux},
  TITLE =        {{Monte} {Carlo} and Quasi-{Monte} {Carlo} Sampling},
  PUBLISHER =    {Springer},
  YEAR =         {2009},
  address =      {New York},
  isbn =         {9780387781648},
}

@article{lenis:nguyen:dong:stuart:2017,
    author = {Lenis, David and Nguyen, Trang Quynh and Dong, Nianbo and Stuart, Elizabeth A},
    title = {It's all about balance: propensity score matching in the context of complex survey data},
    journal = {Biostatistics},
    volume = {20},
    number = {1},
    pages = {147-163},
    year = {2017},
    month = {12},
    abstract = 
            "{Many research studies aim to draw causal inferences using data
             from large, nationally representative survey samples, and many of
             these studies use propensity score matching to make those causal
             inferences as rigorous as possible given the non-experimental
             nature of the data. However, very few applied studies are careful
             about incorporating the survey design with the propensity score
             analysis, which may mean that the results do not generate
             population inferences. This may be because few methodological
             studies examine how to best combine these methods. Furthermore,
             even fewer of them investigate different non-response mechanisms.
             This study examines methods for handling survey weights in
             propensity score matching analyses of survey data under different
             non-response mechanisms. Our main conclusions are: (i) whether the
             survey weights are incorporated in the estimation of the
             propensity score does not impact estimation of the population
             treatment effect, as long as good population treated-comparison
             balance is achieved on confounders, (ii) survey weights must be
             used in the outcome analysis, and (iii) the transferring of survey
             weights (i.e., assigning the weights of the treated units to the
             comparison units matched to them) can be beneficial under certain
             non-response mechanisms.}", 
    issn = {1465-4644}, 
    doi = {10.1093/biostatistics/kxx063}, 
    url = {https://doi.org/10.1093/biostatistics/kxx063}, 
    eprint =
     {https://academic.oup.com/biostatistics/article-pdf/20/1/147/27177023/kxx063.pdf},
}


@ARTICLE{lesage:2011,
  AUTHOR =       {Eric Lesage},
  TITLE =        {The user of estimating equations to perform a calibration on complex parameters},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2011},
  volume =       {37},
  number =       {1},
  pages =        {103--108},
}


@Book{less:kals:1992,
  author    = "Virginia M. Lesser and William D. Kalsbeek",
  year      = "1992",
  title     = "Non-sampling Error in Surveys",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{less:kals:1997,
  author    = "Virginia M. Lesser and William D. Kalsbeek",
  year      = "1997",
  title     = "A Comparison of Periodic Survey Designs Employing
          Multi-Stage Sampling",
  journal   = "Environmental and Ecological Statistics",
  volume    = "4",
  pages     = "117--130"
}

@Book{levy:lemeshow:2003,
  author    = {Levy, Paul S. and Lemeshow, Stanley },
  year      = {2003},
  title     = {Sampling of Populations: Methods and Applications},
  address   = {New York},
  edition   = {3rd},
  publisher = {John Wiley \& Sons}
}

@Article{li:le:sun:zidek:99,
  author    = "K.~H. Li and N.~D. Le and L. Sun and J.~V. Zidek",
  year      = 1999,
  title     = "Spatial-temporal models for ambient hourly {PM}$_{10}$ in
          {Vancouver}",
  journal   = "Environmetrics",
  volume    = "10",
  pages     = "321--338"
}

@Article{li:zidek:li:ozka:2000,
  author    = "K.~H. Li and J.~V. Zidek and N.~D. Le and {\:O}zkaynak,
          H.",
  year      = 2000,
  title     = "Interpolating Vancouver's daily ambient {PM}$_{10}$ field",
  journal   = "Environmetrics",
  pages     = "651--663",
  volume    = 11
}

@article{li:valliant:2009,
  title={Survey weighted hat matrix and leverages},
  author={Li, Jianzhu and Valliant, Richard},
  journal={Survey Methodology},
  volume={35},
  number={1},
  pages={15--24},
  year={2009}
}

@article{li:valliant:2011,
  title={Linear regression influence diagnostics for unclustered survey data},
  author={Li, Jianzhu and Valliant, Richard},
  journal={Journal of Official Statistics},
  volume={27},
  number={1},
  pages={99--119},
  year={2011}
}

@article{li:irimata:he:parker:2022,
  author = {Yan Li and Katherine E. Irimata and Yulei He and Jennifer Parker},
  doi = {doi:10.2478/jos-2022-0038},
  url = {https://doi.org/10.2478/jos-2022-0038},
  title = {Variable Inclusion Strategies through Directed Acyclic Graphs 
      to adjust Health Surveys subject to Selection Bias for Producing National Estimates},
  journal = {Journal of Official Statistics},
  number = {3},
  volume = {38},
  year = {2022},
  pages = {875--900}
}



@article{liang:zeger:1986,
  author = {Liang, Kung-Yee and Zeger, Scott L.},
  title = {Longitudinal data analysis using generalized linear models},
  volume = {73},
  number = {1},
  pages = {13--22},
  year = {1986},
  doi = {10.1093/biomet/73.1.13},
  journal = {Biometrika},
  abstract =
     {This paper proposes an extension of generalized linear models
     to the analysis of longitudinal data. We introduce a class of
     estimating equations that give consistent estimates of the
     regression parameters and of their variance under mild
     assumptions about the time dependence. The estimating equations
     are derived without specifying the joint distribution of a
     subject's observations yet they reduce to the score equations
     for niultivariate Gaussian outcomes. Asymptotic theory is
     presented for the general class of estimators. Specific cases
     in which we assume independence, m-dependence and exchangeable
     correlation structures from each subject are discussed.
     Efficiency of the pioposecl estimators in two simple situations
     is considered. The approach is closely related to
     quasi-likelihood.},
}


@Article{lillard:panis:1998,
  author    = {Lillard, Lee A. and Panis, Constantijn W. A.},
  year      = {1998},
  title     = {Panel Attrition from the Panel Study of Income Dynamics:
          Household Income, Marital Status, and Mortality},
  abstract  = {This analysis is concerned with the determinants of panel
          attrition from the Panel Study of Income Dynamics and with
          its consequences for estimation of dynamic behavioral
          models which exploit the panel or longitudinal
          information-household income dynamics, marriage formation
          and dissolution, and adult mortality risk. We develop and
          estimate joint models of attrition and one or more of these
          substantive processes, and allow for correlation across the
          equations through random effects. Although we find evidence
          of significant selectivity in attrition behavior, the
          biases that are introduced by ignoring selective attrition
          are very mild.},
  doi       = {10.2307/146436},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {437--457},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@article{lin:2000,
    abstract =
        {Binder (1992) proposed a method of fitting Cox's proportional
        hazards to models to survey data with complex sampling designs. He
        defined the regression parameter of interest as the solution to the
        partial likelihood score equation based on all the data values of the
        survey population under study, and developed heuristically a
        procedure to estimate the regression parameter and the corresponding
        variance. In this paper, we provide a formal justification of
        Binder's method. Furthermore, we present an alternative approach
        which regards the survey population as a random sample from an
        infinite universe and accounts for this randomness in the statistical
        inference. Under the alternative approach, the regression parameter
        retains its original interpretation as the log hazard ratio, and the
        statistical conclusion applies to other populations. The related
        problem of survival function estimation is also studied.},
    author = {Lin, D. Y.},
    doi = {10.1093/biomet/87.1.37},
    journal = {Biometrika},
    number = {1},
    pages = {37--47},
    title = {On fitting Cox's proportional hazards models to survey data},
    volume = {87},
    year = {2000}
}

@Book{linden:hamble:1996,
  editor    = {Wim J. van der Linden and Ronald K. Hambleton},
  year      = {1996},
  title     = {Handbook of Modern Item Response Theory},
  publisher = {Springer},
  address   = {New York}
}

@Article{lisk:numm:1995,
  author    = {Liski, Erkki P. and Nummi, Tapio},
  year      = 1995,
  title     = {Prediction and Inverse Estimation in Repeated-measures
          Models},
  journal   = {Journal of Statistical Planning and Inference},
  volume    = 47,
  pages     = {141--151}
}

@Article{lisk:numm:1996,
  author    = {Liski, Erkki P. and Nummi, Tapio},
  year      = 1996,
  title     = {Prediction in Repeated-measures Models With Engineering
          Applications},
  journal   = {Technometrics},
  volume    = 38,
  pages     = {25--36}
}

@Book{litl:rubin:1987,
  author    = "R. J. A. Little and D. B. Rubin",
  year      = 2002,
  title     = "Statistical Analysis with Missing Data",
  series    = "Wiley Series in Probability and Statistics",
  publisher = "Wiley",
  edition   = "2nd",
  address   = "New York"
}

@Book{litl:rubin:2002,
  author    = "R. J. A. Little and D. B. Rubin",
  year      = 2002,
  title     = "Statistical Analysis with Missing Data",
  series    = "Wiley Series in Probability and Statistics",
  publisher = "Wiley",
  edition   = "2nd",
  address   = "New York"
}

@Article{litt:schl:1985,
  author    = {Little, Roderick J. A. and Schluchter, Mark D.},
  year      = 1985,
  title     = {Maximum Likelihood Estimation for Mixed Continuous and
          Categorical Data With Missing Values},
  journal   = {Biometrika},
  volume    = 72,
  pages     = {497--512}
}

@Article{little:1986,
  author    = {Little, Roderick J. A.},
  year      = {1986},
  title     = {Survey Nonresponse Adjustments for Estimates of Means},
  doi       = {10.2307/1403140},
  journal   = {International Statistical Review},
  number    = {2},
  pages     = {139--157},
  volume    = {54}
}

@Article{little:1993,
  author    = {Little, Roderick J. A.},
  year      = {1993},
  title     = {Pattern-Mixture Models for Multivariate Incomplete Data},
  doi       = {10.2307/2290705},
  journal   = {Journal of the American Statistical Association},
  number    = {421},
  pages     = {125--134},
  volume    = {88}
}

@Article{little:1995,
  author    = {Little, Roderick J. A.},
  year      = {1995},
  title     = {Modeling the Drop-Out Mechanism in Repeated-Measures
          Studies},
  abstract  = {Subjects often drop out of longitudinal studies
          prematurely, yielding unbalanced data with unequal numbers
          of measures for each subject. Modern software programs for
          handling unbalanced longitudinal data improve on methods
          that discard the incomplete cases by including all the
          data, but also yield biased inferences under plausible
          models for the drop-out process. This article discusses
          methods that simultaneously model the data and the drop-out
          process within a unified model-based framework. Models are
          classified into two broad classes--random-coefficient
          selection models and random-coefficient pattern-mixture
          models--depending on how the joint distribution of the data
          and drop-out mechanism is factored. Inference is
          likelihood-based, via maximum likelihood or Bayesian
          methods. A number of examples in the literature are placed
          in this framework, and possible extensions outlined. Data
          collection on the nature of the drop-out process is
          advocated to guide the choice of model. In cases where the
          drop-out mechanism is not well understood, sensitivity
          analyses are suggested to assess the effect on inferences
          about target quantities of alternative assumptions about
          the drop-out process.},
  doi       = {10.2307/2291350},
  journal   = {Journal of the American Statistical Association},
  number    = {431},
  pages     = {1112--1121},
  volume    = {90}
}

@InCollection{little:2003:bayesian,
  author    = {Roderick J. Little},
  editor    = {R. Chambers and C. J. Skinner},
  year      = {2003},
  title     = {The {B}ayesian Approach to Sample Survey Inference},
  booktitle = {Analysis of Survey Data},
  publisher = {John Wiley and Sons},
  chapter   = {4}
}

@InCollection{little:2003:nonresponse,
  author    = {Roderick J. Little},
  editor    = {R. Chambers and C. J. Skinner},
  year      = {2003},
  title     = {Bayesian Methods for Unit and Item Nonresponse},
  booktitle = {Analysis of Survey Data},
  publisher = {John Wiley and Sons},
  chapter   = {18}
}

@Book{little:rubin:2002,
  author    = "R. J. A. Little and D. B. Rubin",
  year      = 2002,
  title     = "Statistical Analysis with Missing Data",
  series    = "Wiley Series in Probability and Statistics",
  publisher = "Wiley",
  edition   = "2nd",
  address   = "New York"
}

@article{little:varti:2003,
    abstract =
        {A basic estimation strategy in sample surveys is to weight units
        inversely proportional to the probability of selection and response.
        Response weights in this method are usually estimated by the inverse
        of the sample-weighted response rate in an adjustment cell, that is,
        the ratio of the sum of the sampling weights of respondents in a cell
        to the sum of the sampling weights for respondents and
        non-respondents in that cell. We show by simulations that weighting
        the response rates by the sampling weights to adjust for design
        variables is either incorrect or unnecessary. It is incorrect, in the
        sense of yielding biased estimates of population quantities, if the
        design variables are related to survey non-response; it is
        unnecessary if the design variables are unrelated to survey
        non-response. The correct approach is to model non-response as a
        function of the adjustment cell and design variables, and to estimate
        the response weight as the inverse of the estimated response
        probability from this model. This approach can be implemented by
        creating adjustment cells that include design variables in the
        cross-classification, if the number of cells created in this way is
        not too large. Otherwise, response propensity weighting can be
        applied.},
    author = {Little, Roderick J. and Vartivarian, Sonya},
    doi = {10.1002/sim.1513},
    journal = {Statistics in Medicine},
    number = {9},
    pages = {1589--1599},
    title = {On weighting the rates in non-response weights},
    volume = {22},
    year = {2003}
}

@Article{little:vart:2005,
  author    = {Roderick J. Little and Sonya Vartivarian},
  year      = {2005},
  title     = {Does Weighting for Nonresponse Increase the Variance of
          Survey Means?},
  journal   = {Survey Methodology},
  volume    = {31},
  number    = {2},
  pages     = {161--168},
  publisher = {Statistics Canada}
}

@article{little:2012,
    author = {Little, Roderick J.},
    journal = {Journal of Official Statistics},
    number = {3},
    pages = {309--334},
    title = {Calibrated {B}ayes, an Alternative Inferential Paradigm for Official Statistics},
    volume = {28},
    year = {2012}
}
@Book{liu2002monte,
  author    = {Liu, Jun S.},
  year      = {2002},
  title     = {{Monte} {Carlo} Strategies in Scientific Computing},
  abstract  = {{A large number of scientists and engineers employ Monte
          Carlo simulation and related global optimization techniques
          (such as simulated annealing) as an essential tool in their
          work. For such scientists, there is a need to keep up to
          date with several recent advances in Monte Carlo
          methodologies such as cluster methods, data- augmentation,
          simulated tempering and other auxiliary variable methods.
          There is also a trend in moving towards a population-based
          approach. All these advances in one way or another were
          motivated by the need to sample from very complex
          distribution for which traditional methods would tend to be
          trapped in local energy minima. It is our aim to provide a
          self-contained and up to date treatment of the Monte Carlo
          method to this audience. The Monte Carlo method is a
          computer-based statistical sampling approach for solving
          numerical problems concerned with a complex system. The
          methodology was initially developed in the field of
          statistical physics during the early days of electronic
          computing (1945-55) and has now been adopted by researchers
          in almost all scientific fields. The fundamental idea for
          constructing Markov chain based Monte Carlo algorithms was
          introduced in the 1950s. This idea was later extended to
          handle more and more complex physical systems. In the
          1980s, statisticians and computer scientists developed
          Monter Carlo-based algorithms for a wide variety of
          integration and optimization tasks. In the 1990s, the
          method began to play an important role in computational
          biology. Over the past fifty years, reasearchers in diverse
          scientific fields have studied the Monte Carlo method and
          contributed to its development. Today, a large number of
          scientisits and engineers employ Monte Carlo techniques as
          an essential tool in their work. For such scientists, there
          is a need to keep up-to-date with recent advances in Monte
          Carlo methodologies.}},
  howpublished  = {Hardcover},
  isbn      = {0387952306},
  publisher = {Springer}
}

@Article{liu2006multicategory,
  author    = {Liu, Yufeng and Shen, Xiaotong},
  year      = {2006},
  title     = {Multicategory \$\Psi\$-Learning},
  abstract  = {In binary classification, margin-based techniques usually
          deliver high performance. As a result, a multicategory
          problem is often treated as a sequence of binary
          classifications. In the absence of a dominating class, this
          treatment may be suboptimal and may yield poor performance,
          such as for support vector machines (SVMs). We propose a
          novel multicategory generalization of -learning that treats
          all classes simultaneously. The new generalization
          eliminates this potential problem while at the same time
          retaining the desirable properties of its binary
          counterpart. We develop a statistical learning theory for
          the proposed methodology and obtain fast convergence rates
          for both linear and nonlinear learning examples. We
          demonstrate the operational characteristics of this method
          through a simulation. Our results indicate that the
          proposed methodology can deliver accurate class prediction
          and is more robust against extreme observations than its
          SVM counterpart.},
  comment   = {Extend the idea of support vectors and SVM for multiple
          categories by introducing special multivariate
          kernel/spline functions.},
  doi       = {10.1198/016214505000000781},
  issn      = {0162-1459},
  journal   = {Journal of the American Statistical Association},
  number    = {474},
  pages     = {500--509},
  publisher = {American Statistical Association},
  volume    = {101}
}

@Article{liu:1988,
  author    = {Liu, Regina Y. },
  year      = {1988},
  title     = {Bootstrap Procedures under some Non-I.I.D. Models},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {1696--1708},
  volume    = {16}
}

@Book{liu:2002,
  author    = {Liu, Jun S. },
  year      = {2002},
  title     = {{Monte} {Carlo} Strategies in Scientific Computing},
  howpublished  = {Hardcover},
  isbn      = {0387952306},
  publisher = {Springer},
  address   = {New York}
}

@article{liu:chen:2010,
    abstract =
       {Empirical likelihood is a popular nonparametric or
       semi-parametric statistical method with many nice statistical
       properties. Yet when the sample size is small, or the
       dimension of the accompanying estimating function is high,
       the application of the empirical likelihood method can be
       hindered by low precision of the chi-square approximation and
       by nonexistence of solutions to the estimating equations. In
       this paper, we show that the adjusted empirical likelihood is
       effective at addressing both problems. With a specific level
       of adjustment, the adjusted empirical likelihood achieves the
       high-order precision of the Bartlett correction, in addition
       to the advantage of a guaranteed solution to the estimating
       equations. Simulation results indicate that the confidence
       regions constructed by the adjusted empirical likelihood have
       coverage probabilities comparable to or substantially more
       accurate than the original empirical likelihood enhanced by
       the Bartlett correction.},
    author = {Liu, Yukun and Chen, Jiahua},
    journal = {Annals of Statistics},
    number = {3},
    pages = {1341--1362},
    title = {Adjusted empirical likelihood with high-order precision},
    volume = {38},
    year = {2010}
}

@Article{ljung:1982,
  author    = "Greta M. Ljung",
  year      = 1982,
  title     = "The Likelihood Function for a Stationary Gaussian
          Autoregressive-Moving Average Process with Missing
          Observations",
  journal   = "Biometrika",
  volume    = 69,
  number    = 1,
  pages     = "265--268"
}

@Book{loehlin2004latent,
  author    = {Loehlin, John C.},
  year      = {2004},
  title     = {Latent Variable Models: An Introduction to Factor, Path,
          and Structural Equation Analysis (Latent Variable Models:
          An Introduction to)},
  abstract  = {{<P>This book introduces multiple-latent variable models
          by utilizing path diagrams to explain the underlying
          relationships in the models. This approach helps less
          mathematically inclined students grasp the underlying
          relationships between path analysis, factor analysis, and
          structural equation modeling more easily. A few sections of
          the book make use of elementary matrix algebra. An appendix
          on the topic is provided for those who need a review. The
          author maintains an informal style so as to increase the
          book's accessibility. Notes at the end of each chapter
          provide some of the more technical details. The book is not
          tied to a particular computer program, but special
          attention is paid to LISREL, EQS, AMOS, and Mx.<br> <br>
          <b>New in the fourth edition of <i>Latent Variable
          Models:</i> <br> *<b>a data CD</b> that features the
          correlation and covariance matrices used in the exercises;
          <br> *<b>new sections</b> on missing data, non-normality,
          mediation, factorial invariance, and automating the
          construction of path diagrams; and<br> *<b>reorganization
          of</b> chapters 3-7 to enhance the flow of the book and its
          flexibility for teaching.<br> <br> Intended for advanced
          students and researchers in the areas of social,
          educational, clinical, industrial, consumer, personality,
          and developmental psychology, sociology, political science,
          and marketing, some prior familiarity with correlation and
          regression is helpful.<br></P>}},
  howpublished  = {Paperback},
  isbn      = {0805849106},
  publisher = {{Lawrence Erlbaum Associates, Inc.}}
}

@Book{loehlin:2004,
  author    = {Loehlin, John C. },
  year      = {2004},
  title     = {Latent Variable Models: An Introduction to Factor, Path,
          and Structural Equation Analysis (Latent Variable Models:
          An Introduction to)},
  isbn      = {0805849106},
  publisher = {{Lawrence Erlbaum Associates, Inc.}}
}

@Book{lohr:1999,
  author    = {Lohr, Sharon L. },
  year      = {1999},
  title     = {Sampling: Design and Analysis},
  howpublished  = {Hardcover},
  isbn      = {0534353614},
  publisher = {{Duxbury Press}}
}

@Book{lohr:2009:sampling,
  author    = {Lohr, Sharon L. },
  year      = {2009},
  title     = {Sampling: Design and Analysis},
  howpublished  = {Hardcover},
  publisher = {{Duxbury Press}},
  edition   = {2nd}
}

@incollection{lohr:2009:multiframe,
    author = {Lohr, Sharon L.},
    doi = {10.1016/S0169-7161(08)00004-7},
    pages = {71--88},
    publisher = {Elsevier},
    title = {Multiple-Frame Surveys},
    booktitle = {Handbook of Statistics; Sample Surveys: Design, Methods and Applications},
    volume = {29},
    issue = {A},
    year = {2009}
}

@article{lohr:brick:2014,
    abstract =
        {This paper presents several results for the optimal allocation of
        dual frame designs under different conditions, assuming the
        estimators are poststratified to domain totals. Both screener and
        overlap designs are discussed, and the results of the allocation for
        the two types of designs are illustrated by showing the efficiency of
        optimal designs in dual frame landline and cell phone surveys. We
        begin with results on optimal sample allocation to the frames
        assuming complete response and a linear cost function. We then
        compare the variance for estimates from the optimal screening and
        overlap designs, and extend these results to the situation where
        response rates differ for the landline and cell phone domains. We
        find that a screening design achieves a smaller variance than an
        overlap design with the same total cost under many cost structures.},
    author = {Lohr, Sharon L. and Brick, J. Michael},
    doi = {10.1093/jssam/smu016},
    journal = {Journal of Survey Statistics and Methodology},
    number = {4},
    pages = {388--409},
    title = {Allocation for Dual Frame Telephone Surveys with Nonresponse},
    volume = {2},
    year = {2014}
}

@Book{long:1997,
  author    = "J. Scott Long",
  year      = 1997,
  title     = "Regression Models for Categorical and Limited Dependent
          Variables",
  publisher = "SAGE",
  address   = "Thousand Oaks"
}

@book{long:2008,
    author = {Long, J. Scott},
    isbn = {9781597180474},
    publisher = {Stata Press},
    title = {The Workflow of Data Analysis Using Stata},
    year = {2008}
}

@book{longford:2005,
    address = {New York},
    author = {Longford, Nicholas T.},
    isbn = {1852337605},
    publisher = {Springer},
    series = {Statistics for Social and Behavioral Sciences},
    title = {Missing Data and Small-Area Estimation: Modern Analytical Equipment for the Survey Statistician},
    year = {2005}
}

@Book{lord:1980,
  author    = {Lord, Frederic M.},
  year      = {1980},
  title     = {Applications of Item Response Theory to Practical Testing
          Problems},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Hillsdale, NJ}
}

@Article{louis:1982,
  author    = "T. A. Louis",
  year      = 1982,
  title     = "Finding the Observed Information Matrix When Using the
          {EM} algorithm",
  journal   = "Journal of the Royal Statistical Society",
  volume    = 44,
  series    = "B",
  pages     = "226--233"
}

@Article{lubo:witt:2004
}

@Book{luce:1959,
  author    = {Luce, R. D.},
  year      = {1959},
  title     = {Individual Choice Behavior: A Theoretical Analysis},
  publisher = {John Wiley and Sons},
  address   = {New York}
}

@MISC{lumley:scott:2013,
  author =       {Thomas Lumley and Alastair Scott},
  title =        {AIC and BIC for Survey Data},
  howpublished = {Presentation at the 2013 Graybill Conference on Modern Survey Statistics},
  year =         {2013},
  address =      {Colorado State University, Fort Collins, Colorado},
  note =         {Available at http://www.stat.colostate.edu/graybillconference2013/Presentations/Scott.pdf},
}

@article{lumley:scott:2015,
  author =       {Thomas Lumley and Alastair Scott},
  title =        {AIC and BIC for Modeling with Complex Survey Data},
  year =         {2015},
  journal =      {Journal of Survey Statistics and Methodology},
  volume =       {3},
  number =       {1},
  pages =        {1--18}
}

@misc{lumley:2015:svy3303,
  AUTHOR =       {Lumley, Thomas S.},
  TITLE =        {Package `survey', v. 3.30-3},
  YEAR =         {2015},
}

@misc{lumley:2018:svy334,
  AUTHOR =       {Lumley, Thomas S.},
  TITLE =        {Package `survey', v. 3.34},
  YEAR =         {2018},
  note =         {Available at http://r-survey.r-forge.r-project.org/survey/.}
}


@book{lumley:2010,
    author = {Lumley, Thomas S.},
    isbn = {0470284307},
    publisher = {Wiley},
    title = {Complex Surveys: A Guide to Analysis Using R (Wiley Series in Survey Methodology)},
    address   = {New York},
    year = {2010}
}

@article{lundstrom:sarndal:1999,
    author = {Lundstr{\"{o}}m, Sixten and S{\"{a}}rndal, Carl-Erik},
    journal = {Journal of Official Statistics},
    number = {2},
    pages = {305--327},
    title = {Calibration as a Standard Method for Treatment of Nonresponse},
    volume = {15},
    year = {1999}
}


@Article{maccallum:austin:2000,
  author    = {Mac{C}allum, Robert C. and Austin, James T. },
  year      = {2000},
  title     = {Applications of Structural Equation Modeling in
          Psychological Research},
  doi       = {10.1146/annurev.psych.51.1.201},
  journal   = {Annual Review of Psychology},
  number    = {1},
  pages     = {201--226},
  volume    = {51},
  abstract  = {This chapter presents a review of applications of
          structural equation modeling (SEM) published in
          psychological research journals in recent years. We focus
          first on the variety of research designs and substantive
          issues to which SEM can be applied productively. We then
          discuss a number of methodological problems and issues of
          concern that characterize some of this literature. Although
          it is clear that SEM is a powerful tool that is being used
          to great benefit in psychological research, it is also
          clear that the applied SEM literature is characterized by
          some chronic problems and that this literature can be
          considerably improved by greater attention to these
          issues.}
}

@Article{maccallum:roznowski:necowitz:1992,
  author    = {Mac{C}allum, Robert C. and Roznowski, Mary and Necowitz,
          Lawrence B.},
  year      = {1992},
  title     = {Model modifications in covariance structure analysis: The
          problem of capitalization on chance.},
  journal   = {Psychological Bulletin},
  number    = {3},
  pages     = {490--504},
  volume    = {111},
  abstract  = {In applications of covariance structure modeling in which
          an initial model does not fit sample data well, it has
          become common practice to modify that model to improve its
          fit. Because this process is data driven, it is inherently
          susceptible to capitalization on chance characteristics of
          the data, thus raising the question of whether model
          modifications generalize to other samples or to the
          population. This issue is discussed in detail and is
          explored empirically through sampling studies using 2 large
          sets of data. Results demonstrate that over repeated
          samples, model modifications may be very inconsistent and
          cross-validation results may behave erratically. These
          findings lead to skepticism about generalizability of
          models resulting from data-driven modifications of an
          initial model. The use of alternative a priori models is
          recommended as a preferred strategy.}
}

@article{maccallum:browne:sugawara:1996,
    address = {Department of Psychology, Ohio State University, 1885 Neil Avenue, Columbus, OH 43210, United States},
    author = {Mac{C}allum, R. C. and Browne, M. W. and Sugawara, H. M.},
    journal = {Psychological Methods},
    number = {2},
    pages = {130--149},
    title = {Power analysis and determination of sample size for covariance structure modeling},
    volume = {1},
    year = {1996},
    abstract =
       {A framework for hypothesis testing and power analysis in the
       assessment of fit of covariance structure models is
       presented. We emphasize the value of confidence intervals for
       fit indices, and we stress the relationship of confidence
       intervals to a framework for hypothesis testing. The approach
       allows for testing null hypotheses of not-good fit, reversing
       the role of the null hypothesis in conventional tests of
       model fit, so that a significant result provides strong
       support for good fit. The approach also allows for direct
       estimation of power, where effect size is defined in terms of
       a null and alternative value of the root-mean-square error of
       approximation fit index proposed by J. H. Steiger and J. M.
       Lind (1980). It is also feasible to determine minimum sample
       size required to achieve a given level of power for any test
       of fit in this framework. Computer programs and examples are
       provided for power analyses and calculation of minimum sample
       sizes. Copyright 1996 by the American Psychological
       Association, Inc.},
}


@article{maccallum:browne:cai:2006,
    author = {Mac{C}allum, Robert C. and Browne, Michael W. and Cai, Li},
    doi = {10.1037/1082-989X.11.1.19},
    issn = {1082-989X},
    journal = {Psychological Methods},
    number = {1},
    pages = {19--35},
    title = {Testing differences between nested covariance structure models:
             Power analysis and null hypotheses.},
    volume = {11},
    year = {2006},
    abstract =
       {For comparing nested covariance structure models, the
       standard procedure is the likelihood ratio test of the
       difference in fit, where the null hypothesis is that the
       models fit identically in the population. A procedure for
       determining statistical power of this test is presented where
       effect size is based on a specified difference in overall fit
       of the models. A modification of the standard null hypothesis
       of zero difference in fit is proposed allowing for testing an
       interval hypothesis that the difference in fit between models
       is small, rather than zero. These developments are combined
       yielding a procedure for estimating power of a test of a null
       hypothesis of small difference in fit versus an alternative
       hypothesis of larger difference.},
}

@article{maccallum:lee:browne:2010,
    author = {Mac{C}allum, R. C. and Taehun Lee and Browne, M. W.},
    journal = {Structural Equation Modeling: A Multidisciplinary Journal},
    number = {17},
    pages = {23--41},
    title = {The Issue of Isopower in Power Analysis for Tests of Structural Equation Models},
    volume = {1},
    year = {2010},
}



@Article{mackinnon:white:1985,
  author    = {{MacK}innon, James G. and White, Halbert },
  year      = {1985},
  title     = {Some heteroskedasticity-consistent covariance matrix
          estimators with improved finite sample properties},
  comment   = {doi:10.1016/0304-4076(85)90158-7},
  journal   = {Journal of Econometrics},
  number    = {3},
  pages     = {305--325},
  volume    = {29},
  abstract  = {We examine several modified versions of the
          heteroskedasticity-consistent covariance matrix estimator
          of Hinkley (1977) and White (1980). On the basis of
          sampling experiments which compare the performance of quasi
          t-statistics, we find that one estimator, based on the
          jackknife, performs better in small samples than the rest.
          We also examine the finite-sample properties of using
          modified critical values based on Edgeworth approximations,
          as proposed by Rothenberg (1984). In addition, we compare
          the power of several tests for heteroskedasticity, and find
          that it may be wise to employ the jackknife
          heteroskedasticity-consistent covariance matrix even in the
          absence of detected heteroskedasticity.}
}

@Article{mackinnon:1996,
  author={Mac{K}innon, James G},
  title={Numerical Distribution Functions for Unit Root and Cointegration Tests},
  journal={Journal of Applied Econometrics},
  year=1996,
  volume={11},
  number={6},
  pages={601--618},
}

@Article{mackinnon:haug:michelis:1999,
  author={Mac{K}innon, James G and Haug, Alfred A and Michelis, Leo},
  title={Numerical Distribution Functions of Likelihood Ratio Tests for Cointegration},
  journal={Journal of Applied Econometrics},
  year=1999,
  volume={14},
  number={5},
  pages={563--577},
}

@Article{macurdy:mroz:gritz:1998,
  author    = {Macurdy, Thomas and Mroz, Thomas and Gritz, Mark R.},
  year      = {1998},
  title     = {An Evaluation of the National Longitudinal Survey on
          Youth},
  doi       = {10.2307/146435},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {345--436},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Book{madd:limdep:1983,
  author    = "Maddala, G.",
  year      = 1983,
  title     = "Limited Dependent and Qualitative Variables in
          Econometrics",
  publisher = "Cambridge University Press"
}

@Book{maddala:limdep:1983,
  author    = "G. S. Maddala",
  year      = 1983,
  title     = "Limited-Dependent and Qualitative Variables in
          Econometrics",
  publisher = "Cambridge University Press",
  series    = "Econometric Society Monographs",
  volume    = 3,
  address   = "Cambridge, UK"
}

@Book{maddala:panel:1993,
  author    = "Maddala, G.",
  year      = 1993,
  title     = "The Econometrics of Panel Data",
  publisher = "Brookfield"
}

@Book{magn:neud:1999,
  author    = {Magnus, Jan R. and Neudecker, Heinz},
  year      = {1999},
  title     = {Matrix calculus with applications in statistics and
          econometrics},
  pages     = {395},
  edition   = "2nd",
  publisher = {John Wiley \& Sons}
}

@Article{magnus:1999,
  author    = {Jan Magnus},
  year      = {1999},
  title     = {The traditional pretest estimator},
  journal   = {Theory of Probability and Its Applications},
  volume    = {44},
  pages     = {293--308},
  note      = {http://center.kub.nl/staff/magnus/paper49.pdf}
}

@Book{magnus:abadir:2005,
  author    = {Abadir, Karim M. and Magnus, Jan R.},
  year      = {2005},
  title     = {Matrix Algebra (Econometric Exercises)},
  abstract  = {{The first volume of the Econometric Exercises Series,
          Matrix Algebra contains exercises relating to course
          material in matrix algebra that students are expected to
          know while enrolled in an (advanced) undegraduate or a
          postgraduate course in econometrics or statistics. The book
          features a comprehensive collection of exercises with
          complete answers. More than just a collection of exercises,
          the volume is a textbook organized in a completely
          different manner than the usual textbook. It can be used as
          a self-contained course in matrix algebra or as a
          supplementary text.}},
  howpublished  = {Hardcover},
  isbn      = {0521822890},
  publisher = {{Cambridge University Press}}
}

@Article{magnus:durbin:1999,
  author    = {J. R. Magnus and J. Durbin},
  year      = {1999},
  title     = {Estimation of regression coefficients of interest when
          other regression coefficients are of no interest},
  journal   = {Econometrica},
  volume    = {67},
  pages     = {639--643}
}

@Article{mammen:1993,
  author    = {Mammen, Enno},
  year      = {1993},
  title     = {Bootstrap and Wild Bootstrap for High Dimensional Linear
          Models},
  abstract  = {In this paper two bootstrap procedures are considered for
          the estimation of the distribution of linear contrasts and
          of F-test statistics in high dimensional linear models. An
          asymptotic approach will be chosen where the dimension p of
          the model may increase for sample size n\&\#8594;\&\#8734;.
          The range of validity will be compared for the normal
          approximation and for the bootstrap procedures.
          Furthermore, it will be argued that the rates of
          convergence are different for the bootstrap procedures in
          this asymptotic framework. This is in contrast to the usual
          asymptotic approach where p is fixed.},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {255--285},
  volume    = {21}
}

@Book{manly:1997,
  author    = "B. F. J. Manly",
  year      = 1997,
  title     = "Randomization, Bootstrap and {Monte} {Carlo} Methods in
          Biology",
  edition   = "2nd",
  publisher = "Chapman and Hall",
  address   = "London"
}

@Book{manski:1999,
  author    = {Manski, Charles F.},
  year      = {1999},
  title     = {Identification Problems in the Social Sciences},
  isbn      = {0674442849},
  publisher = {{Harvard University Press}}
}

@Book{manski:2003,
  author    = {Manski, Charles F.},
  year      = {2003},
  title     = {Partial Identification of Probability Distributions
          (Springer Series in Statistics)},
  howpublished  = {Hardcover},
  isbn      = {0387004548},
  publisher = {Springer}
}

@Book{marc:moust:2002,
  editor    = {G. A. Marcoulides and I. Moustaki},
  year      = {2002},
  title     = {Latent Variable and Latent Structure Models},
  publisher = {Lawrence Erlbaum},
  address   = {Mahwah, NJ}
}

@Book{marc:schum:1996,
  editor    = {George A. Marcoulides and Randall E. Schumacker},
  year      = {1996},
  title     = {Advanced Structural Equation Modeling: Issues and
          Techniques},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, New Jersey}
}

@Book{marcoulides:moustaki:2002ed,
  editor    = {Marcoulides, George and Moustaki, Irini},
  year      = {2002},
  title     = {Latent Variable and Latent Structure Models (Quantitative
          Methodology Series)},
  abstract  = {This edited volume features cutting-edge topics from the
          leading researchers in the areas of latent variable
          modeling. Content highlights include coverage of approaches
          dealing with missing values, semi-parametric estimation,
          robust analysis, hierarchical data, factor scores,
          multi-group analysis, and model testing. New methodological
          topics are illustrated with real applications. The material
          presented brings together two traditions: psychometrics and
          structural equation modeling. \_Latent Variable and Latent
          Structure Models'\_ thought-provoking chapters from the
          leading researchers in the area will help to stimulate
          ideas for further research for many years to come. This
          volume will be of interest to researchers and practitioners
          from a wide variety of disciplines, including biology,
          business, economics, education, medicine, psychology,
          sociology, and other social and behavioral sciences. A
          working knowledge of basic multivariate statistics and
          measurement theory is assumed.},
  howpublished  = {Hardcover},
  isbn      = {080584046X},
  publisher = {Lawrence Erlbaum}
}

@Article{mard:1970,
  author    = {K.V. Mardia},
  year      = {1970},
  title     = {Measures of multivariate skewness and kurtosis with
          applications},
  journal   = {Biometrika},
  volume    = {57},
  pages     = {519--530}
}

@Article{mardia:1970,
  author    = {Mardia, K. V.},
  year      = {1970},
  title     = {Measures of multivariate skewness and kurtosis with
          applications},
  journal   = {Biometrika},
  volume    = {57},
  pages     = {519--530}
}

@Book{mardia:kent:bibby:multivariate,
  author    = "K. V. Mardia and J. T. Kent and J. M. Bibby",
  year      = "1980",
  title     = "Multivariate Analysis",
  publisher = "Academic Press",
  address   = "London"
}

@Article{marinicausality,
  author    = {Marini, Margaret M. and Singer, Burton},
  title     = {Causality in the Social Sciences}
}

@Article{maronna:1976,
  author    = {Maronna, Ricardo A. },
  year      = {1976},
  title     = {Robust {$M$}-Estimators of Multivariate Location and
          Scatter},
  journal   = {Annals of Statistics},
  number    = {1},
  pages     = {51--67},
  volume    = {4},
  abstract  = {Let $\mathbf{x}_1,\cdots, \mathbf{x}_n$ be a sample from
          an $m$-variate distribution which is spherically symmetric
          up to an affine transformation. This paper deals with the
          robust estimation of the location vector $\mathbf{t}$ and
          scatter matrix $\mathbf{V}$ by means of "$M$-estimators,"
          defined as solutions of the system: $\sum_i
          u_1(d_i)(\mathbf{x}_i - \mathbf{t}) = \mathbf{0}$ and
          $n^{-1}\sum_i u_2(d_i^2)(\mathbf{x}_i -
          \mathbf{t})(\mathbf{x}_i - \mathbf{t})' = \mathbf{V}$,
          where $d_i^2 = (\mathbf{x}_i -
          \mathbf{t})'\mathbf{V}^{-1}(\mathbf{x}_i - \mathbf{t})$.
          Existence and uniqueness of solutions of this system are
          proved under general assumptions about the functions $u_1$
          and $u_2$. Then the estimators are shown to be consistent
          and asymptotically normal. The breakdown bound and the
          influence function are calculated, showing some weaknesses
          of the estimates for high dimensionality. An algorithm for
          the numerical calculation of the estimators is described.
          Finally, numerical values of asymptotic variances, and
          Monte Carlo small-sample results are exhibited.},
  comment   = {Shows how Brower's fixed point theorem can be used in the
          proof of consistency and asymptotic normality.}
}

@Book{maronna:martin:yohai:2006,
  author    = {Maronna, Ricardo A. and Martin, Douglas R. and Yohai,
          Victor J. },
  year      = {2006},
  title     = {Robust Statistics: Theory and Methods},
  address   = {New York},
  publisher = {John Wiley and Sons}
}

@Article{maronna:yohai:1981,
  author    = {Maronna, Ricardo A. and Yohai, Victor J.},
  year      = {1981},
  title     = {Asymptotic behavior of general M-estimates for regression
          and scale with random carriers},
  comment   = {Example of the use of Brower's fixed point theorem in
          establishing the consistency of M-estimates.},
  doi       = {10.1007/BF00536192},
  journal   = {Probability Theory and Related Fields},
  number    = {1},
  pages     = {7--19==20},
  volume    = {58}
}

@InCollection{marsh:balla:hau:1996,
  author    = {Herbert W. Marsh and John R. Balla and Kit-Tai Hau},
  editor    = {George Marcoulides and Randy Schumaker},
  year      = {1996},
  title     = {An Evaluation of Incremental Fit Indices: A Clarification
          of Mathematical and Empirical Properties},
  booktitle = {Advanced Structural Equation Modeling Techniques},
  pages     = {315--353},
  publisher = {Erlbaum},
  address   = {Mahwah, NJ}
}

@Article{marsh:byrne:craven:1992,
  author    = {Marsh, Herbert W. and Byrne, Barbara M. and Craven, Rhonda
          },
  year      = {1992},
  title     = {Overcoming Problems in Confirmatory Factor Analyses of
          MTMM Data: The Correlated Uniqueness Model and Factorial
          Invariance},
  abstract  = {The general model typically used in the confirmatory
          factor analysis (CFA) approach to multitrait-multimethod
          (MTMM) data is plagued with methodological problems and
          frequently results in improper or unstable solutions. Here
          we reanalyze data from a previously published study,
          demonstrating that this model may lead to inappropriate
          interpretations even when it does converge to a proper
          solution, and describe safeguards against this occurrence.
          The results support the correlated uniqueness model,
          diagnostic tests of the validity of CFA-MTMM solutions, the
          inclusion of external validity criteria in the MTMM design
          as described by Marsh (1988; 1989; Marsh \&amp; Bailey,
          19911, and the application of factorial invariance to test
          the stability of CFA-MTMM solutions. More generally, we
          demonstrate the flexibility of the CFA-MTMM approach for
          testing a variety of construct validity issues.},
  doi       = {http://dx.doi.org/10.1207/s15327906mbr2704\_1},
  journal   = {Multivariate Behavioral Research},
  number    = {4},
  pages     = {489--507},
  publisher = {Psychology Press},
  volume    = {27}
}


@INCOLLECTION{marsh:hau:grayson:2005,
  AUTHOR =       {Marsh, Herbert W. and Hau, Kit-Tai and Grayson, David},
  TITLE =        {Goodness of Fit in Structural Equation Models},
  BOOKTITLE =    {Contemporary psychometrics: A festschrift for Roderick P. {McD}onald.},
  PUBLISHER =    {Lawrence Erlbaum Associates Publishers},
  YEAR =         {2005},
  editor =       {Maydeu-Olivares, Albert and McArdle, John J.},
  pages =        {275--340},
}


@Book{masc:whin:green:1995,
  author    = "Andreu Mas-Colell and Michael D. Whinston and Jerry R.
          Green",
  year      = 1995,
  title     = "Microeconomic Theory",
  publisher = "Oxford University Press",
  address   = "Oxford, UK"
}

@Book{mascolell:whinston:green:1995,
  author    = {Mas-Colell, Andreu and Whinston, Michael D. and Green,
          Jerry R.},
  year      = {1995},
  title     = {Microeconomic Theory},
  abstract  = {{Many instructors of microeconomic theory have been
          waiting for a text that provides balanced and in-depth
          analysis of the essentials of microeconomics. Masterfully
          combining the results of years of teaching microeconomics
          at Harvard University, Andreu Mas-Colell, Michael Whinston,
          and Jerry Green have filled that conspicuous vacancy with
          their groundbreaking text, Microeconomic Theory. <P>The
          authors set out to create a solid organizational foundation
          upon which to build the effective teaching tool for
          microeconomic theory. The result presents unprecedented
          depth of coverage in all the essential topics, while
          allowing professors to "tailor-make" their course to suit
          personal priorities and style. Topics such as
          noncooperative game theory, information economics,
          mechanism design, and general equilibrium under uncertainty
          receive the attention that reflects their stature within
          the discipline. The authors devote an entire section to
          game theory alone, making it "free-standing" to allow
          instructors to return to it throughout the course when
          convenient. Discussion is clear, accessible, and engaging,
          enabling the student to gradually acquire confidence as
          well as proficiency. Extensive exercises within each
          chapter help students to hone their skills, while the
          text's appendix of terms, fully cross-referenced throughout
          the previous five sections, offers an accessible guide to
          the subject matter's terminology. Teachers of
          microeconomics need no longer rely upon scattered lecture
          notes to supplement their textbooks. Deftly written by
          three of the field's most influential scholars,
          Microeconomic Theory brings the readability,
          comprehensiveness, and versatility to the first-year
          graduate classroom that has long been missing.}},
  howpublished  = {Hardcover},
  isbn      = {0195073401},
  publisher = {Oxford University Press, USA}
}

@Article{mattson:1997,
  author    = {Mattson, Stefan },
  year      = {1997},
  title     = {How to Generate Non-normal Data for Simulation of
          Structural Equation Models},
  doi       = {10.1207/s15327906mbr3204\_3},
  journal   = {Multivariate Behavioral Research},
  number    = {4},
  pages     = {355--373},
  volume    = {32},
  abstract  = {A procedure for generating non-normal data for simulation
          of structural equation models is proposed. A simple
          transformation of univariate random variables is used for
          the generation of data on latent and error variables under
          some restrictions for the elements of the covariance
          matrices for these variables. Data on the observed
          variables is then computed from latent and error variables
          according to the model. It is shown that by controlling
          univariate skewness and kurtosis on pre-specified random
          latent and error variables, observed variables can be made
          to have a relatively wide range of univariate skewness and
          kurtosis characteristics according to the pre-specified
          model. Univariate distributions are used for the generation
          of data which enables a user to choose from a large number
          of different distributions. The use of the proposed
          procedure is illustrated for two different structural
          equation models and it is shown how PRELIS can be used to
          generate the data.}
}

@Book{matyas:gmm1999,
  editor    = "L\'aszl\'o M\'aty\'as",
  year      = 1999,
  title     = "Generalized Method of Moments Estimation",
  publisher = "Cambridge University Press"
}

@TechReport{maydeu:2001,
  author    = "Albert Maydeu-Olivares",
  year      = "2001",
  title     = "Testing categorized bivariate normality with two-stage
          polychoric correlation estimates",
  institution   = "University of Barcelona, Dept. of Psychology"
}

@Article{mcardle:mcdonald:1984,
  author    = {Mc{A}rdle, J. J. and Mc{D}onald, R. P. },
  year      = {1984},
  title     = {Some algebraic properties of the Reticular Action Model
          for moment structures.},
  journal   = {The British Journal of Mathematical and Statistical
          Psychology},
  pages     = {234--251},
  comment   = {http://view.ncbi.nlm.nih.gov/pubmed/6509005},
  volume    = {37},
  issue     = {2}
}

@Article{mccarthy:1976s,
  author    = {Mc{C}arthy, Philip J.},
  year      = {1976},
  title     = {The Use of Balanced Half-Sample Replication in
          Cross-Validation Studies},
  abstract  = {Cross-validation techniques involve omitting a portion of
          the available data, fitting a prediction function to the
          portion remaining, and then testing the fitted function on
          the omitted data. Ideally, one would like to repeat this
          process on all possible splits. In many instances, this is
          not computationally feasible. This paper argues and
          demonstrates, using half samples by way of illustration,
          that the balanced sampling techniques developed for the
          analysis of complex sample survey data provide an efficient
          way of sampling the population of all possible splits.},
  journal   = {Journal of the American Statistical Association},
  number    = {355},
  pages     = {596--604},
  volume    = {71}
}

@Article{mccarthy:1969,
  author    = {Mc{C}arthy, P. J. },
  year      = {1969},
  title     = {Pseudo-Replication: Half Samples},
  comment   = {This article introduces balanced repeated replication
          (BRR)... I'll tell more when I read it.},
  journal   = {Review of the International Statistical Institute},
  number    = {3},
  pages     = {239--264},
  volume    = {37}
}

@Article{mccarthy:1976,
  author    = {Mc{C}arthy, Philip J. },
  year      = {1976},
  title     = {The Use of Balanced Half-Sample Replication in
          Cross-Validation Studies},
  journal   = {Journal of the American Statistical Association},
  number    = {355},
  pages     = {596--604},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28197609%2971%3A355%3C596%3ATUOBHR%3E2.0.CO%3B2-R}
          ,
  volume    = {71},
  abstract  = {Cross-validation techniques involve omitting a portion of
          the available data, fitting a prediction function to the
          portion remaining, and then testing the fitted function on
          the omitted data. Ideally, one would like to repeat this
          process on all possible splits. In many instances, this is
          not computationally feasible. This paper argues and
          demonstrates, using half samples by way of illustration,
          that the balanced sampling techniques developed for the
          analysis of complex sample survey data provide an efficient
          way of sampling the population of all possible splits.}
}

@InCollection{mccarthy:snowden:1985,
  author    = {Mc{C}arthy, Philip J. and Snowden, C. B. },
  year      = 1985,
  title     = {The bootstrap and finite population sampling},
  booktitle = {Vital and Health Statistics},
  pages     = {2--95},
  type      = {Public Health Service Publications},
  number    = {85-1369},
  organization  = {U.S.\ Government Printing Office},
  address   = {Washington, DC}
}

@book{mcconnell:2004,
    author = {McConnell, Steve},
    edition = {2nd},
    isbn = {0735619670},
    publisher = {Microsoft Press},
    title = {Code Complete: A Practical Handbook of Software Construction},
    year = {2004}
}

@Book{mccul:nelder:1989,
  author    = "P. Mc{C}ullagh and J. A. Nelder",
  year      = 1989,
  title     = "Generalized Linear Models",
  edition   = "2nd",
  publisher = "Chapman and Hall",
  address   = "London"
}

@Article{mccul:rossi:1994,
  author    = {Mc{C}ulloch, Robert and Rossi, Peter E.},
  year      = 1994,
  title     = {An exact likelihood analysis of the multinomial probit
          model},
  journal   = {Journal of Econometrics},
  volume    = {64},
  number    = {1--2},
  pages     = {207--240}
}

@Book{mccul:searle:2000,
  author    = "Charles E. McCulloch and Shayle R. Searle",
  year      = 2000,
  title     = "Generalized, Linear, and Mixed Models",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{mccullagh:2000,
  author    = {Mc{c}ullagh, Peter},
  year      = {2000},
  title     = {Resampling and exchangeable arrays},
  abstract  = {The nonparametric, or resampling, bootstrap for a single
          unstructured sample corresponds to the algebraic operation
          of monoid composition, with a uniform distribution on the
          monoid. With this interpretation, the notion of resampling
          can be extended to designs having a certain
          group-invariance property. Two types of exchangeable array
          structures are considered in some detail, namely the
          one-way layout, and the two-way row-column exchangeable
          design. Although in both cases there is a unique group
          under which the sampling distribution of the observations
          is exchangeable, the choice of monoid is not unique.
          Different choices of monoid can lead to drastically
          different, and in some cases quite misleading,
          inferences.},
  journal   = {Bernoulli},
  number    = {2},
  pages     = {285--301},
  volume    = {6}
}

@Article{mcculloch:2009,
  author    = {McCulloch, Charles},
  year      = {2008},
  title     = {Joint modelling of mixed outcome types using latent
          variables},
  comment   = {http://smm.sagepub.com/content/17/1/53.full.pdf},
  doi       = {10.1177/0962280207081240},
  journal   = {Statistical Methods in Medical Research},
  number    = {1},
  pages     = {53--73},
  volume    = {17},
  abstract  = {After a brief review of the use of latent variables to
          accommodate the correlation among multiple outcomes of
          mixed types, through theoretical and numerical calculation,
          the consequences of such a construction are quantified. The
          effects of including latent variables on marginal inference
          in these models are contrasted with the situation for
          jointly normal outcomes. A simulation study illustrates the
          efficiency and reduction in bias gains possible in using
          joint models, and analysis of an example from the field of
          osteoarthritis illustrates potential practical differences.
          10.1177/0962280207081240}
}

@Article{mcdo:kran:1979,
  author    = {R.P. McDonald and W.R. Krane},
  year      = {1979},
  title     = {A {Monte Carlo} study of local indentifiability and
          degrees of freedom in the asymptotic likelihood ratio
          test},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {32},
  pages     = {121-132}
}

@Article{mcdon:2003,
  author    = "Trent L. McDonald",
  year      = "2003",
  title     = "Review of Environmental Monitoring Methods: Survey
          Designs",
  journal   = "Environmental Monitoring and Assessment",
  volume    = "85",
  pages     = "277--292"
}

@Article{mckev:zavo:1975,
  author    = {Mc{K}elvey, R.D. and Zavonia, W.},
  year      = {1975},
  title     = {A statistical model for the analysis of ordinal level
          dependent variables},
  journal   = {Journal of Mathematical Sociology},
  volume    = {4},
  pages     = {103--120}
}

@Book{mclac:krish:97,
  author    = "Geoffrey G. McLachlan and Thriyambakam Krishnan",
  year      = 1997,
  title     = "The {EM} Algorighm and Extensions",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Book{mclac:krish:2008,
  author    = "Geoffrey G. McLachlan and Thriyambakam Krishnan",
  year      = 2008,
  title     = "The {EM} Algorighm and Extensions",
  publisher = "John Wiley and Sons",
  address   = "New York",
  edition   = "2nd"
}

@Book{mclac:peel:2000,
  author    = {{McL}achlan, Geoffrey and Peel, David },
  year      = {2000},
  title     = {Finite Mixture Models},
  series    = {Wiley Series in Probability and Statistics},
  isbn      = {0471006262},
  publisher = {Wiley-Interscience},
  address   = {New York}
}

@Article{mclachlan:1987,
  author    = {Mc{L}achlan, G. J. },
  year      = {1987},
  title     = {On Bootstrapping the Likelihood Ratio Test Stastistic for
          the Number of Components in a Normal Mixture},
  doi       = {10.2307/2347790},
  journal   = {Applied Statistics},
  number    = {3},
  pages     = {318--324},
  volume    = {36},
  abstract  = {An important but difficult problem in practice is
          assessing the number of components g in a mixture. An
          obvious way of proceeding is to use the likelihood ratio
          test statistic ? to test for the smallest value of g
          consistent with the data. Unfortunately with mixture
          models, regularity conditions do not hold for - 2 log? to
          have it usual asymptotic null distribution of chi-squared.
          In this paper the role of the bootstrap is highlighted for
          the assessment of the null distribution of - 2 log? for the
          test of a single normal density versus a mixture of two
          normal densities in the univariate case.}
}

@ARTICLE{mcneish:2014,
  AUTHOR =       {D. M. Mc{N}eish},
  TITLE =        {Modeling sparsely clustered data: Design-based, model-based, and single-level methods},
  JOURNAL =      {Psychological Methods},
  YEAR =         {2014},
  volume =       {19},
  pages =        {552--563},
}


@article{mcquitty:2004,
    author = {{McQ}uitty, S.},
    doi = {10.1016/S0148-2963(01)00301-0},
    issn = {01482963},
    journal = {Journal of Business Research},
    number = {2},
    pages = {175--183},
    title = {Statistical power and structural equation models in business research},
    volume = {57},
    year = {2004},
    abstract =
       {It has long been recognized that statistical power is
       important for structural equation models, but only recently
       has it become possible to estimate the power associated with
       the test of an entire model. This article discusses the
       relevance of power for structural equation models and
       measurement validation, then examines the question of the
       degree of power associated with models published in business
       journals. Addressing this matter is essential, because
       statistical power directly affects the confidence with which
       test results can be interpreted. The issue is particularly
       appropriate in light of the increased use of structural
       equation models in business research. Using articles from
       some leading business journals as examples, a survey finds
       that power tends to be either very low, implying that too
       many false models will not be rejected (Type II error), or
       extremely high, causing overrejection of tenable models (Type
       I error). The implications of this discovery are explored,
       and recommendations that should improve the validity and
       application of structural equation modeling in business
       research are offered.},
}


@Book{mead1990design,
  author    = {Mead, Roger},
  year      = {1990},
  title     = {The Design of Experiments: Statistical Principles for
          Practical Applications},
  abstract  = {{Describes the statistical principles of good experimental
          design, explaining that good design of experiments is
          crucial to the success of research. Emphasizing the logical
          principles of statistical design, Professor Mead employs a
          minimum of mathematics. Throughout he assumes that the
          large-scale analysis of data will be performed by computers
          and he thus devotes more attention to discussions of how
          all of the available information can be used to extract the
          clearest answers to many questions. The principles are
          illustrated with a wide range of examples drawn from
          medicine, agriculture, industry, and other disciplines.
          Numerous exercises are given to help the reader practice
          techniques and to appreciate the difference that good
          design of experiments can make to a scientific project.}},
  howpublished  = {Paperback},
  isbn      = {0521287626},
  publisher = {{Cambridge University Press}}
}

@Book{mead:1990,
  author    = {Mead, Roger },
  year      = {1990},
  title     = {The Design of Experiments: Statistical Principles for
          Practical Applications},
  isbn      = {0521287626},
  publisher = {{Cambridge University Press}}
}

@Article{medina:solis:perez:nunez:maupome:casa:rosa:2006,
  author    = {Medina-Sol{\'\i}s, Carlo E. and {N}ez, Ricardo P. and
          Maupom{\'e}, Gerardo and Casanova-Rosado, Juan F.},
  year      = {2006},
  title     = {Edentulism Among Mexican Adults Aged 35 Years and Older
          and Associated Factors},
  journal   = {American Journal of Public Health},
  number    = {9},
  pages     = {1578--1582},
  volume    = {96}
}

@Article{meng:rubin:1991,
  author    = "Xiao-Li Meng and Donald B. Rubin",
  year      = 1991,
  title     = "Using {EM} to Obtain Asymptotic Variance-Covariance
          Matrices: The {SEM} Algorithm",
  journal   = "Journal of the American Statistical Association",
  volume    = 86,
  number    = 416,
  pages     = "899--909"
}

@article{melas:2005,
  title = "On the functional approach to optimal designs for nonlinear models",
  journal = "Journal of Statistical Planning and Inference",
  volume = "132",
  number = "1--2",
  pages = "93--116",
  year = "2005",
  doi = "DOI: 10.1016/j.jspi.2004.06.017",
  author = "Viatcheslav B. Melas",
}

@INPROCEEDINGS{merkouris:2010,
  author =       {Takis Merkouris},
  title =        {An Estimation Method for Matrix Survey Sampling},
  booktitle =    {Proceedings of the American Statistical Association},
  year =         {2010},
  series =       {Survey Research Methods Section},
  pages =        {4880--4886},
  address =      {Alexandria, VA},
  organization = {American Statistical Association},
  publisher =    {American Statistical Association},
}

@article{mercer:kreuter:keeter:stuart:2017,
    author = {Mercer, Andrew W. and Kreuter, Frauke and 
              Keeter, Scott and Stuart, Elizabeth A.},
    title = {Theory and Practice in Nonprobability Surveys: Parallels between Causal Inference and Survey Inference},
    journal = {Public Opinion Quarterly},
    volume = {81},
    number = {S1},
    pages = {250-271},
    year = {2017},
    month = {04},
    abstract = 
      {Many in the survey research community have expressed concern at the
      growing popularity of nonprobability surveys. The absence of random
      selection prompts justified concerns about self-selection producing
      biased results and means that traditional, design-based estimation is
      inappropriate. This paper seeks to provide insight into the conditions
      under which nonprobability surveys can be expected to provide estimates
      free of selection bias. In fields such as epidemiology and economics that
      routinely work with observational data, researchers have identified the
      necessary conditions for unbiased estimation of causal effects when
      treatments are not assigned randomly. Similar conditions apply to survey
      estimates when respondents are not randomly selected. Drawing on this
      body of research, we propose a framework composed of three elements that
      determine the level of selection bias in survey estimates. In this paper,
      we first provide a general overview of these components and demonstrate
      the link between causal inference and survey inference in the
      probability-based setting. Second, we give simplified examples to
      demonstrate how each of the components can contribute to bias in survey
      estimates. Finally, we review current practices in the area of
      nonprobability data collection and estimation, and specify how these
      methods relate to the elements identified here.},
    issn = {0033-362X},
    doi = {10.1093/poq/nfw060},
    url = {https://doi.org/10.1093/poq/nfw060},
    eprint = {https://academic.oup.com/poq/article-pdf/81/S1/250/18138915/nfw060.pdf},
}





@Book{meyer:2001,
  author    = {Carl D. Meyer},
  year      = {2001},
  title     = {Matrix Analysis and Applied Linear Algebra},
  publisher = {SIAM}
}

@Book{mikh:wider:2003,
  editor    = "Vladimir Mikhalev",
  year      = 2003,
  title     = "Inequality and Social Structure During the Transition",
  publisher = "Palgrave Macmillan",
  series    = "Studies in Development Economics and Policy"
}

@InProceedings{mili:ugar:1997,
  author    = {Militino, A. F. and Ugarte, M. D.},
  year      = 1997,
  title     = {Robustizing the Non-parametric {ML} Estimation of a
          Spatial Linear Mixed Model Using the {EM} Algorithm},
  booktitle = {PCmpSSt29B},
  volume    = 29,
  pages     = {533--539}
}

@article{micceri:1989,
    author = {Micceri, Theodore},
    journal = {Psychological Bulletin},
    number = {1},
    pages = {156--166},
    title = {The Unicorn, The Normal Curve, and Other Improbable Creatures},
    volume = {105},
    year = {1989}
}

@Book{minoux:1986,
  author    = "M. Minoux",
  year      = 1986,
  title     = "Mathematical Programming",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@book{mitchell:2010,
    author = {Mitchell, Michael N.},
    isbn = {9781597180764},
    publisher = {Stata Press},
    title = {Data Management Using Stata: A Practical Handbook},
    year = {2010}
}

@Book{mittel:judge:miller:2000,
  author    = {Mittelhammer, Ron C. and Judge, George G. and Miller,
          Douglas J. },
  year      = {2000},
  title     = {Econometric Foundations},
  howpublished  = {Hardcover},
  isbn      = {0521623944},
  publisher = {Cambridge University Press},
  abstract  = {Econometric Foundations establishes a new paradigm for
          teaching econometric problems to talented upper-level
          undergraduates, graduate students, and professionals. The
          complete package (text, accompanying CD-ROM, and electronic
          guide) provides relevance, clarity, and organization to
          those wishing to acquaint themselves with the principles
          and procedures for information processing and recovery from
          samples of economic data. In the real world such data are
          usually limited or incomplete, and the parameters sought
          are unobserved and not subject to direct observation or
          measurement. Econometric Foundations fully provides an
          operational understanding of a rich set of estimation and
          inference tools to master such data, including traditional
          likelihood based and non-traditional non-likelihood based
          procedures, that can be used in conjunction with the
          computer to address economic problems. The accompanying
          CD-ROM contains reviews of probability theory, principles
          of classical estimation and inference, and handling of
          ill-posed inverse problems in text-searchable electronic
          documents, an interactive Matrix Review manual with GAUSS
          LIGHT software, and an electronic Examples Manual. A
          separate Guide, which may be accessed through the Internet,
          further enhances the student's mastery of the topics by
          providing solutions guides to the questions and problems in
          the text. This text, CD-ROM, and electronic guide package
          make Econometric Foundations the most up-to-date and
          comprehensive learning resource available.}
}

@InBook{mittle:judge:schoen:2005,
  author    = {Mittelhammer, Ron C. and Judge, George G. and Schoenberg,
          Ron },
  editor    = {Andrews, Donald W. K. and Stock, James H. },
  year      = {2005},
  title     = {Empirical Evidence Concerning the Finite Sample
          Performance of {EL}-Type Structural Equation Estimation and
          Inference Methods},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {12},
  abstract  = {This paper presents empirical evidence concerning the
          finite sample performance of conventional and generalized
          empirical likelihood-type estimators that utilize
          instruments in the context of linear structural models
          characterized by endogenous explanatory variables. There
          are suggestions in the literature that traditional and
          non-traditional asymptotically efficient estimators based
          on moment equations may, for the relatively small sample
          sizes usually encountered in econometric practice, have
          relatively large biases and/or variances and provide an
          inadequate basis for estimation and inference. Given this
          uncertainty we use a range of data sampling processes and
          Monte Carlo sampling procedures to accumulate finite sample
          empirical evidence concerning these questions for a family
          of generalized empirical likelihood-type estimators in
          comparison to conventional 2SLS and GMM estimators.
          Solutions to EL-type empirical momentconstrained
          optimization problems present formidable numerical
          challenges. We identify effective optimization algorithms
          for meeting these challenges.}
}

@article{mitofsky:1970,
  title={Sampling of telephone households},
  author={Mitofsky, Warren},
  journal={unpublished CBS News memorandum},
  year={1970}
}

@Book{mitton:suth:weeks:2000,
  author    = "Mitton, Lavinia and Holly Sutherland and Melvyn Weeks",
  year      = "2000",
  title     = "Microsimulation Modelling for Policy Analysis, Challenges
          and Innovations",
  publisher = "Cambridge University Press",
  address   = "Cambridge"
}

@Book{mohr:1995,
  author    = {Lawrence B. Mohr},
  year      = {1995},
  title     = {Impact Analysis for Program Evaluation},
  publisher = {SAGE},
  address   = {Thousand Oaks, CA}
}

@Article{molenberghs2007likelihood,
  author    = {Molenberghs, Geert and Verbeke, Geert},
  year      = {2007},
  title     = {Likelihood Ratio, Score, and Wald Tests in a Constrained
          Parameter Space},
  comment   = {The article reviews the problem from the point of view of
          biostatistics and problems typical in biostat applications,
          mostly variance components estimation, and SAS
          implementations. Chernoff (1954), Perlman (1968) or Andrews
          (early 2000s) are not mentioned, although Silvapulle and
          Sen (2005) is. Not a terribly strong paper, in my
          opinion.},
  doi       = {10.1198/000313007X171322},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {1},
  pages     = {22--27},
  publisher = {American Statistical Association},
  volume    = {61}
}

@article{molina:rao:2010,
    author = {Molina, Isabel and Rao, J. N. K.},
    doi = {10.1002/cjs.10051},
    issn = {03195724},
    journal = {Canadian Journal of Statistics},
    number = {3},
    pages = {369--385},
    title = {Small area estimation of poverty indicators},
    volume = {38},
    year = {2010}
}

@Article{monahan2006professor,
  author    = {Monahan, John F.},
  year      = {2006},
  title     = {Professor C. R. Mudgeon and the "Order," or Writing
          Regression Questions So That Students Do Not Need a
          Calculator},
  comment   = {A pretty cool way to make sure all of your regression
          examples result in integer answers.},
  doi       = {10.1198/000313006X90323},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {1},
  pages     = {50--52},
  publisher = {American Statistical Association},
  volume    = {60}
}

@Article{mont:grag:burk:pare:2000,
  author    = {Montgomery, M.R. and M. Gragnolati and K.A. Burke and E.
          Paredes},
  year      = {2000},
  title     = {Measuring Living Standards with Proxy Variables},
  journal   = {Demography},
  volume    = {37},
  number    = {2},
  pages     = {155--174}
}

@article{mooijaart:1985,
    author = {Mooijaart, Ab},
    doi = {10.1007/BF02294108},
    journal = {Psychometrika},
    number = {3},
    pages = {323--342},
    title = {Factor analysis for non-normal variables},
    volume = {50},
    year = {1985},
    abstract =
        {Factor analysis for nonnormally distributed variables is
        discussed in this paper. The main difference between our
        approach and more traditional approaches is that not only
        second order cross-products (like covariances) are utilized,
        but also higher order cross-products. It turns out that
        under some conditions the parameters (factor loadings) can
        be uniquely determined. Two estimation procedures will be
        discussed. One method gives Best Generalized Least Squares
        (BGLS) estimates, but is computationally very heavy, in
        particular for large data sets. The other method is a least
        squares method which is computationally less heavy. In one
        example the two methods will be compared by using the
        bootstrap method. In another example real life data are
        analyzed.},
}

@Article{mooijaart:satorra:2009,
  author    = {Mooijaart, Ab and Satorra, Albert},
  year      = {2009},
  title     = {On Insensitivity of the Chi-Square Model Test to Nonlinear
          Misspecification in Structural Equation Models},
  doi       = {10.1007/s11336-009-9112-5},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {443--455},
  volume    = {74},
  abstract  = {In this paper, we show that for some structural equation
          models (SEM), the classical chi-square goodness-of-fit test
          is unable to detect the presence of nonlinear terms in the
          model. As an example, we consider a regression model with
          latent variables and interactions terms. Not only the model
          test has zero power against that type of misspecifications,
          but even the theoretical (chi-square) distribution of the
          test is not distorted when severe interaction term
          misspecification is present in the postulated model. We
          explain this phenomenon by exploiting results on asymptotic
          robustness in structural equation models. The importance of
          this paper is to warn against the conclusion that if a
          proposed linear model fits the data well according to the
          chi-quare goodness-of-fit test, then the underlying model
          is linear indeed; it will be shown that the underlying
          model may, in fact, be severely nonlinear. In addition, the
          present paper shows that such insensitivity to nonlinear
          terms is only a particular instance of a more general
          problem, namely, the incapacity of the classical chi-square
          goodness-of-fit test to detect deviations from zero
          correlation among exogenous regressors (either being them
          observable, or latent) when the structural part of the
          model is just saturated.}
}

@Book{mooney:duval:1993,
  author    = {Christopher Z. Mooney and Robert D. Duval},
  year      = {1993},
  title     = {Bootstrapping: A Nonparametric Approach to Statistical
          Inference},
  publisher = {SAGE Publications},
  volume    = {95},
  series    = {Quantitative Applications in the Social Sciences},
  address   = {Thousand Oaks, CA}
}

@Article{morgan:palmer:ridout:2007,
  author    = {Morgan, B. J. T. and Palmer, K. J. and Ridout, M. S. },
  year      = {2007},
  title     = {Negative Score Test Statistic},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {285--288},
  volume    = {61},
  abstract  = {If one uses the observed information matrix, rather than
          the expected information matrix in a score test, then it is
          possible to obtain a negative test statistic. A simple
          illustration of this is provided for a zero-inflated
          Poisson distribution.}
}

@book{morgan:winship:2007,
    author = {Morgan, Stephen L. and Winship, Christopher},
    isbn = {0521829984},
    publisher = {Cambridge University Press},
    title = {Counterfactuals and Causal Inference:
        Methods and Principles for Social Research (Analytical Methods for Social Research)},
    year = {2007},
}

@Article{morokoff:caflisch:1994,
  author    = {Morokoff, William J. and Caflisch, Russel E. },
  year      = {1994},
  title     = {Quasi-Random Sequences and Their Discrepancies},
  journal   = {SIAM Journal on Scientific Computing},
  number    = {6},
  pages     = {1251--1279},
  publisher = {SIAM},
  comment   = {http://link.aip.org/link/?SCE/15/1251},
  volume    = {15}
}

@Book{morton:rolph:2000,
  editor    = {Sally C. Morton and John E. Rolph},
  year      = {2000},
  title     = {Public Policy and Statistics : Case Studies from RAND},
  publisher = {Springer},
  series    = {Statistics for Social Science and Behavorial Sciences},
  address   = {New York},
  isbn      = {0387987770},
}

@Article{moustaki:victoria:feser:2006,
  author    = {Moustaki, Irini and Victoria-Feser, Maria-Pia},
  year      = {2006},
  title     = {Bounded-Influence Robust Estimation in Generalized Linear
          Latent Variable Models},
  journal   = {Journal of the American Statistical Association},
  number    = {474},
  pages     = {644--653},
  volume    = {101}
}

@Manual{mplus:2004,
  author    = {Linda K. Muth{\'e}n and Bengt O. Muth{\'e}n},
  year      = {2004},
  title     = {M\textit{plus}: Statistical Analysis with Latent
          Variables. User's Guide},
  address   = {Los Angeles, CA},
  edition   = {3rd}
}

@Article{mroz:zayats:2008,
  author    = {Mroz, Thomas A. and Zayats, Yaraslau V. },
  year      = {2008},
  title     = {Arbitrarily Normalized Coefficients, Information Sets, and
          False Reports of ``Biases'' in Binary Outcome Models},
  journal   = {The Review of Economics and Statistics},
  number    = {3},
  pages     = {406--413},
  abstract  = {Empirical researchers sometimes misinterpret how
          additional regressors, heterogeneity corrections, and
          multilevel factors impact the interpretation of the
          estimated parameters in binary outcome models such as logit
          and probit. This can result in incorrect inferences about
          the importance of incorporating such features in these
          nonlinear statistical models. Some reports of biases in
          binary outcome models appear related to the arbitrary
          variance normalization required in binary outcome models. A
          focus on readily interpretable numerical quantities, rather
          than conveniently chosen ?effects? as measured by
          arbitrarily scaled coefficients, would eliminate nearly all
          of the interpretation problems we highlight in this
          paper.},
  volume    = {90}
}

@Article{mueller:1997,
  author    = {Mueller, Ralph O.},
  year      = {1997},
  title     = {Structural equation modeling: Back to basics.},
  journal   = {Structural Equation Modeling},
  number    = {4},
  pages     = {353--369},
  volume    = {4}
}

@Book{muirhead:2005,
  author    = {Robb J. Muirhead},
  year      = {2005},
  title     = {Aspects of Multivariate Statistical Theory},
  publisher = {Wiley-Interscience},
  series    = {Wiley Series in Probability and Statistics},
  address   = {New York},
  edition   = {2nd revised}
}

@book{mulaik:2009,
    author = {Mulaik, Stanley A.},
    isbn = {9781439800386},
    publisher = {Chapman and Hall/CRC},
    title = {Linear Causal Modeling with Structural Equations},
    address = {Boca Raton, Florida, USA},
    year = {2009}
}

@book{mulaik:2009:cfa,
    author = {Mulaik, Stanley A.},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
    title = {Foundations of Factor Analysis},
    series = {Statistics in the Social and Behavioral Sciences},
    year = {2009}
}

@Article{murphy:topel:1985,
  author    = {Murphy, Kevin M. and Topel, Robert H.},
  year      = {1985},
  title     = {Estimation and Inference in Two-Step Econometric Models},
  abstract  = {A commonly used procedure in a wide class of empirical
          applications is to impute unobserved regressors, such as
          expectations, from an auxiliary econometric model. This
          two-step (T-S) procedure fails to account for the fact that
          imputed regressors are measured with sampling error, so
          hypothesis tests based on the estimated covariance matrix
          of the second-step estimator are biased, even in large
          samples. We present a simple yet general method of
          calculating asymptotically correct standard errors in T-S
          models. The procedure may be applied even when joint
          estimation methods, such as full information maximum
          likelihood, are inappropriate or computationally
          infeasible. We present two examples from recent empirical
          literature in which these corrections have a major impact
          on hypothesis testing.},
  comment   = {(private-note)available at
          http://ideas.repec.org/a/bes/jnlbes/v3y1985i4p370-79.html},
  doi       = {doi:10.2307/1391724},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {4},
  pages     = {370--79},
  volume    = {3}
}

@Article{muth:1984,
  author    = {Muth{\'e}n, B. },
  year      = {1984},
  title     = {A general structural equation model with dichotomous,
          ordered categorical, and continuous latent variable
          indicators},
  journal   = {Psychometrika},
  pages     = {115--132},
  volume    = {49}
}

@Article{muthen:1991,
  author    = {Muth{\'e}n, Bengt O. },
  year      = {1991},
  title     = {Multilevel Factor Analysis of Class and Student
          Achievement Components},
  journal   = {Journal of Educational Measurement},
  number    = {4},
  pages     = {338--354},
  comment   = {http://dx.doi.org/10.1111/j.1745-3984.1991.tb00363.x},
  volume    = {28},
  abstract  = {This article analyzes mathematics achievement data from
          the Second International Mathematics Study (SIMS;
          Crosswhite, Dossey, Swafford, McKnight, \& Cooney, 1985) in
          which U.S. students are measured at the beginning and end
          of eighth grade. The aim of the article is to address some
          substantive analysis questions in the SIMS data and show
          the potential of multilevel factor analysis methodology.
          Issues related to between- and within-class decomposition
          of achievement variance and the change of this
          decomposition over the course of the eighth grade are
          studied. As a starting point, random effects ANOVA is
          considered for each achievement score. Each score contains
          a large amount of measurement error. The effects of
          unreliability on variance decomposition are shown with the
          help of a multilevel factor analysis model. Unreliability
          has severely distorting effects on this type of ANOVA while
          multilevel factor analysis gives results corresponding to
          what would be obtained with perfectly reliable scores.}
}

@Article{muthen:kaplan:1985,
  author    = {Muth{\'e}n, Bengt and Kaplan, David},
  year      = {1985},
  title     = {A comparison of some methodologies for the factor analysis
          of non-normal {L}ikert variables},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {38},
  number    = {2},
  pages     = {171--189},
  abstract  = {Considers the problem of applying factor analysis to
          nonnormal categorical variables. A Monte Carlo study was
          conducted where 5 prototypical cases of nonnormal variables
          were generated. Maximum-likelihood (ML) and generalized
          least-squares (GLS) normal theory estimators are compared
          to M. W. Browne's (1982) asymptotically distribution-free
          (ADF) estimator. A categorical variable methodology (CVM)
          estimator of B. Muthen (see record 1984-16817-001) is also
          considered for the most severely skewed case. Selected
          results show that ML and GLS chi-square tests are quite
          robust but obtain too large values for variables that are
          severely skewed and kurtotic but the ADF performs well. The
          CVM estimator appears to work well when applied to severely
          skewed variables that had been dichotomized. ML and GLS
          results for a kurtosis-only case showed no distortion of
          chi-square or parameter estimates and only a slight
          downward bias in estimated standard errors. }
}

@Article{muthen:kaplan:1992,
  author    = {Muth{\'e}n, Bengt and Kaplan, David},
  year      = {1992},
  title     = {A comparison of some methodologies for the factor analysis
          of non-normal {L}ikert variables: A note on the size of the
          model},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {45},
  number    = {1},
  pages     = {19--30},
  abstract  = {Normal theory generalized least squares (GLS) and a
          recently developed asymptotic distribution free (ADF)
          estimator are compared for 6 cases of non-normality, 2
          sample sizes, and 4 models of increasing size in a Monte
          Carlo framework with a large number of replications.
          Results show that GLS and ADF chi-square tests are
          increasingly sensitive to non-normality when the size of
          the model increases. No parameter estimate bias was
          observed for GLS and only slight parameter bias was found
          for ADF. A downward bias in estimated standard errors was
          found for GLS that remains constant across model size. For
          ADF, a downward bias in estimated standard errors was also
          found that became increasingly worse with the size of the
          model.}
}

@Article{muthen:satorra:1995,
  author    = {Bengt O. Muth{\'e}n and Albert Satorra},
  year      = {1995},
  title     = {Complex Sample Data in Structural Equation Modeling},
  journal   = {Sociological Methodology},
  volume    = {25},
  pages     = {267--316}
}

@ARTICLE{muthen:muthen:2002,
  AUTHOR =       {Linda Muth{\'e}n and Bengt Muth{\'e}n},
  TITLE =        {How to Use a {Monte} {Carlo} Study to Decide on
                  Sample Size and Determine Power},
  JOURNAL =      {Structural Equation Modeling},
  YEAR =         {2002},
  volume =       {9},
  number =       {4},
  pages =        {599--620},
}


@Article{narain:1951,
  author    = {Narain, R.D.},
  year      = {1951},
  title     = {On sampling without replacement with varying
          probabilities},
  journal   = {Journal of the Indian Society of Aricultural Statistics},
  volume    = {3},
  pages     = {169--174}
}

@Article{nash:1950,
  author    = {Nash, John F., Jr.},
  year      = {1950},
  title     = {The Bargaining Problem},
  journal   = {Econometrica},
  volume    = {18},
  number    = {2},
  pages     = {155--162},
  doi       = {10.2307/1907266}
}

@Article{neud:sato:1991,
  author    = {Heinz Neudecker and Albert Satorra},
  year      = {1991},
  title     = {Linear Structural Relations: Gradient and {H}essian of the
          Fitting Function},
  journal   = {Statistics and Probability Letters},
  volume    = {11},
  pages     = {57--61}
}

@article{nelson:powg:town:kovar:2003,
  title={A comparison of national estimates from the {N}ational {H}ealth
    {I}nterview {S}urvey and the {B}ehavioral {R}isk {F}actor {S}urveillance {S}ystem},
  author={Nelson, David E and Powell-Griner, Eve and Town, Machell and Kovar, Mary Grace},
  journal={American journal of public health},
  volume={93},
  number={8},
  pages={1335--1341},
  year={2003},
}


@TechReport{neukirch:2002,
  author    = {Neukirch, Thomas},
  year      = {2002},
  title     = {Nonignorable attrition and selectivity biases in the
          Finish subsample of the ECHP: An empirical study using
          additional register information},
  howpublished  = {working paper},
  institution   = {CHINTEX},
  number    = {5}
}

@Article{nevitt:hancock:2004,
  author    = {Nevitt, Jonathan and Hancock, Gregory R.},
  year      = {2004},
  title     = {Evaluating Small Sample Approaches for Model Test
          Statistics in Structural Equation Modeling},
  doi       = {10.1207/S15327906MBR3903\_3},
  journal   = {Multivariate Behavioral Research},
  number    = {3},
  pages     = {439--478},
  volume    = {39},
  abstract  = {Through Monte Carlo simulation, small sample methods for
          evaluating overall data-model fit in structural equation
          modeling were explored. Type I error behavior and power
          were examined using maximum likelihood (ML),
          Satorra-Bentler scaled and adjusted (SB; Satorra & Bentler,
          1988, 1994), residual-based (Browne, 1984), and
          asymptotically distribution free (ADF; Browne, 1982, 1984)
          test statistics. To accommodate small sample sizes the ML
          and SB statistics were adjusted using a k-factor correction
          (Bartlett, 1950); the residual-based and ADF statistics
          were corrected using modified $\chi^2$ and F statistics
          (Yuan & Bentler, 1998, 1999). Design characteristics
          include model type and complexity, ratio of sample size to
          number of estimated parameters, and distributional form.
          The k-factor-corrected SB scaled test statistic was
          especially stable at small sample sizes with both normal
          and nonnormal data. Methodologists are encouraged to
          investigate its behavior under a wider variety of models
          and distributional forms.}
}

@Article{newey1987specification,
  author    = {Newey, Whitney K.},
  year      = {1987},
  title     = {Specification tests for distributional assumptions in the
          Tobit model},
  abstract  = {The recent development of distribution robust estimators
          for the censored regression model allows for the
          construction of specification tests of the normality and
          homoskedasticity assumption of the Tobit model. This paper
          considers Hausman (1978) specification tests for the Tobit
          model that are based on Powell's (1986) symmetrically
          censored least squares estimator. Testing for models with
          explanatory variables that are endogeneous is considered,
          as well as the simpler case where the explanatory variables
          are exogenous. Testing of the maintained conditional
          symmetry assumption of the symmetrically censored least
          squares estimator is also considered. The tests are
          implemented in an empirical example of a model of female
          labor supply.},
  doi       = {10.1016/0304-4076(87)90070-4},
  journal   = {Journal of Econometrics},
  number    = {1-2},
  pages     = {125--145},
  volume    = {34}
}

@InCollection{newey:mcfadden:1994,
  author    = {Newey, W. and Mc{F}adden, D.},
  editor    = {R. Engle and D. Mc{F}adden},
  year      = {1994},
  title     = {Large sample Estimation and hypothesis testing},
  booktitle = {Handbook of econometrics},
  chapter   = {36},
  publisher = {North Holland},
  volume    = {{IV}},
  address   = {New York}
}

@InBook{newey:romalho:smith:2005,
  author    = {Newey, Whitney K. and Romalho, Joaquim J. S. and Smith,
          Richard J. },
  editor    = {Andrews, Donald W. K. and Stock, James H. },
  year      = {2005},
  title     = {Asymptotic Bias for {GMM} and {GEL} Estimators with
          Estimated Nuisance Parameters},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {11},
  comment   = {http://ideas.repec.org/p/ifs/cemmap/05-03.html},
  abstract  = {This papers studies and compares the asymptotic bias of
          GMM and generalized empirical likelihood (GEL) estimators
          in the presence of estimated nuisance parameters. We
          consider cases in which the nuisance parameter is estimated
          from independent and identical samples. A simulation
          experiment is conducted for covariance structure models.
          Empirical likelihood offers much reduced mean and median
          bias, root mean squared error and mean absolute error, as
          compared with two-step GMM and other GEL methods. Both
          analytical and bootstrap bias-adjusted two-step GMM
          estimators are compared. Analytical bias-adjustment appears
          to be a serious competitor to bootstrap methods in terms of
          finite sample bias, root mean squared error and mean
          absolute error. Finite sample variance seems to be little
          affected.}
}

@Article{newey:smith:2004,
  author    = {Whitney K. Newey and Smith, Richard J. },
  year      = {2004},
  title     = {Higher Order Properties of {GMM} and Generalized Empirical
          Likelihood Estimators},
  doi       = {10.1111/j.1468-0262.2004.00482.x},
  journal   = {Econometrica},
  number    = {1},
  pages     = {219--255},
  volume    = {72},
  abstract  = {In an effort to improve the small sample properties of
          generalized method of moments (GMM) estimators, a number of
          alternative estimators have been suggested. These include
          empirical likelihood (EL), continuous updating, and
          exponential tilting estimators. We show that these
          estimators share a common structure, being members of a
          class of generalized empirical likelihood (GEL) estimators.
          We use this structure to compare their higher order
          asymptotic properties. We find that GEL has no asymptotic
          bias due to correlation of the moment functions with their
          Jacobian, eliminating an important source of bias for GMM
          in models with endogeneity. We also find that EL has no
          asymptotic bias from estimating the optimal weight matrix,
          eliminating a further important source of bias for GMM in
          panel data models. We give bias corrected GMM and GEL
          estimators. We also show that bias corrected EL inherits
          the higher order property of maximum likelihood, that it is
          higher order asymptotically efficient relative to the other
          bias corrected estimators.}
}

@Article{newey:west:1987,
  author    = "Kenneth D West and Whitney K Newey",
  year      = 1987,
  title     = "A Simple, Positive Semi-Definite, Heteroskedasticity and
          Autocorrelation Consistent Covariance Matrix",
  journal   = "Econometrica",
  volume    = 55,
  number    = 3,
  pages     = "703--708"
}

@Article{newey:west:1987:gmm,
  author    = {Newey, Whitney K. and West, Kenneth D.},
  year      = {1987},
  title     = {Hypothesis Testing with Efficient Method of Moments
          Estimation},
  doi       = {10.2307/2526578},
  issn      = {00206598},
  journal   = {International Economic Review},
  number    = {3},
  pages     = {777--787},
  volume    = {28}
}

@MANUAL{nydh:2014:xbrfss,
  title =        {Expanded Behavioral Risk Factor Surveillance System},
  author =       {{New York State Department of Health}},
  address =      {Albany, NY},
  year =         {2014},
  note =         {Available at https://www.health.ny.gov/statistics/brfss/expanded/},
}


@Article{neym:1934,
  author    = "Jerzy Neyman",
  year      = "1934",
  title     = "On the Two Different Aspects of the Representative Method:
    The Method of Stratified Sampling and the Method of Purposive Selection",
  journal   = "Journal of the Royal Statistical Society",
  volume    = "109",
  pages     = "558--606"
}

@Article{neym:1938,
  author    = "Jerzy Neyman",
  year      = "1938",
  title     = "Contribution to the Theory of Sampling Human Populations",
  journal   = "The Journal of the American Statistical Association",
  volume    = "33",
  pages     = "101--116"
}

@Article{neyman:pearson:1928,
  author    = "J. Neyman and E. S. Pearson",
  year      = "1928",
  title     = "On the Use and Interpretation of Certain Test Criteria for
          Purposes of Statistical Inference",
  journal   = "Biometrika",
  volume    = "20A",
  pages     = "175--240, 263--294"
}

@Article{neyman:pearson:1933,
  author    = "J. Neyman and E. S. Pearson",
  year      = "1933",
  title     = "On the Problem of the Most Efficient Tests of Statistical
          Hypotheses",
  journal   = "Philosophical Transactions of the Royal Society",
  volume    = 231,
  pages     = "289--337"
}

@techreport{nhis:2000,
  AUTHOR =       {Botman, S.L. and Moore, T.F. and Moriarity, C.L. and Parsons, V.L.},
  TITLE =        {Design and estimation for the
            {N}ational {H}ealth {I}nterview {S}urvey, 1995?-2004},
  series =       {Vital Health Statistics},
  YEAR =         {2000},
  volume =       {2},
  number =       {130},
  institution =  {National Center for Health Statistics},
}

@Book{niederreiter:1992,
  author    = {Harald Niederreiter},
  year      = {1992},
  title     = {Random Number Generation and Quasi-{Monte Carlo} Methods},
  publisher = {Society for Industrial and Applied Mathematics},
  volume    = {63},
  series    = {CBMS-NSF Regional Conference Series in Applied
          Mathematics},
  address   = {Philadelphia},
  isbn      = {0898712955}
}

@Article{nigam:rao:1996,
  author    = {Nigam, A. K. and Rao, J. N. K. },
  year      = {1996},
  title     = {On balanced bootstrap for stratified multistage samples},
  journal   = {Statistica Sinica},
  number    = {1},
  pages     = {199--214},
  volume    = {6}
}

@Article{nuesch:1966,
  author    = "P. E. N{\"u}esch",
  year      = 1966,
  title     = "On the Problem of Testing Location in Multivariate
          Problems for Restricted Alternatives",
  journal   = "The Annals of Mathematical Statistics",
  volume    = 37,
  pages     = "113--119"
}

@ARTICLE{nygard:sandstrom:1989,
  AUTHOR =       {Fredrik Nyg{\r a}rd and Arne Sandstr{\"o}m},
  TITLE =        {Income Inequality Measures Based on Sample Surveys},
  JOURNAL =      {Journal of Econometrics},
  YEAR =         {1989},
  volume =       {42},
  pages =        {81--95},
}

@article{omuir:campa:1998,
    abstract =
        {One of the principal sources of error in data collected from
        structured face-to-face interviews is the interviewer. The other
        major component of imprecision in survey estimates is sampling
        variance. It is rare, however, to find studies in which the complex
        sampling variance and the complex interviewer variance are both
        computed. This paper compares the relative impact of interviewer
        effects and sample design effects on survey precision by making use
        of an interpenetrated primary sampling unit?interviewer experiment
        which was designed by the authors for implementation in the second
        wave of the British Household Panel Study as part of its scientific
        programme. It also illustrates the use of a multilevel (hierarchical)
        approach in which the interviewer and sample design effects are
        estimated simultaneously while being incorporated in a substantive
        model of interest.},
    author = {O'Muircheartaigh, Colm and Campanelli, Pamela},
    doi = {10.1111/1467-985x.00090},
    journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
    number = {1},
    pages = {63--77},
    title = {The relative impact of interviewer effects and sample design effects on survey precision},
    volume = {161},
    year = {1998}
}

@BOOK{odon:door:wags:lind:2008,
  AUTHOR =       {Owen {O'Donnell} and Eddy van Doorslaer and Adam Wagstaff and Magnus Lindelow},
  TITLE =        {Analyzing Health Equity Using Household Survey Data},
  PUBLISHER =    {The World Bank},
  YEAR =         {2008},
  series =       {{WBI} Development},
  address =      {Washington, DC},
  isbn =         {978-0-8213-6933-3},
}

@article{oertzen:2010,
    author = {{von Oertzen}, Timo},
    doi = {10.1348/000711009X441021},
    issn = {0007-1102},
    journal = {British Journal of Mathematical and Statistical Psychology},
    number = {2},
    pages = {257--272},
    publisher = {British Psychological Society},
    title = {Power equivalence in structural equation modelling},
    volume = {63},
    year = {2010},
    abstract =
       {Implementing large-scale empirical studies can be very
       expensive. Therefore, it is useful to optimize study designs
       without losing statistical power. In this paper, we show how
       study designs can be improved without changing statistical
       power by defining power equivalence, a relation between
       structural equation models (SEMs) that holds true if two SEMs
       have the same power on a likelihood ratio test to detect a
       given effect. We show systematic operations of SEMs that
       maintain power, and give an algorithm that efficiently
       reduces SEMs to power-equivalent models with a minimal number
       of observed parameters. In this way, optimal study designs
       can be found without reducing statistical power. Furthermore,
       the algorithm can be used to drastically increase the speed
       of power computations when using Monte Carlo simulations or
       approximation methods.},
}

@Article{ogasawara:2004,
  author    = {Ogasawara, H.},
  year      = {2004},
  title     = {Asymptotic robustness of the asymptotic biases in
          structural equation modeling},
  abstract  = {The asymptotic robustness of the normal theory asymptotic
          biases of the least-squares estimators of the parameters in
          covariance structures against the violation of normality is
          shown, which is obtained under the conditions required for
          the asymptotic robustness for the normal theory standard
          errors and the usual chi-square statistic. The asymptotic
          robustness holds not only for the estimators of the
          parameters whose normal theory asymptotic standard errors
          are asymptotically robust, but also for the non-robust
          ones. The Wishart maximum likelihood estimators are also
          shown to have the asymptotic robustness. A numerical
          illustration for the factor analysis model shows that the
          empirical biases of robust estimators under non-normality
          are close to their corresponding normal theory asymptotic
          biases.},
  doi       = {10.1016/j.csda.2004.06.002},
  issn      = {01679473},
  journal   = {Computational Statistics \& Data Analysis},
  pages     = {79959297+}
}

@Article{ogasawara:2007,
  author    = {Ogasawara and Haruhiko},
  year      = {2007},
  title     = {Asymptotic expansion of the distributions of the
          estimators in factor analysis under non-normality},
  doi       = {10.1348/000711006X110904},
  issn      = {0007-1102},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  number    = {2},
  pages     = {395--420},
  publisher = {British Psychological Society},
  volume    = {60}
}

@Article{olsson:1979,
  author    = "U. Olsson",
  year      = 1979,
  title     = "Maximum Likelihood Estimation of the Polychoric
          Correlation",
  journal   = "Psychometrika",
  volume    = 44,
  pages     = "443--460"
}

@Misc{orcmacro:dhs,
  author    = "{ORC Macro}",
  year      = "2005",
  title     = "Demographic and Health Surveys",
  note      = "http://www.measuredhs.com"
}

@Article{owen1990empirical,
  author    = {Owen, Art},
  year      = {1990},
  title     = {Empirical Likelihood Ratio Confidence Regions},
  abstract  = {An empirical likelihood ratio function is defined and used
          to obtain confidence regions for vector valued statistical
          functionals. The result is a nonparametric version of
          Wilks' theorem and a multivariate generalization of work by
          Owen. Cornish-Fisher expansions show that the empirical
          likelihood intervals for a one dimensional mean are less
          adversely affected by skewness than are those based on
          Student's t statistic. An effective method is presented for
          computing empirical profile likelihoods for the mean of a
          vector random variable. The method is a reduction by convex
          duality to an unconstrained minimization of a convex
          function on a low dimensional domain. Algorithms exist for
          finding the unique global minimum at a superlinear rate of
          convergence. A byproduct is a noncombinatorial algorithm
          for determining whether a given point lies within the
          convex hull of a finite set of points. The multivariate
          empirical likelihood regions are justified for functions of
          several means, such as variances, correlations and
          regression parameters and for statistics with linear
          estimating equations. An algorithm is given for computing
          profile empirical likelihoods for these statistics.},
  doi       = {10.2307/2241537},
  issn      = {00905364},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {90--120},
  publisher = {Institute of Mathematical Statistics},
  volume    = {18}
}

@Article{owen1991empirical,
  author    = {Owen, Art},
  year      = {1991},
  title     = {Empirical Likelihood for Linear Models},
  abstract  = {Empirical likelihood is a nonparametric method of
          inference. It has sampling properties similar to the
          bootstrap, but where the bootstrap uses resampling, it
          profiles a multinomial likelihood supported on the sample.
          Its properties in i.i.d. settings have been investigated in
          works by Owen, by Hall and by DiCiccio, Hall and Romano.
          This article extends the method to regression problems.
          Fixed and random regressors are considered, as are robust
          and heteroscedastic regressions. To make the extension,
          three variations on the original idea are considered. It is
          shown that when some functionals of the distribution of the
          data are known, one can get sharper inferences on other
          functionals by imposing the known values as constraints on
          the optimization. The result is first order equivalent to
          conditioning on a sample value of the known functional. The
          use of a Euclidean alternative to the likelihood function
          is investigated. A triangular array version of the
          empirical likelihood theorem is given. The one-way ANOVA
          and heteroscedastic regression models are considered in
          detail. An example is given in which inferences are drawn
          on the parameters of both the regression function and the
          conditional variance model.},
  doi       = {10.2307/2241901},
  issn      = {00905364},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {1725--1747},
  publisher = {Institute of Mathematical Statistics},
  volume    = {19}
}

@Article{owen1998latin,
  author    = {Owen, Art B.},
  year      = {1998},
  title     = {Latin supercube sampling for very high-dimensional
          simulations},
  address   = {New York, NY, USA},
  doi       = {10.1145/272991.273010},
  issn      = {1049-3301},
  journal   = {ACM Trans. Model. Comput. Simul.},
  number    = {1},
  pages     = {71--102},
  publisher = {ACM Press},
  volume    = {8}
}

@Article{owen:1988,
  author    = {Owen, Art B. },
  year      = {1988},
  title     = {Empirical likelihood ratio confidence intervals for a
          single functional},
  doi       = {10.1093/biomet/75.2.237},
  journal   = {Biometrika},
  number    = {2},
  pages     = {237--249},
  comment   = {http://dx.doi.org/10.1093/biomet/75.2.237},
  volume    = {75},
  abstract  = {The empirical distribution function based on a sample is
          well known to be the maximum likelihood estimate of the
          distribution from which the sample was taken. In this paper
          the likelihood function for distributions is used to define
          a likelihood ratio function for distributions. It is shown
          that this empirical likelihood ratio function can be used
          to construct confidence intervals for the sample mean, for
          a class of M-estimates that includes quantiles, and for
          differentiable statistical functionals. The results are
          nonpara-metric extensions of Wilks's (1938) theorem for
          parametric likelihood ratios. The intervals are illustrated
          on some real data and compared in a simulation to some
          bootstrap confidence intervals and to intervals based on
          Student's t statistic. A hybrid method that uses the
          bootstrap to determine critical values of the likelihood
          ratio is introduced. 10.1093/biomet/75.2.237}
}

@Article{owen:1990,
  author    = {Owen, Art B. },
  year      = {1990},
  title     = {Empirical Likelihood Ratio Confidence Regions},
  journal   = {The Annals of Statistics},
  volume    = {18},
  number    = {1},
  pages     = {90--120}
}

@Article{owen:1991,
  author    = {Owen, Art B. },
  year      = {1991},
  title     = {Empirical Likelihood for Linear Models},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {1725--1747},
  comment   = {http://www.jstor.org/stable/2241901},
  volume    = {19},
  abstract  = {Empirical likelihood is a nonparametric method of
          inference. It has sampling properties similar to the
          bootstrap, but where the bootstrap uses resampling, it
          profiles a multinomial likelihood supported on the sample.
          Its properties in i.i.d. settings have been investigated in
          works by Owen, by Hall and by DiCiccio, Hall and Romano.
          This article extends the method to regression problems.
          Fixed and random regressors are considered, as are robust
          and heteroscedastic regressions. To make the extension,
          three variations on the original idea are considered. It is
          shown that when some functionals of the distribution of the
          data are known, one can get sharper inferences on other
          functionals by imposing the known values as constraints on
          the optimization. The result is first order equivalent to
          conditioning on a sample value of the known functional. The
          use of a Euclidean alternative to the likelihood function
          is investigated. A triangular array version of the
          empirical likelihood theorem is given. The one-way ANOVA
          and heteroscedastic regression models are considered in
          detail. An example is given in which inferences are drawn
          on the parameters of both the regression function and the
          conditional variance model.}
}

@InProceedings{owen:1995,
  author    = {A. B. Owen},
  editor    = {Harald Niederreiter and Peter Jau-Shyong Shiue},
  year      = 1995,
  title     = {Randomly Permuted $(t,m,s)$-Nets and $(t,s)$-Sequences},
  booktitle = {{Monte} {Carlo} and Quasi-{Monte} {Carlo} Methods in Scientific
          Computing},
  pages     = {299--317},
  publisher = {Springer-Verlag},
  address   = {New York}
}

@Article{owen:1998:lss,
  author    = {Art B. Owen},
  year      = {1998},
  title     = {Latin Supercube Sampling for Very High-Dimensional
          Simulations},
  journal   = {ACM Transactions on Modeling and Computer Simulation},
  volume    = {8},
  number    = {1},
  pages     = {71--102}
}

@InProceedings{owen:1998:mcqmc,
  author    = {Art B. Owen},
  year      = {1998},
  title     = {{Monte Carlo} extension of quasi-{Monte Carlo}},
  booktitle = {{WSC} '98: Proceedings of the 30th conference on Winter
          simulation},
  isbn      = {0-7803-5134-7},
  pages     = {571--578},
  location  = {Washington, D.C., United States},
  publisher = {IEEE Computer Society Press},
  address   = {Los Alamitos, CA, USA}
}

@Article{owen:1998:scramble,
  author    = {Art B. Owen},
  year      = {1998},
  title     = {Scrambling {Sobol'} and {N}iederreiter-{X}ing points},
  journal   = {Journal of Complexity},
  volume    = {14},
  pages     = {466--489}
}

@Book{owen:2001,
  author    = {Owen, Art B. },
  year      = {2001},
  title     = {Empirical Likelihood},
  isbn      = {1584880716},
  publisher = {Chapman \& Hall/CRC},
  abstract  = {Empirical likelihood provides inferences whose validity
          does not depend on specifying a parametric model for the
          data. Because it uses a likelihood, the method has certain
          inherent advantages over resampling methods: it uses the
          data to determine the shape of the confidence regions, and
          it makes it easy to combined data from multiple sources. It
          also facilitates incorporating side information, and it
          simplifies accounting for censored, truncated, or biased
          sampling. One of the first books published on the subject,
          Empirical Likelihood offers an in-depth treatment of this
          method for constructing confidence regions and testing
          hypotheses. The author applies empirical likelihood to a
          range of problems, from those as simple as setting a
          confidence region for a univariate mean under IID sampling,
          to problems defined through smooth functions of means,
          regression models, generalized linear models, estimating
          equations, or kernel smooths, and to sampling with
          non-identically distributed data. Abundant figures offer
          visual reinforcement of the concepts and techniques.
          Examples from a variety of disciplines and detailed
          descriptions of algorithms-also posted on a companion Web
          site at- illustrate the methods in practice. Exercises help
          readers to understand and apply the methods. The method of
          empirical likelihood is now attracting serious attention
          from researchers in econometrics and biostatistics, as well
          as from statisticians. This book is your opportunity to
          explore its foundations, its advantages, and its
          application to a myriad of practical problems.}
}

@TechReport{owen:qse:2005,
  author    = {Art B. Owen},
  year      = {2005},
  title     = {On the {Warnock}-{Halton} quasi-standard error},
  institution   = {Stanford University},
  type      = {technical report},
  note      = {http://www-stat.stanford.edu/~owen/reports/qse.pdf},
  abstract  = {This paper investigates an error estimate proposed
          byWarnock and studied by Halton (2005). That error estimate
          is simply the sample standard error applied to certain
          non-randomized quasi-Monte Carlo points. This
          quasi-standard error (QSE) closely tracks the actual error
          in an example, and looks to be at least as accurate as a
          standard error based on random replication. We also show
          that the quasi-standard error is not unreasonably large in
          its intended use. But there are QMC constructions for which
          the QSE severely underestimates the true error. Moreover,
          discrepancy considerations do not separate these
          counterexamples from other cases where the method might be
          reliable. We conclude that the QSE is not yet ready to be
          trusted in applications.}
}

@InProceedings{owen:siggraph2003,
  author    = {A. B. Owen},
  editor    = {H. W. Jensen},
  year      = 2003,
  title     = {{Quasi-Monte Carlo} Sampling},
  booktitle = {{Monte Carlo} Ray Tracing: Siggraph 2003 Course 44},
  publisher = {SIGGRAPH},
  pages     = {69--88}
}

@Article{owen:tribble:2005,
  author    = {Art B. Owen and Seth D. Tribble},
  year      = {2005},
  title     = {A quasi-{Monte Carlo Metropolis} algorithm},
  journal   = {Proceedings of the National Academy of Sciences of the
          USA},
  volume    = {102},
  pages     = {8844--8849},
  note      = {doi:10.1073/pnas.0409596102}
}

@InProceedings{owen:variation:2004,
  author    = {A. B. Owen},
  editor    = {Jianqing Fan and Gang Li},
  year      = 2005,
  title     = {Multidimensional variation for quasi-{Monte Carlo}},
  booktitle = {International Conference on Statistics in honour of
          Professor Kai-Tai Fang's 65th birthday},
  pages     = {49--74}
}

@ARTICLE{pacifico:2014,
  author =       {Daniele Pacifico},
  title =        {sreweight: A Stata command to reweight
survey data to external totals},
  journal =      {The Stata Journal},
  year =         {2014},
  volume =       {14},
  number =       {1},
  pages =        {4--21},
}


@book{pagano:gauvreau:2000,
    author = {Pagano, Marcello and Gauvreau, Kimberlee},
    edition = {2},
    howpublished = {Hardcover},
    isbn = {0534229026},
    publisher = {Cengage Learning},
    title = {Principles of Biostatistics (with {CD}-{ROM})},
    year = {2000}
}
@Article{park:1983,
  author    = {R.N. Parker},
  year      = {1983},
  title     = {Measuring social participation},
  journal   = {American Sociological Review},
  volume    = {48},
  pages     = {864--873}
}

@Article{park:wang:wu:2002,
  author    = "Park, Albert and Wang, Sangui and Wu, Guobao",
  year      = 2002,
  title     = "Regional poverty targeting in {C}hina",
  journal   = "Journal of Public Economics",
  volume    = {86},
  number    = {1},
  pages     = {123-153}
}

@article{park:gelman:bafumi:2004,
    abstract =
        {We fit a multilevel logistic regression model for the mean of a
        binary response variable conditional on poststratification cells.
        This approach combines the modeling approach often used in small-area
        estimation with the population information used in poststratification
        (see Gelman and Little 1997, Survey Methodology 23:127?135). To
        validate the method, we apply it to {U.S}. preelection polls for 1988
        and 1992, poststratified by state, region, and the usual demographic
        variables. We evaluate the model by comparing it to state-level
        election outcomes. The multilevel model outperforms more commonly
        used models in political science. We envision the most important
        usage of this method to be not forecasting elections but estimating
        public opinion on a variety of issues at the state level.},
    author = {Park, David K. and Gelman, Andrew and Bafumi, Joseph},
    doi = {10.1093/pan/mph024},
    journal = {Political Analysis},
    number = {4},
    pages = {375--385},
    title = {Bayesian Multilevel Estimation with Poststratification:
        State-Level Estimates from National Polls},
    volume = {12},
    year = {2004}
}

@article{parker:etal:2020:rands12,
  author    = "Parker, Jennifer and Miller, Kristen and He, Yulei and Scanlon, Paul and Cai, Bill and 
                Shin, Hee-Choon and Parsons, Van and Irimata, Katherine",
  title     = "Overview and initial results of the National Center for Health Statistics’ 
                Research and Development Survey",
  journal   = "Statistical Journal of the IAOS", 
  volume    = 36, 
  issue     = 4, 
  pages     = "1199--1211", 
  year      = 2020
}

@Book{parlett:eigen,
  author    = "B. Parlett",
  year      = 1980,
  title     = "The Symmetric Eigenvalue Problem",
  publisher = "Prentice Hall",
  address   = "Englewood Cliffs, NJ"
}


@article{parzen:1962,
     title = {On Estimation of a Probability Density Function and Mode},
     author = {Parzen, Emanuel},
     journal = {The Annals of Mathematical Statistics},
     volume = {33},
     number = {3},
     pages = {1065--1076},
     year = {1962},
}


@InProceedings{pathak:1988,
  author    = "P. K. Pathak",
  editor    = "P. R. Krishnaiah and C. R. Rao",
  year      = "1988",
  title     = "Simple Random Sampling",
  booktitle = "Handbook of Statistics",
  volume    = "6",
  pages     = "97--109",
  publisher = "Elsevier Science Publishers"
}

@Article{patt:1950,
  author    = "H. D. Patterson",
  year      = "1950",
  title     = "Sampling on Successive Occasions with Partial Replacement
          of Units",
  journal   = "Journal of the Royal Statistical Society, Series B",
  volume    = "12",
  number    = "2",
  pages     = "241-255"
}

@Article{pattersonlatent,
  author    = {Patterson, B. H.},
  title     = {Latent Class Analysis of Complex Sample Survey Data:
          Application to Dietary Data},
  abstract  = {High fruit and vegetable intake is associated with
          decreased cancer risk. However, dietary recall data from
          national surveys suggest that, on any given day, intake
          falls below the recommended minima of three daily servings
          of vegetables and two daily servings of fruit. There is no
          single widely accepted measure of \&quot;usual\&quot;
          intake. One approach is to regard the distribution of
          intake as a mixture of \&quot;regular\&quot; (relatively
          frequent) and \&quot;nonregular\&quot; (relatively
          infrequent) consumers, using an indicator of whether an
          individual consumed the food of interest on the recall day.
          We use a new approach to summarizing dietary data, latent
          class analysis (LCA), to estimate \&quot;usual\&quot;
          intake of vegetables. The data consist of four 24-hour
          dietary recalls from the 1985 Continuing Survey of Intakes
          by Individuals collected from 1,028 women. Traditional LCA
          based on simple random sampling was extended to complex
          survey data by introducing sample weights into the latent
          class estimation algorithm and by accounting for the
          complex sample design through the use of jackknife standard
          errors. A two-class model showed that 18\&\#037; do not
          regularly consume vegetables, compared to an unweighted
          estimate of 33\&\#037;. Simulations showed that ignoring
          sample weights resulted in biased parameter estimates and
          that jackknife variances were slightly conservative but
          provided satisfactory confidence interval coverage. Using a
          survey-wide estimate of the design effect for variance
          estimation is not accurate for LCA. The methods proposed in
          this article are readily implemented for the analysis of
          complex sample survey data.},
  comment   = {Useful simulation design!},
  pages     = {721--741}
}

@book{pearl:2009:2nd,
    author = {Pearl, Judea},
    howpublished = {Hardcover},
    isbn = {052189560X},
    publisher = {Cambridge University Press},
    title = {Causality: Models, Reasoning, and Inference},
    year = {2009},
    edition = {2nd}
}

@TECHREPORT{pearl:bollen:2012,
  AUTHOR =       {Judea Pearl and Kenneth A Bollen},
  TITLE =        {Eight Myths about Causality and Structural Equation Models},
  INSTITUTION =  {UCLA},
  YEAR =         {2012},
  type =         {Technical report},
  number =       {R-393},
}


@Article{pearson:1901:pca,
  author    = "Karl Pearson",
  year      = "1901",
  title     = "On lines and planes of closest fit to systems of points in
          space",
  journal   = "Philosophical Magazine",
  volume    = 2,
  pages     = "559--572"
}

@Article{pearson:1901:tetrachoric,
  author    = "Karl Pearson",
  year      = "1901",
  title     = "Mathematical Contributions to the Theory of Evolution.
          VII. On the Correlation of Characters Not Qualitatively
          Measurable",
  journal   = "Philosophical Transactions of the Royal Society of London,
          Series A",
  volume    = 195,
  pages     = "1--47"
}

@Article{pearson:pearson:1922,
  author    = "Karl Pearson and E. S. Pearson",
  year      = 1922,
  title     = "On polychoric coefficients of correlation",
  journal   = "Biometrika",
  volume    = 14,
  pages     = "127--156"
}

@Book{pedh:1997,
  author    = "Pedhazur, E. J.",
  year      = "1997",
  title     = "Multiple Regression in Behavioral Research: Explanation
          and Prediction",
  edition   = "3rd",
  address   = "New York",
  publisher = "Holt, Rinehart and Winston"
}

@Article{penzer:shea:1997,
  author    = "Jeremy Penzer and Brian Shea",
  year      = 1997,
  title     = "The Exact Likelihood of an Autoregressive-Moving Average
          Model with Incomplete Data",
  journal   = "Biometrika",
  volume    = 84,
  number    = 4,
  pages     = "919--928"
}

@Article{perl:1969,
  author    = "Michael D. Perlman",
  year      = 1969,
  title     = "One-Sided Testing Problems in Multivariate Analysis",
  journal   = "The Annals of Mathematical Statistics",
  volume    = 40,
  number    = 2,
  pages     = "549--567"
}

@ARTICLE{pesaran:deaton:1978,
  AUTHOR =       {M. Hashem Pesaran and Agnus S. Deaton},
  TITLE =        {Testing Non-Nested Nonlinear Regression Models},
  JOURNAL =      {Econometrica},
  YEAR =         {1978},
  volume =       {46},
  number =       {3},
  pages =        {677--694},
}


@article{petersen:2009,
  title={Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches},
  author={Petersen, Mitchell A.},
  journal={Review of Financial Studies},
  volume={22},
  number={1},
  pages={435--480},
  year={2009}
}


@Article{petr:2001,
  author    = "George Petrakos",
  year      = 2001,
  title     = "Patterns of Regional Inequality in Transition Economies",
  journal   = "European Planning Studies",
  volume    = 9,
  number    = 3,
  pages     = "359--383"
}

@TECHREPORT{pew:2012,
  AUTHOR =       {{Pew Research Center}},
  TITLE =        {Assessing the Representativeness of Public Opinion Surveys},
  INSTITUTION =  {Pew Research Center for People and Press},
  YEAR =         {2012},
  note =         {Available at http://www.people-press.org/files/legacy-pdf/Assessing the Representativeness of Public Opinion Surveys.pdf},
}

@TECHREPORT{pew:2015:build:atp,
  author =       {{Pew Research Center}},
  title =        {Building {Pew} {R}esearch {C}enter?s {A}merican {T}rends {P}anel},
  year =         {2015},
  address =      {Washington, DC},
  note =         {available from http://pewrsr.ch/1Jo4nKE},
}

@TECHREPORT{pew:2015:mode,
  author =       {{Pew Research Center}},
  title =        {From Telephone to the Web: The Challenge of Mode of Interview
    Effects in Public Opinion Polls},
  year =         {2015},
  address =      {Washington, DC},
  note =         {available from http://pewrsr.ch/1cT0q6r},
}


@Article{pfef:skin:holm:gold:rasb:1998,
  author    = {D. {Pfeff}ermann and C. J. Skinner and D. J. Holmes and H.
          Goldstein and J. Rasbash},
  year      = {1998},
  title     = {Weighting for unequal selection probabilities in
          multilevel models},
  journal   = {Journal of Royal Statistical Society, Series B},
  volume    = {60},
  number    = {1},
  pages     = {23--40}
}

@Article{pfeff:1993,
  author    = {Pfeffermann, D.},
  year      = {1993},
  title     = {The role of sampling weights when modeling survey data},
  journal   = {International Statistical Review},
  volume    = {61},
  pages     = {317--337}
}

@InCollection{pfeff:lavange:1989,
  author    = {Danny Pfeffermann and Lisa La{V}ange},
  editor    = {C.J. Skinner and D. Holt, and T.M.F. Smith},
  year      = {1989},
  title     = {Regression Models for Stratified Multistage Cluster
          Samples},
  booktitle = {Analysis of Complex Surveys},
  publisher = {John Wiley and Sons},
  chapter   = {12},
  pages     = {237--260},
  address   = {Chichester}
}

@Article{pfeff:smith:1985,
  author    = {Pfeffermann, D. and Smith, T. M. F. },
  year      = {1985},
  title     = {Regression Models for Grouped Populations in Cross-Section
          Surveys},
  journal   = {International Statistical Review},
  number    = {1},
  pages     = {37--59},
  comment   = {http://links.jstor.org/sici?sici=0306-7734%28198504%2953%3A1%3C37%3ARMFGPI%3E2.0.CO%3B2-W}
          ,
  volume    = {53}
}

@article{pfeff:sverchkov:1999,
 abstract =
    {This paper proposes two new classes of estimators for regression models
    fitted to survey data. The proposed estimators account for the effect of
    nonignorable sampling schemes which are known to bias standard
    estimators. Both classes derive from relationships between the population
    distribution and the sample distribution of the sample measurements. The
    first class consists of parametric estimators. These are obtained by
    extracting the sample distribution as a function of the population
    distribution and the sample selection probabilities and applying maximum
    likelihood theory to this distribution. The second class consists of
    semi-parametric estimators, obtained by utilizing existing relationships
    between moments of the two distributions. New tests for sampling
    ignorability based on these relationships are developed. The proposed
    estimators and other estimators in common use are applied to real data
    and further compared in a simulation study. The simulations enable also
    to study the performance of the sampling ignorability tests and bootstrap
    variance estimators.},
 author = {Danny Pfeffermann and Michail Sverchkov},
 journal = {Sankhya: The Indian Journal of Statistics, Series B},
 number = {1},
 pages = {166-186},
 publisher = {Springer},
 title = {Parametric and Semi-Parametric Estimation of Regression Models Fitted to Survey Data},
 volume = {61},
 year = {1999}
}

@article{pfeff:sverchkov:2007,
    author = {Danny Pfeffermann and Michail Sverchkov},
    title = {Small-Area Estimation Under Informative Probability Sampling of Areas and Within the Selected Areas},
    journal = {Journal of the American Statistical Association},
    volume = {102},
    number = {480},
    pages = {1427-1439},
    year = {2007},
    doi = {10.1198/016214507000001094},
    abstract =
        {In this article we show how to predict small-area means and obtain
        valid mean squared error estimators and confidence intervals when the
        areas represented in the sample are sampled with unequal
        probabilities possibly related to the true (unknown) area means and
        the sampling of units within the selected areas is with probabilities
        possibly related to the outcome values. Ignoring the effects of the
        sampling process on the distribution of the observed outcomes in such
        cases may bias the inference very severely. Classical design-based
        inference that uses the randomization distribution of
        probability-weighted estimators cannot be applied for predicting the
        means of nonsampled areas. We propose simple test statistics for
        testing the informativeness of the selection of areas and sampling of
        units within the selected areas. We illustrate the proposed
        procedures by a simulation study and a real application of estimating
        mean body mass index in U.S. counties, using data from the Third
        National Health and Nutrition Examination Survey. }
}


@book{pfeffermann:rao:2009,
    isbn = {9780444534385},
    publisher = {North Holland},
    title = {Handbook of Statistics, Volume 29: Sample Surveys},
    year = {2009},
    editor = {Danny Pfeffermann and C. R. Rao}
}

@book{pfeffermann:rao:2009:v29a,
    isbn = {9780444534385},
    publisher = {North Holland},
    title = {Handbook of Statistics, Volume 29A: Sample Surveys: Design, Methods and Applications},
    year = {2009},
    editor = {Danny Pfeffermann and C. R. Rao}
}

@book{pfeffermann:rao:2009:v29b,
    isbn = {9780444534385},
    publisher = {North Holland},
    title = {Handbook of Statistics, Volume 29B: Sample Surveys: Inference and Analysis},
    year = {2009},
    editor = {Danny Pfeffermann and C. R. Rao}
}

@article{pfeffermann:2002,
    abstract = {The purpose of this paper is to provide a critical review of the main advances in small area estimation ({SAE}) methods in recent years. We also discuss some of the earlier developments, which serve as a necessary background for the new studies. The review focuses on model dependent methods with special emphasis on point prediction of the target area quantities, and mean square error assessments. The new models considered are models used for discrete measurements, time series models and models that arise under informative sampling. The possible gains from modeling the correlations among small area random effects used to represent the unexplained variation of the small area target quantities are examined. For review and appraisal of the earlier methods used for {SAE}, see Ghosh \& Rao (1994).},
    author = {Pfeffermann, Danny},
    doi = {10.1111/j.1751-5823.2002.tb00352.x},
    journal = {International Statistical Review},
    number = {1},
    pages = {125--143},
    title = {Small Area Estimation---New Developments and Directions},
    volume = {70},
    year = {2002}
}

@article{pfeffermann:2011:whymodel,
    author = {Pfeffermann, Danny},
    journal = {Survey Methodology},
    number = {2},
    pages = {115--136},
    title = {Modelling of complex survey data: {Why} model? {Why} is it a problem? {How} can we approach it?},
    volume = {37},
    year = {2011}
}

@article{pfeffermann:2013:sae,
    abstract = {The problem of small area estimation ({SAE}) is how to produce reliable
estimates of characteristics of interest such as means, counts, quantiles,
etc., for areas or domains for which only small samples or no samples are
available, and how to assess their precision. The purpose of this paper is to
review and discuss some of the new important developments in small area
estimation methods. Rao [Small Area Estimation (2003)] wrote a very
comprehensive book, which covers all the main developments in this topic until
that time. A few review papers have been written after 2003, but they are
limited in scope. Hence, the focus of this review is on new developments in the
last 7-8 years, but to make the review more self-contained, I also mention
shortly some of the older developments. The review covers both design-based and
model-dependent methods, with the latter methods further classified into
frequentist and Bayesian methods. The style of the paper is similar to the
style of my previous review on {SAE} published in 2002, explaining the new
problems investigated and describing the proposed solutions, but without
dwelling on theoretical details, which can be found in the original articles. I
hope that this paper will be useful both to researchers who like to learn more
on the research carried out in {SAE} and to practitioners who might be interested
in the application of the new methods.},
    author = {Pfeffermann, Danny},
    journal = {Statistical Science},
    number = {1},
    pages = {40--68},
    title = {New Important Developments in Small Area Estimation},
    volume = {28},
    year = {2013}
}

@Article{phil:park:1988,
  author    = {Phillips, P. C. B. and Park, J. Y.},
  year      = {1988},
  title     = {On the formulation of {Wald} tests of nonlinear
          restrictions},
  journal   = {Econometrica},
  volume    = {56},
  pages     = {1065?-1083}
}

@TechReport{phillips:2004,
  author    = {Phillips, Owen },
  year      = {2004},
  title     = {Using bootstrap weights with {WesVar} and {SUDAAN}},
  comment   = {http://www.statcan.ca/english/freepub/12-002-XIE/2004002/pdf/phillips.pdf}
          ,
  institution   = {Statistics Canada},
  journal   = {The Research Data Centres Information and Technical
          Bulletin},
  number    = {2},
  pages     = {6--15},
  volume    = {1},
  abstract  = {For the purpose of design-based variance estimation, a
          number of Statistics Canada surveys supply bootstrap
          weights with their microdata. While the use of bootstrap
          weights is not explicitly supported by commercially
          available software such as SUDAAN and WesVar, by taking
          advantage of similarities between a commonly used bootstrap
          technique and the method of Balanced Repeated Replication
          (BRR), these software can be used to produce bootstrap
          variance estimates. This article examines the reasoning
          behind this, and shows, by way of example, how this might
          be accomplished. The paper concludes with a brief
          discussion of other design-based approaches to variance
          estimation as well as software, programs and procedures
          where these methods have been employed.}
}

@Article{pickery2001effects,
  author    = {Pickery, Jan and Loosveldt, Geert and Carton, Ann},
  year      = {2001},
  title     = {The Effects of Interviewer and Respondent Characteristics
          on Response Behavior in Panel Surveys: A Multilevel
          Approach},
  abstract  = {Unit nonresponse in the second wave of a panel survey is
          related to several respondent characteristics and to the
          interviewer of that wave. More striking is the effect of
          the interviewer of the first wave, who is not involved in
          the second interview. To analyze both interviewer effects
          simultaneously, the authors use a multilevel
          cross-classified model. In that analysis, the effect of the
          interviewer of the second wave almost disappears. That
          effect turns out to be at least partly spurious due to a
          correlation of both interviewer effects. The authors
          conclude that the interviewer of the first interview is
          very important regarding participation in the subsequent
          waves of a panel survey. 10.1177/0049124101029004004},
  doi       = {10.1177/0049124101029004004},
  journal   = {Sociological Methods Research},
  number    = {4},
  pages     = {509--523},
  volume    = {29}
}

@Article{pickery:loosv:carton:2001,
  author    = {Pickery, Jan and Loosveldt, Geert and Carton, Ann },
  year      = {2001},
  title     = {The Effects of Interviewer and Respondent Characteristics
          on Response Behavior in Panel Surveys: A Multilevel
          Approach},
  doi       = {10.1177/0049124101029004004},
  journal   = {Sociological Methods Research},
  number    = {4},
  pages     = {509--523},
  volume    = {29}
}

@Article{pinh:liu:wu:2001,
  author    = {Pinheiro, Jos\'e C. and Liu, Chuanhai and Wu, Ying Nian},
  year      = 2001,
  title     = {Efficient Algorithms for Robust Estimation in Linear
          Mixed-effects Models Using the Multivariate $t$
          Distribution},
  journal   = {Journal of Computational and Graphical Statistics},
  volume    = 10,
  number    = 2,
  pages     = {249--276}
}

@Article{pitt1999filtering,
  author    = {Pitt, Michael K. and Shephard, Neil},
  year      = {1999},
  title     = {Filtering via Simulation: Auxiliary Particle Filters},
  abstract  = {This article analyses the recently suggested particle
          approach to filtering time series. We suggest that the
          algorithm is not robust to outliers for two reasons: the
          design of the simulators and the use of the discrete
          support to represent the sequentially updating prior
          distribution. Here we tackle the first of these problems.},
  journal   = {Journal of the American Statistical Association},
  number    = {446},
  pages     = {590--599},
  volume    = {94}
}

@Article{pitt:roze:gibb:1993,
  author    = "M. Pitt and M. Rozenzweig and D. Gibbons",
  year      = "1993",
  title     = "The Determinants and Consequences of the Placement of
          Government Programs in {I}ndonesia",
  journal   = "World Bank Economic Review",
  volume    = "7",
  number    = "3",
  pages     = "319--348"
}

@Book{polak:1971,
  author    = "Elijah Polak",
  year      = 1971,
  title     = "Computational Methods in Optimization: A Unified Approach",
  publisher = "Academic Press",
  address   = "New York"
}

@Book{polak:1997,
  author    = "Elijah Polak",
  year      = 1997,
  title     = "Optimization: Algorithms and Consistent Approximations",
  publisher = "Springer",
  address   = "New York",
  series    = "Applied Mathematical Sciences",
  volume    = 124
}

@Article{politis:romano:1994,
  author    = {Politis, Dimitris N. and Romano, Joseph P. },
  year      = {1994},
  title     = {Large Sample Confidence Regions Based on Subsamples under
          Minimal Assumptions},
  doi       = {10.2307/2242497},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {2031--2050},
  volume    = {22},
  abstract  = {In this article, the construction of confidence regions by
          approximating the sampling distribution of some statistic
          is studied. The true sampling distribution is estimated by
          an appropriate normalization of the values of the statistic
          computed over subsamples of the data. In the i.i.d.
          context, the method has been studied by Wu in regular
          situations where the statistic is asymptotically normal.
          The goal of the present work is to prove the method yields
          asymptotically valid confidence regions under minimal
          conditions. Essentially, all that is required is that the
          statistic, suitably normalized, possesses a limit
          distribution under the true model. Unlike the bootstrap,
          the convergence to the limit distribution need not be
          uniform in any sense. The method is readily adapted to
          parameters of stationary time series or, more generally,
          homogeneous random fields. For example, an immediate
          application is the construction of a confidence interval
          for the spectral density function of a homogeneous random
          field.}
}

@Book{politis:romano:wolf:1999,
  author    = {Politis, Dimitris N. and Romano, Joseph P. and Wolf,
          Michael },
  year      = {1999},
  title     = {Subsampling},
  isbn      = {0387988548},
  publisher = {Springer},
  series    = {Springer Series in Statistics},
  address   = {New York},
  abstract  = {Since Efron's profound paper on the bootstrap, an enormous
          amount of effort has been spent on the development of
          bootstrap, jacknife, and other resampling methods. The
          primary goal of these computer-intensive methods has been
          to provide statistical tools that work in complex
          situations without imposing unrealistic or unverifiable
          assumptions about the data generating mechanism. The
          primary goal of this book is to lay some of the foundation
          for subsampling methodology and related methods.}
}

@Article{poon:lew:poon:2000,
  author    = {Poon, W. Y. and Lew, S. F. and Poon, Y. S.},
  year      = {2000},
  title     = {A local influence approach to identifying multiple
          multivariate outliers},
  abstract  = {We make use of Cook's local influence approach and its
          recent modification by Poon and Poon to develop measures
          for detecting multivariate outliers. The motivation and the
          foundation of the theory are geometrical and are different
          from classical approaches; however, whilst the proposed
          measure exhibits a form similar to those in the literature,
          it still has a considerable advantage in having transformed
          the classical measures to the unit interval. The new
          approach unifies outlier identification measures using
          geometrical concepts. It involves no distributional
          assumption or large-sample properties, and allows the
          flexibility of identifying outliers with respect to
          different metrics. The approach therefore provides a valid
          reason for using the various measures in complicated
          situations, such as in non-normal cases and in small-sample
          problems.},
  issn      = {0007-1102},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  pages     = {255--273},
  publisher = {British Psychological Society}
}

@article{pothoff:woodbury:manton:1992,
    abstract =
        {A number of procedures have been proposed to attack different
        inference problems for data drawn from a survey with a complex sample
        design (i.e., a design that entails unequal weighting). Most
        procedures either are based on finite-population assumptions or
        require the specification of an explicit model using a
        superpopulation rationale. Herein we propose some relatively simple
        approximate procedures that are based on a superpopulation model.
        They provide valid variance estimators, test statistics, and
        confidence intervals that allow for sample design effects as
        expressed by design weights and other weights. The procedures do not
        rely on conditioning on model elements such as covariates to adjust
        for design effects. Instead, we obtain estimators by rescaling sample
        weights to sum to the equivalent sample size (equal to sample size
        divided by design effect). Using weighted estimators for
        superpopulation models, we obtain approximations to confidence bounds
        on the mean for simple sampling situations as well as for cluster
        sampling, post-stratification, and stratified sampling. We also
        obtain approximate tests of hypotheses for one-way analysis of
        variance and k ? 2 homogeneity testing. For all of these, further
        refinements based on the concept of equivalent degrees of freedom are
        provided. Additionally, a general method for determining and using
        poststratification weights is described and illustrated. The
        procedures in this article are better justified than the common
        expedient of making proportional adjustments so that the weights add
        to the sample size.},
    author = {Potthoff, Richard F. and Woodbury, Max A. and Manton, Kenneth G.},
    day = {1},
    doi = {10.1080/01621459.1992.10475218},
    journal = {Journal of the American Statistical Association},
    number = {418},
    pages = {383--396},
    title = {Equivalent Sample Size and Equivalent Degrees of Freedom Refinements
        for Inference Using Survey Weights under Superpopulation Models},
    volume = {87},
    year = {1992}
}

@Article{pott:1993,
  author    = "Potthast, Margaret J.",
  year      = 1993,
  title     = "Confirmatory Factor Analysis of Ordered Categorical
          Variables With Large Models",
  journal   = "The British Journal of Mathematical and Statistical
          Psychology",
  volume    = 46,
  pages     = "273--286"
}

@Article{powell1986symmetrically,
  author    = {Powell, James L.},
  year      = {1986},
  title     = {Symmetrically Trimmed Least Squares Estimation for Tobit
          Models},
  abstract  = {This paper proposes alternatives to maximum likelihood
          estimation of the censored and truncated regression models
          (known to economists as \&quot;Tobit\&quot; models). The
          proposed estimators are based upon symmetric censoring or
          truncation of the upper tail of the distribution of the
          dependent variable. Unlike methods based on the assumption
          of identically distributed Gaussian errors, the estimators
          are semiparametric, in the sense that they are consistent
          and asymptotically normal for a wide class of (symmetric)
          error distributions with heteroskedasticity of unknown
          form. The paper gives the regularity conditions and proofs
          of these large sample properties, demonstrates how to
          construct consistent estimators of the asymptotic
          covariance matrices, and presents the results of a
          simulation study for the censored case. Extensions and
          limitations of the approach are also considered.},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1435--1460},
  volume    = {54}
}

@ARTICLE{powers:mishra:young:2005,
  AUTHOR =       {Powers, J. R. and G. Mishra and A. F. Young},
  TITLE =        {Differences in Mail and Telephone Responses to Self-Rated Health:
        Use of Multiple Imputation in Correcting for Response Bias},
  JOURNAL =      {Australian and New Zealand Journal of Public Health},
  YEAR =         {2005},
  volume =       {29},
  pages =        {149?154},
}


@Article{prasad:rao:1990,
  author    = {Prasad, N. G. N. and Rao, J. N. K. },
  year      = {1990},
  title     = {The Estimation of the Mean Squared Error of Small-Area
          Estimators},
  journal   = {Journal of the American Statistical Association},
  number    = {409},
  pages     = {163--171},
  volume    = {85}
}

@InCollection{praskova:sen:2009,
  author    = {Zuzana Pr{\' a}{\^ s}kov{\' a} and Pranab Kumar Sen},
  editor    = {D. Pfeffermann and C. R. Rao},
  year      = {2009},
  title     = {Asymptotics in Finite Population Sampling},
  chapter   = {40},
  booktitle = {Sample Surveys: Inference and Analysis},
  publisher = {Elsevier},
  address   = {Oxford, UK},
  series    = {Handbook of Statistics},
  volume    = {29B},
  pages     = {489--522}
}



@ARTICLE{preston:2009,
  AUTHOR =       {John Preston},
  TITLE =        {Rescaled bootstrap for stratified multistage sampling},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2009},
  volume =       {35},
  number =       {2},
  pages =        {227--234},
}

@article{presser:stinson:1998,
 doi = {10.2307/2657486},
 abstract =
    {Compared to conventional interviewer-administered questions about
    attendance at religious services, self-administered items and time-use
    items should minimize social desirability pressures. In fact, they each
    reduce claims of weekly religious attendance by about one-third. This
    difference in measurement approach does not generally affect associations
    between attendance and demographic characteristics. It does, however,
    alter the observed trend in religious attendance over time: In contrast
    to the almost constant attendance rate recorded by conventional
    interviewer-administered items, approaches minimizing social desirability
    bias reveal that weekly attendance has declined continuously over the
    past three decades. These results provide support for the hypothesis that
    America has become more secularized, and they demonstrate the role of
    mode of administration in reducing measurement error.},
 author = {Stanley Presser and Linda Stinson},
 journal = {American Sociological Review},
 number = {1},
 pages = {137--145},
 title = {Data Collection Mode and Social Desirability Bias in Self-Reported Religious Attendance},
 volume = {63},
 year = {1998}
}



@Article{prokhorov:2009,
  author    = {Artem Prokhorov},
  year      = {2009},
  title     = {On relative efficiency of quasi-{MLE} and {GMM} estimators
          of covariance structure models},
  journal   = {Economic Letters},
  volume    = {102},
  pages     = {4--6},
  doi       = {10.1016/j.econlet.2008.08.019}
}

@Book{puge:tomer:eye:2002,
  editor    = {Bruce H. Pugesek and Adrian Tomer and Alexander von Eye},
  year      = {2002},
  title     = {Structural Equation Modeling: Applications in Ecological
          and Evolutionary Biology},
  howpublished  = {Hardcover},
  isbn      = {0521781337},
  publisher = {Cambridge University Press}
}

@Book{pugesek:tomer:von:eye:2002,
  editor    = { Bruce H. Pugesek and Adrian Tomer and Alexander von Eye
          },
  year      = {2002},
  title     = {Structural Equation Modeling: Applications in Ecological
          and Evolutionary Biology},
  abstract  = {{This book presents an introduction to the methodology of
          structural equation modeling, illustrates its use, and goes
          on to argue that it has revolutionary implications for the
          study of natural systems. A major theme of this book is
          that we have, up to this point, attempted to study systems
          primarily using methods (such as the univariate model) that
          were designed only for considering individual processes.
          Understanding systems requires the capacity to examine
          simultaneous influences and responses. Structural equation
          modeling (SEM) has such capabilities. It also possesses
          many other traits that add strength to its utility as a
          means of making scientific progress. In light of the
          capabilities of SEM, it can be argued that much of
          ecological theory is currently locked in an immature state
          that impairs its relevance. It is further argued that the
          principles of SEM are capable of leading to the development
          and evaluation of multivariate theories of the sort vitally
          needed for the conservation of natural systems.
          Supplementary information can be found at the authors
          website,http://www.jamesbgrace.com/. \&\#149; Details why
          multivariate analyses should be used to study ecological
          systems \&\#149; Exposes unappreciated weakness in many
          current popular analyses \&\#149; Emphasises the future
          methodological developments needed to advance our
          understanding of ecological systems}},
  howpublished  = {Hardcover},
  isbn      = {0521781337},
  publisher = {{Cambridge University Press}}
}

@Article{puls:tenhave:landis:1998,
  author    = {Pulkstenis, Erik P. and Thomas and Landis, Richard J.},
  year      = {1998},
  title     = {Model for the Analysis of Binary Longitudinal Pain Data
          Subject to Informative Dropout through Remedication},
  abstract  = {We address the problem of accounting for informative
          dropout in the form of rescue medication when comparing
          pain relievers with respect to longitudinal binary
          pain-relief outcomes. We present a selection model approach
          for binary longitudinal data that accommodates informative
          dropout. The relationship between dropout or remedication
          and the binary pain-relief response is assumed to be
          characterized by a random effect. That is, conditional on
          this random effect, response and dropout are independent.
          Unlike previous approaches to this problem, which rely on
          numerical or approximation methods, we obtain a closed-form
          expression for the marginal log-likelihood of response and
          dropout by specifying a complementary log-log link function
          for both components and a conjugate log-gamma random effect
          distribution. A data analysis supported by simulation
          results suggest that the model fits reasonably well.
          Results are compared to those obtained from conventional,
          but somewhat inappropriate analyses.},
  doi       = {10.2307/2670091},
  issn      = {01621459},
  journal   = {Journal of the American Statistical Association},
  number    = {442},
  pages     = {438--450},
  publisher = {American Statistical Association},
  volume    = {93}
}

@Article{qin:lawless:1994,
  author    = {Qin, Jin and Lawless, Jerry },
  year      = {1994},
  title     = {Empirical Likelihood and General Estimating Equations},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {300--325},
  volume    = {22},
  abstract  = {For some time, so-called empirical likelihoods have been
          used heuristically for purposes of nonparametric
          estimation. Owen showed that empirical likelihood ratio
          statistics for various parameters T(F) of an unknown
          distribution F have limiting chi-square distributions and
          may be used to obtain tests or confidence intervals in a
          way that is completely analogous to that used with
          parametric likelihoods. Our objective in this paper is
          twofold: first, to link estimating functions or equations
          and empirical likelihood; second, to develop methods of
          combining information about parameters. We do this by
          assuming that information about F and T is available in the
          form of unbiased estimating functions. Empirical
          likelihoods for parameters are developed and shown to have
          properties similar to those for parametric likelihood.
          Efficiency results for estimates of both T and F are
          obtained. The methods are illustrated on several problems,
          and areas for future investigation are noted.}
}

@Book{qmc:2002,
  editor    = {Harald Niederreiter},
  year      = {2004},
  title     = {{Monte Carlo} and Quasi-{Monte Carlo} Methods 2002:
          Proceedings of a Conference held at the National University
          of Singapore, Republic of Singapore, November 25-28, 2002},
  publisher = {Springer-Verlag},
  address   = {New York}
}

@article{qu:lindsay:li:2000,
    author = {Qu, Annie and Lindsay, Bruce G. and Li, Bing},
    doi = {10.1093/biomet/87.4.823},
    journal = {Biometrika},
    number = {4},
    pages = {823--836},
    title = {Improving generalised estimating equations using quadratic inference functions},
    volume = {87},
    year = {2000},
    abstract =
       {Generalised estimating equations enable one to estimate
       regression parameters consistently in longitudinal data
       analysis even when the correlation structure is misspecified.
       However, under such misspecification, the estimator of the
       regression parameter can be inefficient. In this paper we
       introduce a method of quadratic inference functions that does
       not involve direct estimation of the correlation parameter,
       and that remains optimal even if the working correlation
       structure is misspecified. The idea is to represent the
       inverse of the working correlation matrix by the linear
       combination of basis matrices, a representation that is valid
       for the working correlations most commonly used. Both
       asymptotic theory and simulation show that under misspecified
       working assumptions these estimators are more efficient than
       estimators from generalised estimating equations. This
       approach also provides a chi-squared inference function for
       testing nested models and a chi-squared regression
       misspecification test. Furthermore, the test statistic
       follows a chi-squared distribution asymptotically whether or
       not the working correlation structure is correctly specified.
       10.1093/biomet/87.4.823},
}

@Article{qu:song:2004,
  author    = {Qu, Annie and Song, Peter X.},
  year      = {2004},
  title     = {Assessing robustness of generalised estimating equations
          and quadratic inference functions},
  abstract  = {In the presence of data contamination or outliers, some
          empirical studies have indicated that the two methods of
          generalised estimating equations and quadratic inference
          functions appear to have rather different robustness
          behaviour.This paper presents a theoretical investigation
          from the perspective of the influence function to identify
          the causes for the difference. We show that quadratic
          inference functions lead to bounded influence functions and
          the corresponding M-estimator has a redescending property,
          but the generalised estimating equation approach does not.
          We also illustrate that, unlike generalised estimating
          equations, quadratic inference functions can still provide
          consistent estimators even if part of the data is
          contaminated. We conclude that the quadratic inference
          function is a preferable method to the generalised
          estimating equation as far as robustness is concerned. This
          conclusion is supported by simulations and real-data
          examples. 10.1093/biomet/91.2.447},
  doi       = {10.1093/biomet/91.2.447},
  journal   = {Biometrika},
  number    = {2},
  pages     = {447--459},
  volume    = {91}
}

@Article{rabe:hesk:skro:2006,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders},
  year      = {2006},
  title     = {Multilevel modelling of complex survey data},
  doi       = {10.1111/j.1467-985X.2006.00426.x},
  issn      = {0964-1998},
  journal   = {Journal of the Royal Statistical Society: Series A
          (Statistics in Society)},
  number    = {4},
  pages     = {805--827},
  publisher = {Blackwell Publishing},
  volume    = {169}
}

@Book{rabe:hesketh:skrondal:2005,
  author    = {Sophia Rabe-Hesketh and Anders Skrondal},
  year      = {2005},
  title     = {Multilevel and Longitudinal Modeling Using Stata},
  publisher = {Stata Press},
  address   = {College Station, TX},
  isbn      = {1597180084}
}

@Article{rabe:hesketh:skrondal:2006:survey,
  author    = {Sophia Rabe-Hesketh and Anders Skrondal},
  year      = {2006},
  title     = {Multilevel modelling of complex survey data},
  journal   = {Journal of the Royal Statistical Society, Series A},
  volume    = {169},
  number    = {4},
  pages     = {805--827}
}

@Book{rabe:hesketh:skrondal:2008,
  author    = {Sophia Rabe-Hesketh and Anders Skrondal},
  year      = {2008},
  title     = {Multilevel and Longitudinal Modeling Using Stata},
  publisher = {Stata Press},
  address   = {College Station, TX},
  edition   = {2nd}
}

@Article{rabe:hesketh:skrondal:2008:smmr,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders},
  year      = {2008},
  title     = {Classical latent variable models for medical research},
  doi       = {10.1177/0962280207081236},
  journal   = {Statistical Methods in Medical Research},
  number    = {1},
  pages     = {5--32},
  volume    = {17},
  abstract  = {Latent variable models are commonly used in medical
          statistics, although often not referred to under this name.
          In this paper we describe classical latent variable models
          such as factor analysis, item response theory, latent class
          models and structural equation models. Their usefulness in
          medical research is demonstrated using real data. Examples
          include measurement of forced expiratory flow, measurement
          of physical disability, diagnosis of myocardial infarction
          and modelling the determinants of clients' satisfaction
          with counsellors' interviews. 10.1177/0962280207081236}
}

@Book{rabe:hesketh:skrondal:2008:stata:2ed,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders},
  year      = {2008},
  title     = {Multilevel and Longitudinal Modeling Using Stata, Second
          Edition},
  abstract  = {{<b>Multilevel and Longitudinal Modeling Using Stata,
          Second Edition</b> discusses regression modeling of
          clustered or hierarchical data, such as data on students
          nested in schools, patients in hospitals, or employees in
          firms. Longitudinal data are also clustered with, for
          instance, repeated measurements on patients or several
          panel waves per survey respondent. Multilevel and
          longitudinal modeling can exploit the richness of such data
          and can disentangle processes operating at different
          levels. <p>Assuming some knowledge of linear regression,
          this bestseller explains models and their assumptions,
          applies methods to real data using Stata, and shows how to
          interpret the results. The applications and exercises span
          a wide range of disciplines, making the book suitable for
          courses on multilevel and longitudinal modeling in the
          medical, social, and behavioral sciences and in applied
          statistics. This extensively revised second edition
          includes 3 new chapters, comprehensive updates for Stata
          10, 38 new exercises, and 27 new data sets. <p>The authors
          teach multilevel and longitudinal modeling at their
          universities and frequently hold workshops at international
          conferences. They have been developing a general modeling
          framework, GLLAMM, and Stata software gllamm for multilevel
          and latent variable modeling. This work has been published
          in their highly acclaimed book <i>Generalized Latent
          Variable Modeling: Multilevel, Longitudinal, and Structural
          Equation Models</i> and in many journals, including
          <i>Biometrics</i>, <i>Psychometrika</i>, <i>Journal of
          Econometrics</i>, and <i>Journal of the Royal Statistical
          Society</i>.}},
  edition   = {2},
  howpublished  = {Paperback},
  isbn      = {1597180408},
  publisher = {Stata Press}
}

@Article{rabe:hesketh:skrondal:pickles:2002,
  author    = {Sophia Rabe-Hesketh and Anders Skrondal and Andrew
          Pickles},
  year      = {2002},
  title     = {Reliable estimation of generalized linear mixed models
          using adaptive quadrature},
  journal   = {Stata Journal},
  volume    = {2},
  number    = {1},
  pages     = {1--21}
}

@Article{rabe:hesketh:skrondal:pickles:2004,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders and Pickles,
          Andrew },
  year      = {2004},
  title     = {Generalized multilevel structural equation modeling},
  doi       = {10.1007/BF02295939},
  journal   = {Psychometrika},
  number    = {2},
  pages     = {167--190},
  volume    = {69},
  abstract  = {A unifying framework for generalized multilevel structural
          equation modeling is introduced. The models in the
          framework, called generalized linear latent and mixed
          models (GLLAMM), combine features of generalized linear
          mixed models (GLMM) and structural equation models (SEM)
          and consist of a response model and a structural model for
          the latent variables. The response model generalizes GLMMs
          to incorporate factor structures in addition to random
          intercepts and coefficients. As in GLMMs, the data can have
          an arbitrary number of levels and can be highly unbalanced
          with different numbers of lower-level units in the
          higher-level units and missing data. A wide range of
          response processes can be modeled including ordered and
          unordered categorical responses, counts, and responses of
          mixed types. The structural model is similar to the
          structural part of a SEM except that it may include latent
          and observed variables varying at different levels. For
          example, unit-level latent variables (factors or random
          coefficients) can be regressed on cluster-level latent
          variables. Special cases of this framework are explored and
          data from the British Social Attitudes Survey are used for
          illustration. Maximum likelihood estimation and empirical
          Bayes latent score prediction within the GLLAMM framework
          can be performed using adaptive quadrature in gllamm, a
          freely available program running in Stata.}
}

@Article{rabe:hesketh:skrondal:pickles:2004:sj,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders and Pickles,
          Andrew },
  year      = {2004},
  title     = {Maximum likelihood estimation of generalized linear models
          with covariate measurement error},
  journal   = {Stata Journal},
  number    = {4},
  pages     = {386--411},
  volume    = {3}
}

@Article{rabe:hesketh:skrondal:pickles:2005,
  author    = {Rabe-Hesketh, Sophia and Skrondal, Anders and Pickles,
          Andrew },
  year      = {2005},
  title     = {Maximum likelihood estimation of limited and discrete
          dependent variable models with nested random effects},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {301--323},
  volume    = {128}
}

@article{rafei:flannagan:elliott:2020,
  title={Big {Data} for finite population inference: 
  Applying quasi-random approaches to naturalistic driving data 
  using {B}ayesian additive regression trees},
  author={Rafei, Ali and Flannagan, Carol A. C. and Elliott, Michael R.},
  journal={Journal of Survey Statistics and Methodology},
  volume={8},
  number={1},
  pages={148--180},
  year={2020},
  publisher={Oxford University Press}
}

@InCollection{raftery:1993,
  author    = {Adrian Raftery},
  editor    = {Kenneth A Bollen and J Scott Long},
  year      = {1993},
  title     = {Bayesian model selection in structural equation models},
  booktitle = {Testing structural equation models},
  publisher = {Sage Publications},
  volume    = {154},
  series    = {Focus Edition},
  address   = {Newbury Park}
}

@Book{raftery:tanner:wells:2002,
  editor    = {Adrian E. Raftery and Martin A. Tanner and Martin T.
          Wells},
  year      = {2002},
  title     = {Statistics in the 21st century},
  publisher = {Chapman and {Hall}/{CRC} and American Statitical
          Association},
  volume    = {93},
  series    = {Monographs on Statistics and Applied Probability},
  address   = {Boca Raton, FL}
}

@Article{raghunathan:1993,
  author    = {Raghunathan, T. E.},
  year      = {1993},
  title     = {A Quasi-Empirical Bayes Method for Small Area Estimation},
  abstract  = {This article develops an empirical Bayes-type approach for
          small area estimation based only on the specification of a
          set of conditionally independent hierarchical mean and
          variance functions describing the first two moments of the
          process generating the data. The objective of the analysis
          is to draw inference about the intermediate or the random
          means. We combine the quasi-likelihood functions to
          construct the quasi-posterior density of the random means
          conditional on the data and the marginal parameters. We
          describe a method for estimating the marginal parameters
          that are then substituted in the quasi-posterior density of
          the random means. The empirical quasi-posterior density so
          derived is used to obtain the quasi-empirical Bayes
          estimates of the random parameters. We apply the
          methodology to estimate the utilization rates of cancer
          chemotherapy for the selected counties in the state of
          Washington.},
  journal   = {Journal of the American Statistical Association},
  number    = {424},
  pages     = {1444--1448},
  volume    = {88}
}

@ARTICLE{raghu:grizzle:1995,
  author =       {Raghunathan, T.E., and Grizzle, J.E.},
  title =        {A Split Questionnaire Survey Design},
  journal =      {Journal of the American Statistical Association},
  year =         {1995},
  volume =       {90},
  pages =        {54--63},
}


@article{ragh:xie:sche:pars:davi:dodd:feue:2006,
    abstract =
        {Cancer surveillance research requires estimates of the prevalence of
        cancer risk factors and screening for small areas such as counties.
        Two popular data sources are the Behavioral Risk Factor Surveillance
        System ({BRFSS}), a telephone survey conducted by state agencies, and
        the National Health Interview Survey ({NHIS}), an area probability
        sample survey conducted through face-to-face interviews. Both data
        sources have advantages and disadvantages. The {BRFSS} is a larger
        survey and almost every county is included in the survey, but it has
        lower response rates as is typical with telephone surveys and it does
        not include subjects who live in households with no telephones. On
        the other hand, the {NHIS} is a smaller survey, with the majority of
        counties not included; but it includes both telephone and
        nontelephone households, and has higher response rates. A preliminary
        analysis shows that the distributions of cancer screening and risk
        factors are different for telephone and nontelephone households.
        Thus, information from the two surveys may be combined to address
        both nonresponse and noncoverage errors. A hierarchical Bayesian
        approach that combines information from both surveys is used to
        construct county-level estimates. The proposed model incorporates
        potential noncoverage and nonresponse biases in the {BRFSS} as well
        as complex sample design features of both surveys. A Markov chain
        Monte Carlo method is used to simulate draws from the joint posterior
        distribution of unknown quantities in the model that uses
        design-based direct estimates and county-level covariates. Yearly
        prevalence estimates at the county level for 49 states, as well as
        for the entire state of Alaska and the District of Columbia, are
        developed for six outcomes using {BRFSS} and {NHIS} data from the
        years 1997?2000. The outcomes include smoking and use of common
        cancer screening procedures. The {NHIS}/{BRFSS} combined county-level
        estimates are substantially different from those based on the {BRFSS}
        alone.},
    author = {Raghunathan, Trivellore E. and Xie, Dawei and Schenker, Nathaniel and Parsons, Van L. and Davis,
        William W. and Dodd, Kevin W. and Feuer, Eric J.},
    doi = {10.1198/016214506000001293},
    journal = {Journal of the American Statistical Association},
    number = {478},
    pages = {474--486},
    title = {Combining Information From Two Surveys to Estimate County-Level Prevalence Rates of Cancer Risk Factors and Screening},
    volume = {102},
    year = {2007}
}

@Book{ramsey:silver:1997,
  author    = {James O. Ramsey and Bernard W. Silverman},
  year      = {1997},
  title     = {Functional Data Analysis},
  publisher = {Springer},
  series    = {Springer Series in Statistics},
  address   = {New York}
}

@ARTICLE{rao:1968,
  AUTHOR =       {Rao, C. R.},
  TITLE =        {A note on a previous lemma in the theory of least squares and some further results},
  JOURNAL =      {Sankhya},
  YEAR =         {1968},
  pages =        {259--266},
}


@Article{rao1985inference,
  author    = {Rao, J. N. K. and Wu, C. F. J.},
  year      = {1985},
  title     = {Inference From Stratified Samples: Second-Order Analysis
          of Three Methods for Nonlinear Statistics},
  abstract  = {For stratified samples and nonlinear statistics that can
          be expressed as functions of estimated totals, second-order
          asymptotic expansions of the linearization, jackknife, and
          balanced repeated-replication variance estimators are
          obtained. Based on these, comparisons are made in terms of
          their biases. Some higher order asymptotic equivalence
          results are also established. The special case of a
          combined ratio estimator is investigated in detail. Some
          results on bias reduction achieved by the jackknife and
          balanced repeated-replication estimators of a nonlinear
          function of totals are also given.},
  journal   = {Journal of the American Statistical Association},
  number    = {391},
  pages     = {620--630},
  volume    = {80}
}

@Article{rao1987simple,
  author    = {Rao, J. N. K. and Scott, A. J.},
  year      = {1987},
  title     = {On Simple Adjustments to Chi-Square Tests with Sample
          Survey Data},
  abstract  = {For testing the goodness-of-fit of a \$\log-linear model
          to a multi-way contingency table with cell proportions
          estimated from survey data, Rao and Scott (1984) derived a
          first-order correction, \$\delta\ldot\$, to Pearson
          chi-square statistic, \$X^2\$ (or the likelihood ratio
          statistic, \$G^2\$) that takes account of the survey
          design. It was also shown that \$\delta\ldot\$ requires the
          knowledge of only the cell design effects (deffs) and the
          marginal deffs provided the model admits direct solution to
          likelihood equations under multinomial sampling. Simple
          upper bounds on \$\delta\ldot\$ are obtained here for
          models not admitting direct solutions, also requiring only
          cell deffs and marginal deffs or some generalized deffs not
          depending on any hypothesis. Applicability of an
          \$F\$-statistic used in GLIM to test a nested hypothesis is
          also investigated. In the case of a logit model involving a
          binary response variable, simple upper bounds on
          \$\delta\ldot\$ are obtained in terms of deffs of response
          proportions for each factor combination or some generalized
          deffs not depending on any hypothesis. Applicability of the
          GLIM \$F\$-statistic for nested hypotheses is also studied.},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {385--397},
  volume    = {15}
}

@Article{rao1988resampling,
  author    = {Rao, J. N. K. and Wu, C. F. J.},
  year      = {1988},
  title     = {Resampling Inference With Complex Survey Data},
  comment   = {A cornerstone paper on variance estimation by resampling
          in the case of complex survey data. Compares the jackknife,
          balanced repeated replication, and the bootstrap
          procedures. Things are not so trivial with dependent data,
          so if you want to do bootstrap with clustered data, this
          paper is a must to read.},
  doi       = {10.2307/2288945},
  issn      = {01621459},
  journal   = {Journal of the American Statistical Association},
  number    = {401},
  pages     = {231--241},
  publisher = {American Statistical Association},
  volume    = {83}
}

@Article{rao1990estimating,
  author    = {Rao, J. N. K. and Kovar, J. G. and Mantel, H. J.},
  year      = {1990},
  title     = {On Estimating Distribution Functions and Quantiles from
          Survey Data Using Auxiliary Information},
  abstract  = {Ratio and difference estimators of a population
          distribution function under a general sampling design are
          obtained, using auxiliary population information. The
          relative mean errors and relative mean square errors of
          these estimators and a model-based estimator of Chambers
          \&amp; Dunstan (1986) are compared through a simulation
          study. The advantages of the design-based estimators over
          the model-based estimator under model misspecifications,
          especially for large samples, are demonstrated. Ratio and
          difference estimators of a population quantile are also
          studied.},
  journal   = {Biometrika},
  number    = {2},
  pages     = {365--375},
  volume    = {77}
}

@Article{rao:shao:1999,
  author    = {Rao, J. N. K. and Shao, J.},
  year      = {1999},
  title     = {Modified balanced repeated replication for complex survey
          data},
  abstract  = {Balanced repeated replication, commonly used to estimate
          the variances of nonlinear estimators from stratified
          sampling designs, can run into problems in estimating a
          ratio of small domain totals or in the case of
          poststratification or unit nonresponse adjustment involving
          ratio weighting within cells if some cell sizes are small.
          This problem is mainly caused by a sharp perturbation of
          the weights to construct replicate estimates. To avoid
          extreme replicate estimates, Robert Fay proposed a modified
          balanced repeated replication method using gentler
          perturbation of weights. Theoretical properties of the
          modified method are studied here. The method is extended to
          the case of imputation for item nonresponse. Both smooth
          and nonsmooth estimators are studied.
          10.1093/biomet/86.2.403},
  doi       = {10.1093/biomet/86.2.403},
  journal   = {Biometrika},
  number    = {2},
  pages     = {403--415},
  volume    = {86}
}

@Article{rao:1970,
  author    = {Rao, C.R.},
  year      = {1970},
  title     = {Estimation of heteroscedastic variances in linear model},
  journal   = {Journal of the American Statistical Association},
  volume    = {65},
  pages     = {161--172}
}

@Book{rao:1971,
  author    = "Rao, C.R.",
  year      = 1971,
  title     = "Linear Statistical Inference and Its Applications",
  edition   = "2nd",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{rao:1996,
  author    = {Rao, J. N. K. },
  year      = {1996},
  title     = {On Variance Estimation With Imputed Survey Data},
  journal   = {Journal of the American Statistical Association},
  number    = {434},
  pages     = {499--506},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28199606%2991%3A434%3C499%3AOVEWIS%3E2.0.CO%3B2-E}
          ,
  volume    = {91},
  abstract  = {Unit nonresponse and item nonresponse both occur
          frequently in surveys. Unit nonresponse is customarily
          handled by weighting adjustment, whereas item nonresponse
          is usually treated by some form of imputation. In
          particular, deterministic or stochastic imputation is often
          used to assign values for missing item responses. We
          provide an account of some recent work on jackknife
          variance estimation based on adjusted imputed values, using
          only a single imputation and hence a single completed data
          set. We also present linearized versions of the jackknife
          variance estimators. We study both stratified simple random
          sampling and stratified multistage sampling. Existing
          computer programs for jackknife and linearization variance
          estimation can be modified to implement the proposed
          variance estimators without requiring the creation and
          permanent retention of supplemental data sets. But for
          secondary analyses, the completed data set must include
          information on response status for each item as well as on
          the imputation class.}
}

@ARTICLE{rao:1999,
  AUTHOR =       {J. N. K. Rao},
  TITLE =        {Some recent advances in model-based small area estimation},
  JOURNAL =      {Survey Methodology},
  YEAR =         {1999},
  volume =       {25},
  number =       {2},
  pages =        {175--186},
}


@Book{rao:2003,
  author    = {J. N. K. Rao},
  year      = {2003},
  title     = {Small Area Estimation},
  publisher = {John Wiley and Sons},
  series    = {Wiley series in survey methodology},
  address   = {New York}
}

@Book{rao:molina:2015,
  author    = {J. N. K. Rao and Isabel Molina},
  year      = {2015},
  title     = {Small Area Estimation},
  edition   = {2nd},
  publisher = {John Wiley and Sons},
  series    = {Wiley series in survey methodology},
  address   = {Hoboken, NJ}
}

@Article{rao:2005,
  author    = {J. N. K. Rao},
  year      = {2005},
  title     = {Interplay Between Sample Survey Theory and Practice: An
          Appraisal},
  journal   = {Survey Methodology},
  volume    = {31},
  number    = {2},
  pages     = {117--138}
}

@Article{rao:2006:appraisal,
  author    = {Rao, J. N. K.},
  year      = {2006},
  title     = {Interplay between sample survey theory and practice: An
          appraisal},
  abstract  = {A large part of sample survey theory has been directly
          motivated by practical problems encountered in the design
          and analysis of sample surveys. On the other hand, sample
          survey theory has influenced practice, often leading to
          significant improvements. This paper will examine this
          interplay over the past 60 years or so. Examples where new
          theory is needed or where theory exists but is not used
          will also be presented.},
  comment   = {This is a paper in Invited Waksberg Series of Statistical
          Society of Canada. Rao reviews the history of survey
          sampling and estimation noting the major cornerstones and
          the most important publications. The reader should be at
          least marginally familiar with the survey statistics world,
          but if he is, the paper is a superb summary of where survey
          statistics is at now. The list of reference is a good
          reading per se!},
  journal   = {Survey Methodology},
  number    = {2},
  pages     = {117--138},
  volume    = {31}
}

@Article{rao:grah:rota:1964,
  author    = "Rao, J. N. K. and Graham, Jack E.",
  year      = 1964,
  title     = "Rotation Designs for Sampling on Repeated Occasions",
  journal   = "Journal of the American Statistical Association",
  volume    = 59,
  number    = 306,
  pages     = "492--509"
}

@article{rao:kovar:mantel:1990,
    author = {Rao, J. N. K. and Kovar, J. G. and Mantel, H. J.},
    doi = {10.1093/biomet/77.2.365},
    journal = {Biometrika},
    month = {June},
    number = {2},
    pages = {365--375},
    title = {On estimating distribution functions and quantiles from survey data
             using auxiliary information},
    volume = {77},
    year = {1990},
    abstract =
       {Ratio and difference estimators of a population distribution
       function under a general sampling design are obtained, using
       auxiliary population information. The relative mean errors
       and relative mean square errors of these estimators and a
       model-based estimator of Chambers \& Dunstan (1986) are
       compared through a simulation study. The advantages of the
       design-based estimators over the model-based estimator under
       model misspecifications, especially for large samples, are
       demonstrated. Ratio and difference estimators of a population
       quantile are also studied. 10.1093/biomet/77.2.365},
}

@Article{rao:scott:1981,
  author    = {Rao, J.N.K. and Scott, A.J.},
  year      = {1981},
  title     = {The analysis of categorical data from complex sample
          surveys: Chi-squared tests for goodness of fit and
          independence in two-way tables},
  journal   = {The Journal of the American Statistical Association},
  volume    = {76},
  pages     = {221--230}
}

@Article{rao:scott:1984,
  author    = {Rao, J.N.K. and Scott, A.J.},
  year      = {1984},
  title     = {On chi-squared tests for multi-way tables with cell proportions estimated
               from survey data},
  journal   = {The Annals of Statistics},
  pages     = {46--60},
  volume    = {12},
  number    = 1
}

@Article{rao:scott:1987,
  author    = {Rao, J.N.K. and Scott, A.J.},
  year      = {1987},
  title     = {On Simple Adjustments to Chi-Square Tests with Sample
          Survey Data},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {385--397},
  volume    = {15},
  abstract  = {For testing the goodness-of-fit of a $\log-linear model to
          a multi-way contingency table with cell proportions
          estimated from survey data, Rao and Scott (1984) derived a
          first-order correction, $\delta\ldot$, to Pearson
          chi-square statistic, $X^2$ (or the likelihood ratio
          statistic, $G^2$) that takes account of the survey design.
          It was also shown that $\delta\ldot$ requires the knowledge
          of only the cell design effects (deffs) and the marginal
          deffs provided the model admits direct solution to
          likelihood equations under multinomial sampling. Simple
          upper bounds on $\delta\ldot$ are obtained here for models
          not admitting direct solutions, also requiring only cell
          deffs and marginal deffs or some generalized deffs not
          depending on any hypothesis. Applicability of an
          $F$-statistic used in GLIM to test a nested hypothesis is
          also investigated. In the case of a logit model involving a
          binary response variable, simple upper bounds on
          $\delta\ldot$ are obtained in terms of deffs of response
          proportions for each factor combination or some generalized
          deffs not depending on any hypothesis. Applicability of the
          GLIM $F$-statistic for nested hypotheses is also studied.}
}

@Article{rao:scott:benhin:2003,
  author    = {J.N.K. Rao and A.J. Scott and E. Benhin},
  year      = {2003},
  title     = {Undoing Complex Survey Data Structures: Some Theory and
          Applications of Inverse Sampling (with discussion)},
  journal   = {Survey Methodology},
  volume    = {29},
  number    = {2},
  pages     = {107--128},
  abstract  = {Application of classical statistical methods to data from
          complex sample surveys without making allowance for the
          survey design features can lead to erroneous inferences.
          Methods have been developed that account for the survey
          design, but these methods require additional information
          such as survey weights, design effects or cluster
          identification for microdata. Inverse sampling (Hinkins, Oh
          and Scheuren 1997) provides an alternative approach by
          undoing the complex survey data structures so that standard
          methods can be applied. Repeated subsamples with
          unconditional simple random sampling structure are drawn
          and each subsample analysed by standard methods and then
          combined to increase the efficiency. This method has the
          potential to preserve confidentiality of microdata,
          although computer-intensive. We present some theory of
          inverse sampling and explore its limitations. A combined
          estimating equations approach is proposed for handling
          complex parameters such as ratios and ?census? linear
          regression and logistic regression parameters. The method
          is applied to a cluster correlated data set reported in
          Battese, Harter and Fuller (1988).}
}

@Article{rao:shao:1992,
  author    = {Rao, J. N. K. and Shao, J.},
  year      = {1992},
  title     = {Jackknife Variance Estimation with Survey Data Under Hot
          Deck Imputation},
  doi       = {10.2307/2337236},
  journal   = {Biometrika},
  number    = {4},
  pages     = {811--822},
  volume    = {79},
  abstract  = {Hot deck imputation is commonly employed for item
          nonresponse in sample surveys. It is also a common practice
          to treat the imputed values as if they are true values, and
          then compute the variance estimates using standard
          formulae. This procedure, however, could lead to serious
          underestimation of the true variance, when the proportion
          of missing values for an item is appreciable. We propose a
          jackknife variance estimator for stratified multistage
          surveys which is obtained by first adjusting the imputed
          values for each pseudo-replicate and then applying the
          standard jackknife formula. The proposed jackknife variance
          estimator is shown to be consistent as the sample size
          increases, assuming equal response probabilities within
          imputation classes and using a particular hot deck
          imputation.}
}

@Article{rao:shao:1999,
  author    = {Rao, J.N.K. and Shao, J. },
  year      = {1999},
  title     = {Modified balanced repeated replication for complex survey
          data},
  doi       = {10.1093/biomet/86.2.403},
  journal   = {Biometrika},
  number    = {2},
  pages     = {403--415},
  comment   = {http://biomet.oxfordjournals.org/cgi/content/abstract/86/2/403}
          ,
  volume    = {86},
  abstract  = {Balanced repeated replication, commonly used to estimate
          the variances of nonlinear estimators from stratified
          sampling designs, can run into problems in estimating a
          ratio of small domain totals or in the case of
          poststratification or unit nonresponse adjustment involving
          ratio weighting within cells if some cell sizes are small.
          This problem is mainly caused by a sharp perturbation of
          the weights to construct replicate estimates. To avoid
          extreme replicate estimates, Robert Fay proposed a modified
          balanced repeated replication method using gentler
          perturbation of weights. Theoretical properties of the
          modified method are studied here. The method is extended to
          the case of imputation for item nonresponse. Both smooth
          and nonsmooth estimators are studied.
          10.1093/biomet/86.2.403}
}

@Book{rao:sinha:2007,
  editor    = {C. Rao and S. Sinharay},
  year      = {2007},
  title     = {Handbook of Statistics: Psychometrics},
  publisher = {Elsevier},
  volume    = {26},
  isbn      = {0-444-52103-8},
  series    = {Handbook of Statistics}
}

@Book{rao:szekely:ed:2000,
  year      = {2000},
  title     = {Statistics for the 21st Century: Methodologies for
          Applications of the Future (Statistics: a Series of
          Textbooks and Monogrphs)},
  abstract  = {A selection of articles presented at the Eighth Lukacs
          Symposium held at the Bowling Green State University, Ohio.
          They discuss consistency and accuracy of the sequential
          bootstrap, hypothesis testing, geometry in multivariate
          analysis, the classical extreme value model, the analysis
          of cross-classified data, diffusion models for neural
          activity, estimation with quadratic loss, econometrics,
          higher order asymptotics, pre- and post-limit theorems, and
          more.},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {0824790294},
  publisher = {CRC}
}

@InCollection{rao:thomas:1989,
  author    = {J. N. K. Rao and D. R. Thomas},
  editor    = {C. J. Skinner and D. Holt and T. M. F. Smith},
  year      = {1989},
  title     = {Chi-Squared Tests for Contingency Tables},
  booktitle = {Analysis of Complex Surveys},
  publisher = {John Wiley and Sons},
  chapter   = {4},
  address   = {Chichester, UK}
}

@InCollection{rao:thomas:2003,
  author    = {J. N. K. Rao and D. R. Thomas},
  editor    = {R. L. Chambers and C. J. Skinner},
  year      = {2003},
  title     = {Analysis of Categorical Response Data from Complex Surveys:
               an Appraisal and Update},
  booktitle = {Analysis of Survey Data},
  publisher = {John Wiley and Sons},
  chapter   = {7},
  address   = {Chichester, UK}
}

@Article{rao:wu:1985,
  author    = {Rao, J. N. K. and Wu, C. F. J. },
  year      = {1985},
  title     = {Inference From Stratified Samples: Second-Order Analysis
          of Three Methods for Nonlinear Statistics},
  journal   = {Journal of the American Statistical Association},
  number    = {391},
  pages     = {620--630},
  volume    = {80},
  abstract  = {For stratified samples and nonlinear statistics that can
          be expressed as functions of estimated totals, second-order
          asymptotic expansions of the linearization, jackknife, and
          balanced repeated-replication variance estimators are
          obtained. Based on these, comparisons are made in terms of
          their biases. Some higher order asymptotic equivalence
          results are also established. The special case of a
          combined ratio estimator is investigated in detail. Some
          results on bias reduction achieved by the jackknife and
          balanced repeated-replication estimators of a nonlinear
          function of totals are also given.}
}

@Article{rao:wu:1988,
  author    = {Rao, J. N. K. and Wu, C. F. J. },
  year      = {1988},
  title     = {Resampling Inference With Complex Survey Data},
  journal   = {Journal of the American Statistical Association},
  number    = {401},
  pages     = {231--241},
  volume    = {83},
  comment   = {A cornerstone paper on variance estimation by resampling
          in the case of complex survey data. Compares the jackknife,
          balanced repeated replication, and the bootstrap
          procedures. Things are not so trivial with dependent data,
          so if you want to do bootstrap with clustered data, this
          paper is a must to read.
          http://links.jstor.org/sici?sici=0162-1459%28198803%2983%3A401%3C231%3ARIWCSD%3E2.0.CO%3B2-O}

}

@InCollection{rao:wu:2009,
  author    = {J. N. K. Rao and Changbao Wu},
  year      = {2009},
  title     = {Empirical Likelihood Methods},
  booktitle = {Sample Surveys: Theory, Methods and Inference},
  publisher = {Elsevier},
  volume    = {29},
  series    = {Handbook of Statistics}
}

@Article{rao:wu:yue:1992,
  author    = {J. N. K. Rao and C. F. J. Wu and K. Yue},
  year      = {1992},
  title     = {Some recent work on resampling methods for complex
          surveys},
  journal   = {Survey Methodology},
  volume    = {18},
  number    = {2},
  pages     = {209--217},
  source    = {Milorad}
}

@article{rao:verret:hidir:2013,
    author = {Rao, J. N. K. and Verret, Franc{\c{c}}ois and Hidiroglou, Mike A.},
    journal = {Survey Methodology},
    number = {2},
    pages = {263--282},
    title = {A weighted composite likelihood approach to inference for two-level models from survey data},
    volume = {39},
    year = {2013}
}

@article{rao:2021:nonprob,
  author = {J. N. K. Rao},
  journal = {Sankhyā B: The Indian Journal of Statistics},
  doi = {10.1007/s13571-020-00227-w},
  title = {On Making Valid Inferences by Integrating Data from Surveys and Other Sources},
  volume = 83,
  pages = {242-272},
  year = {2021}
  
}

@Book{rasch:1960,
  author    = {Rasch, G.},
  year      = {1960},
  title     = {Probabilistic models for some intelligence and attainment
          tests.},
  publisher = {The University of Chicago Press}
}

@Book{raud:bryk:2002,
  author    = "Raudenbush, Stephen and Bryk, Anthony",
  year      = 2002,
  title     = "Hierarchical Linear Models",
  edition   = "2nd",
  publisher = "SAGE",
  address   = "Thousand Oaks, CA"
}

@Book{raykov:marcoulides:2006,
  author    = {Raykov, Tenko and Marcoulides, George A. },
  year      = {2006},
  title     = {A First Course in Structural Equation Modeling},
  isbn      = {0805855882},
  publisher = {Lawrence Erlbaum Associates}
}

@InCollection{raykov:penev:2002,
  author    = {Raykov, T., and Penev, S.},
  editor    = {G. A. Marcoulides and I. Moustaki},
  year      = {2002},
  title     = {Exploring Structural Equation Model Misspecifications via
          Latent Individual Residuals},
  booktitle = {Latent Variable and Latent Structure Models},
  publisher = {Lawrence Erlbaum},
  address   = {Mahwah, NJ}
}

@Book{rencher:2002,
  author    = "Alvin C. Rencher",
  year      = 2002,
  title     = "Methods of Multivariate Analysis",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@TechReport{reichlin:2002,
  author    = "Lucrezia Reichlin",
  year      = 2002,
  title     = "Factor Models in Large Cross-Sections of Time Series",
  institution   = "CEPR",
  number    = "DP3285",
  type      = "Discussion Paper"
}

@Article{rein:echa:chin:2002,
  author    = {Reinartz, Werner J. and Echambadi, Raj and Chin, Wynne
          W.},
  year      = {2002},
  title     = {Generating Non-normal Data for Simulation of Structural
          Equation Models Using Mattson's Method},
  abstract  = {SEM researchers use Monte-Carlo simulations to ascertain
          the robustness of statistical estimators and the
          performance of various fit indices under varying conditions
          of nonnormality. The efficacy of these Monte-Carlo
          simulations is closely related to the generation of
          non-normal data. Traditionally, SEM researchers have used
          approaches proposed by Fleishman (1978) and Vale and
          Maurelli (1983) to generate multivariate nonnormal random
          numbers. However, both approaches do not provide a method
          to determine univariate skewness and kurtosis of the
          observed variables when a non-normal distribution is
          specified for some or all of the latent variables. Mattson
          (1997) proposed a method for generating data on the latent
          variables with controlled skewness and kurtosis of the
          observed variables. We empirically test the applicability
          of Mattson's theoretical method in a Monte-Carlo
          simulation. Specifically, we assess the impact of data
          generation, selection of transformation method, sample size
          and degree of skewness/kurtosis on the performance of the
          method. Our results suggest that Mattson's method seems to
          be a good approach to generate data with defined levels of
          skewness and kurtosis. In addition, based on the results of
          our analysis, we provide practicing researchers
          recommendations regarding their empirical implementation
          schemes.},
  doi       = {10.1207/S15327906MBR3702\_03},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {227--244},
  volume    = {37}
}

@article{reiter:2002,
  title={Satisfying disclosure restrictions with synthetic data sets},
  author={Reiter, Jerome P},
  journal={Journal of Official Statistics},
  volume={18},
  number={4},
  pages={531},
  year={2002},
  publisher={Statistics Sweden (SCB)}
}

@article{reiter:2012:poq,
  title={Statistical approaches to protecting confidentiality 
       for microdata and their effects on the quality of statistical inferences},
  author={Reiter, Jerome P},
  journal={Public Opinion Quarterly},
  volume={76},
  number={1},
  pages={163--181},
  year={2012},
  publisher={Oxford University Press}
}

@Book{rench:1998,
  author    = "A. C. Rencher",
  year      = 1998,
  title     = "Multivariate Statistical Inference and Applications",
  publisher = "Wiley",
  address   = "New York"
}

@Book{rench:2002,
  author    = "A. C. Rencher",
  year      = 2002,
  title     = "Methods of Multivariate Analysis",
  publisher = "Wiley",
  address   = "New York"
}

@article{ridgeway:kovalchik:griffin:kabeto:2015,
  author    = {Ridgeway, Greg and Kovalchik, Stephanie Ann and 
                Griffin, Beth Ann and Kabeto, Mohammed U.},
  title     = {Propensity Score Analysis with Survey Weighted Data},
  journal   = {Journal of Causal Inference},
  year      = {2015},
  volume    = {3},
  issue     = {2},
  pages     = {237--249},
  doi       = {10.1515/jci-2014-0039}
}

@Article{rigd:ferg:1991,
  author    = "Rigdon, Edward E. and Ferguson, Carl E., Jr",
  year      = 1991,
  title     = "The Performance of the Polychoric Correlation Coefficient
          and Selected Fitting Functions in Confirmatory Factor
          Analysis With Ordinal Data",
  journal   = "Journal of Marketing Research",
  volume    = 28,
  pages     = "491--497"
}

@Article{rindskopf:1983,
  author    = {Rindskopf, David },
  year      = {1983},
  title     = {Parameterizing inequality constraints on unique variances
          in linear structural models},
  doi       = {10.1007/BF02314677},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {73--83},
  volume    = {48}
}

@Article{rindskopf:1984,
  author    = {Rindskopf, David},
  year      = {1984},
  title     = {Structural Equation Models: Empirical Identification,
          {H}eywood Cases, and Related Problems},
  journal   = {Sociological Methods and Research},
  volume    = {13},
  pages     = {109--119}
}

@inproceedings{rivers:2007,
    author = {Rivers, Douglas},
    publisher = {The American Statistical Association},
    booktitle = {Proceedings of Survey Research Methods Section},
    title = {Sampling for Web Surveys},
    year = {2007},
    address = {Alexandria, VA}
}

@article{robert:casella:2011,
    abstract =
        {We attempt to trace the history and development of Markov chain
        Monte Carlo ({MCMC}) from its early inception in the late 1940s
        through its use today. We see how the earlier stages of Monte Carlo
        ({MC}, not {MCMC}) research have led to the algorithms currently in
        use. More importantly, we see how the development of this methodology
        has not only changed our solutions to problems, but has changed the
        way we think about problems.},
    author = {Robert, Christian and Casella, George},
    doi = {10.1214/10-sts351},
    journal = {Statistical Science},
    number = {1},
    pages = {102--115},
    title = {A Short History of {Markov} Chain {Monte} {Carlo}: Subjective Recollections from Incomplete Data},
    volume = {26},
    year = {2011}
}

@Book{rogers:swami:hamble:1991,
  author    = {Rogers, Jane H. and Swaminathan, Hariharan and Hambleton,
          Ronald K. },
  year      = {1991},
  title     = {Fundamentals of Item Response Theory},
  isbn      = {0803936478},
  publisher = {Sage Publications, Inc},
  series    = {Measurement Methods for the Social Science}
}

@article{rogers:1993,
  title={Regression standard errors in clustered samples},
  author={Rogers, W.H.},
  journal={Stata Technical Bulletin},
  volume={13},
  pages={19--23},
  year={1993},
  comment={http://www.stata.com/support/faqs/stat/stb13\_rogers.pdf}
}

@Article{romano:1989,
  author    = {Romano, Joseph P. },
  year      = {1989},
  title     = {Bootstrap and Randomization Tests of some Nonparametric
          Hypotheses},
  doi       = {10.2307/2241507},
  journal   = {The Annals of Statistics},
  number    = {1},
  pages     = {141--159},
  volume    = {17},
  abstract  = {In this paper, the asymptotic behavior of some
          nonparametric tests is studied in situations where both
          bootstrap tests and randomization tests are applicable.
          Under fairly general conditions, the tests are
          asymptotically equivalent in the sense that the resulting
          critical values and power functions are appropriately
          close. This implies, among other things, that the
          difference in the critical functions of the tests,
          evaluated at the observed data, tends to 0 in probability.
          Randomization tests may be preferable since an exact
          desired level of the test may be obtained for finite
          samples. Examples considered are: testing independence,
          testing for spherical symmetry, testing for
          exchangeability, testing for homogeneity, and testing for a
          change point.}
}

@Book{romes:2004,
  author    = {Charles Romesburg},
  year      = {2004},
  title     = {Cluster Analysis For Researchers},
  publisher = {Lulu press}
}

@TechReport{roodman:2009,
  author    = {David Roodman},
  year      = {2009},
  title     = {Estimating fully observed recursive mixed-process models
          with {\tt cmp}},
  institution   = {Center for Global Development},
  type      = {Working Paper},
  number    = {168},
  address   = {Washington, DC}
}

@article{roodman:2011,
    author = {Roodman, David},
    journal = {The Stata Journal},
    number = {2},
    pages = {159--206},
    title = {Fitting fully observed recursive mixed-process models with cmp},
    volume = {11},
    year = {2011}
}

@Article{rose:1984,
  author    = {Rosenbaum, Paul R.},
  year      = {1984},
  title     = {Conditional permutation tests and the propensity score in
          observational studies},
  journal   = {Journal of the American Statistical Association},
  volume    = {79},
  pages     = {565--574}
}

@article{rosenbaum:1987,
  title={Sensitivity analysis for certain permutation inferences in matched observational studies},
  author={Rosenbaum, Paul R},
  journal={Biometrika},
  volume={74},
  number={1},
  pages={13--26},
  year={1987},
  publisher={Oxford University Press}
}

@Article{rose:1987:corr,
  author    = {Rosenbaum, Paul R.},
  year      = {1987},
  title     = {Sensitivity analysis for certain permutation inferences in
          matched observational studies ({C}orr: {V}75 p396)},
  journal   = {Biometrika},
  volume    = {74},
  pages     = {13--26}
}

@Article{rose:1988,
  author    = {Rosenbaum, Paul R.},
  year      = {1988},
  title     = "Permutation Tests for Matched Pairs with Adjustments for
          Covariates",
  journal   = {Applied Statistics},
  volume    = {37},
  pages     = {401--411}
}

@Article{rose:1989,
  author    = {Rosenbaum, Paul R.},
  year      = {1989},
  title     = {On permutation tests for hidden biases in observational
          studies: {A}n application of {H}olley's inequality to the
          {S}avage lattice},
  journal   = {The Annals of Statistics},
  volume    = {17},
  pages     = {643--653}
}

@Article{rose:krie:1990,
  author    = {Rosenbaum, Paul R. and Krieger, Abba M.},
  year      = {1990},
  title     = "Sensitivity of Two-Sample Permutation Inferences in
          Observational Studies",
  journal   = {Journal of the American Statistical Association},
  volume    = {85},
  pages     = {493--498}
}

@Article{rose:rubin:1983,
  author    = {Rosenbaum, P. and D. Rubin},
  year      = {1983},
  title     = {The central role of the propensity score in observational
          studies for causal effects},
  journal   = {Biometrika},
  volume    = {70},
  number    = {1},
  pages     = {41--55}
}

@Article{rosenbaum1983central,
  author    = {Rosenbaum, Paul R. and Rubin, Donald B.},
  year      = {1983},
  title     = {The central role of the propensity score in observational
          studies for causal effects},
  abstract  = {The propensity score is the conditional probability of
          assignment to a particular treatment given a vector of
          observed covariates. Both large and small sample theory
          show that adjustment for the scalar propensity score is
          sufficient to remove bias due to all observed covariates.
          Applications include: (i) matched sampling on the
          univariate propensity score, which is a generalization of
          discriminant matching, (ii) multivariate adjustment by
          subclassification on the propensity score where the same
          subclasses are used to estimate treatment effects for all
          outcome variables and in all subpopulations, and (iii)
          visual representation of multivariate covariance adjustment
          by a two- dimensional plot. 10.1093/biomet/70.1.41},
  doi       = {10.1093/biomet/70.1.41},
  journal   = {Biometrika},
  number    = {1},
  pages     = {41--55},
  volume    = {70}
}

@Book{rosenbaum:2002,
  author    = {Rosenbaum, Paul R.},
  year      = {2002},
  title     = {Observational Studies},
  abstract  = {An Observational study is an empiric investigation of the
          effects caused by a treatment, policy , or intervention in
          which it is not possible to assign subjects at random to
          treatment or control, as would be done in a controlled
          experiment. Observational studies are common in most fields
          that study the effects of treatments on people. The second
          edition of ?Observational Studies? is about 50 percent
          longer than the first edition, with many new examples and
          methods. There are new chapters on nonadditive models for
          treatment effects (Chapter 5) and planning observational
          studies (Chapter 11) and Chapter 9, on coherence, has been
          extensively rewritten. Paul R. Rosenbaum is Robert G.
          Putzel Professor, Department of Statistics, The Wharton
          School of the University of Pennsylvania. He is a fellow of
          the American Statistical Association.},
  edition   = {2nd},
  howpublished  = {Hardcover},
  isbn      = {0387989676},
  publisher = {Springer}
}

@book{rosner:2010,
    author = {Rosner, Bernard},
    edition = {7},
    howpublished = {Hardcover},
    isbn = {0538733497},
    publisher = {Cengage Learning},
    title = {Fundamentals of Biostatistics},
    year = {2010}
}

@InCollection{rothenberg:1984:hbook,
  author    = {Rothenberg, Thomas J. },
  editor    = {Griliches, Z. and Intriligator, M. D. },
  year      = {1984},
  title     = {Approximating the distributions of econometric estimators
          and test statistics},
  booktitle = {Handbook of Econometrics},
  volume    = {1}
}

@InBook{rothenberg:2005,
  author    = {Rothenberg, Thomas J.},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {Incredible Structural Inference},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  comment   = {Two stories on some quite strange conclusions regarding
          identification and inference: (i) measuring the dimensions
          of a rectangular table with error -- if the error variance
          is unknown, it better be large! (ii) regression with
          normally distributed regressors allows one to get away with
          pretty much no assumptions on errors!

          The examples would make perfect brain teasers for the first
          class in structural inference and identification.},
  publisher = {Cambridge University Press}
}

@Article{roy:2003,
  author    = {Roy, Jason},
  year      = {2003},
  title     = {Modeling Longitudinal Data with Nonignorable Dropouts
          Using a Latent Dropout Class Model},
  abstract  = {In longitudinal studies with dropout, pattern-mixture
          models form an attractive modeling framework to account for
          nonignorable missing data. However, pattern-mixture models
          assume that the components of the mixture distribution are
          entirely determined by the dropout times. That is, two
          subjects with the same dropout time have the same
          distribution for their response with probability one. As
          that is unlikely to be the case, this assumption made lead
          to classification error. In addition, if there are certain
          dropout patterns with very few subjects, which often occurs
          when the number of observation times is relatively large,
          pattern-specific parameters may be weakly identified or
          require identifying restrictions. We propose an alternative
          approach, which is a latent-class model. The dropout time
          is assumed to be related to the unobserved (latent) class
          membership, where the number of classes is less than the
          number of observed patterns; a regression model for the
          response is specified conditional on the latent variable.
          This is a type of shared-parameter model, where the shared
          "parameter" is discrete. Parameter estimates are obtained
          using the method of maximum likelihood. Averaging the
          estimates of the conditional parameters over the
          distribution of the latent variable yields estimates of the
          marginal regression parameters. The methodology is
          illustrated using longitudinal data on depression from a
          study of HIV in women.},
  doi       = {10.2307/3695322},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {4},
  pages     = {829--836},
  publisher = {International Biometric Society},
  volume    = {59}
}

@Article{roy:daniels:2008,
  author    = {Roy and Jason and Daniels and Michael, J.},
  year      = {2008},
  title     = {A General Class of Pattern Mixture Models for Nonignorable
          Dropout with Many Possible Dropout Times},
  doi       = {10.1111/j.1541-0420.2007.00884.x},
  issn      = {0006-341X},
  journal   = {Biometrics},
  number    = {2},
  pages     = {538--545},
  publisher = {Blackwell Publishing},
  volume    = {64}
}

@Article{roy:lin:2002,
  author    = {Roy, Jason and Lin, Xihong},
  year      = 2002,
  title     = {Analysis of Multivariate Longitudinal Outcomes With
          Nonignorable Dropouts and Missing Covariates: {C}hanges in
          Methadone Treatment Practices},
  journal   = {Journal of the American Statistical Association},
  volume    = 97,
  number    = 457,
  pages     = {40--52}
}

@Article{royall:cumb:1978,
  author    = {R. M. Royall and W. G. Cumberland},
  year      = {1978},
  title     = {Variance Estimation in Finite Population Sampling},
  journal   = {Journal of the American Statistical Association},
  volume    = {73},
  number    = {362},
  pages     = {351--358}
}

@article{royston:2005,
    author = "Royston, P.",
    title = "Multiple imputation of missing values: update",
    journal = "Stata Journal",
    publisher = "Stata Press",
    address = "College Station, TX",
    volume = "5",
    number = "2",
    year = "2005",
    pages = "188--201",
    comment = "http://www.stata-journal.com/article.html?article=st0067_1"
}


@Article{rubi:1992,
  author    = {Rubin, Donald B.},
  year      = 1992,
  title     = {Computational Aspects of Analysing Random
          Effects/longitudinal Models},
  journal   = {Statistics in Medicine},
  volume    = 11,
  pages     = {1809--1821}
}

@Article{rubin:1976,
  author    = {Rubin, Donald B. },
  year      = {1976},
  title     = {Inference and missing data},
  doi       = {10.1093/biomet/63.3.581},
  journal   = {Biometrika},
  number    = {3},
  pages     = {581--592},
  volume    = {63},
  abstract  = {When making sampling distribution inferences about the
          parameter of the data, {theta}, it is appropriate to ignore
          the process that causes missing data if the missing data
          are missing at random' and the observed data are observed
          at random', but these inferences are generally conditional
          on the observed pattern of missing data. When making
          direct-likelihood or Bayesian inferences about {theta}, it
          is appropriate to ignore the process that causes missing
          data if the missing data are missing at random and the
          parameter of the missing data process is distinct' from
          {theta}. These conditions are the weakest general
          conditions under which ignoring the process that causes
          missing data always leads to correct inferences.
          10.1093/biomet/63.3.581}
}

@article{little:1988,
    abstract =
        {Useful properties of a general-purpose imputation method for
        numerical data are suggested and discussed in the context of several
        large government surveys. Imputation based on predictive mean
        matching is proposed as a useful extension of methods in existing
        practice, and versions of the method are presented for unit
        nonresponse and item nonresponse with a general pattern of
        missingness. Extensions of the method to provide multiple imputations
        are also considered. Pros and cons of weighting adjustments are
        discussed, and weighting-based analogs to predictive mean matching
        are outlined.},
    author = {Little, Roderick J. A.},
    doi = {10.1080/07350015.1988.10509663},
    journal = {Journal of Business and Economic Statistics},
    number = {3},
    pages = {287--296},
    title = {Missing-Data Adjustments in Large Surveys},
    volume = {6},
    year = {1988}
}

@article{rubin:1996,
    author = {Rubin, Donald B.},
    doi = {10.1080/01621459.1996.10476908},
    journal = {Journal of the American Statistical Association},
    number = {434},
    pages = {473--489},
    title = {Multiple Imputation after 18+ Years},
    volume = {91},
    year = {1996},
    abstract =
        {Multiple imputation was designed to handle the problem of missing
        data in public-use data bases where the data-base constructor and the
        ultimate user are distinct entities. The objective is valid frequency
        inference for ultimate users who in general have access only to
        complete-data software and possess limited knowledge of specific
        reasons and models for nonresponse. For this situation and objective,
        I believe that multiple imputation by the data-base constructor is
        the method of choice. This article first provides a description of
        the assumed context and objectives, and second, reviews the multiple
        imputation framework and its standard results. These preliminary
        discussions are especially important because some recent commentaries
        on multiple imputation have reflected either misunderstandings of the
        practical objectives of multiple imputation or misunderstandings of
        fundamental theoretical results. Then, criticisms of multiple
        imputation are considered, and, finally, comparisons are made to
        alternative strategies.},
}

@Article{rubin:bleuer:kratina:2005,
  author    = {Rubin-Bleuer, Susana and Kratina, Ioana S. },
  year      = {2005},
  title     = {On the two-phase framework for joint model and
          design-based inference},
  comment   = {10.1214/009053605000000651},
  journal   = {Annals of Statistics},
  number    = {6},
  pages     = {2789--2810},
  volume    = {33}
}

@Article{rubin:thayer:1982,
  author    = {Rubin, Donald and Thayer, Dorothy },
  year      = {1982},
  title     = {{EM} algorithms for {ML} factor analysis},
  abstract  = {The details of EM algorithms for maximum likelihood factor
          analysis are presented for both the exploratory and
          confirmatory models. The algorithm is essentially the same
          for both cases and involves only simple least squares
          regression operations; the largest matrix inversion
          required is for aq ?q symmetric matrix whereq is the
          matrix of factors. The example that is used demonstrates
          that the likelihood for the factor analysis model may have
          multiple modes that are not simply rotations of each other;
          such behavior should concern users of maximum likelihood
          factor analysis and certainly should cast doubt on the
          general utility of second derivatives of the log likelihood
          as measures of precision of estimation.},
  doi       = {http://dx.doi.org/10.1007/BF02293851},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {69--76},
  volume    = {47}
}

@article{rust:kalton:1987,
    author = {Rust, Keith and Kalton, Graham},
    journal = {Journal of Official Statistics},
    number = {1},
    pages = {69--81},
    title = {Strategies for collapsing strata for variance estimation},
    volume = {3},
    year = {1987}
}

@Article{rust:rao:1996,
  author    = {Rust, K. F. and Rao, J. N. },
  year      = {1996},
  title     = {Variance estimation for complex surveys using replication
          techniques},
  address   = {Westat, Inc., Rockville, MD 20850-3129, USA.},
  journal   = {Statistical Methods in Medical Research},
  number    = {3},
  pages     = {283--310},
  comment   = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\&db=pubmed\&dopt=Abstract\&list_uids=8931197}
          ,
  volume    = {5},
  abstract  = {The analysis of survey data requires the application of
          special methods to deal appropriately with the effects of
          the sample design on the properties of estimators and test
          statistics. The class of replication techniques represents
          one approach to handling this problem. This paper discusses
          the use of these techniques for estimating sampling
          variances, and the use of such variance estimates in
          drawing inferences from survey data. The techniques of the
          jackknife, balanced repeated replication (balanced
          half-samples), and the bootstrap are described, and the
          properties of these methods are summarized. Several
          examples from the literature of the use of replication in
          analysing large complex surveys are outlined.}
}

@BOOK{ryan:2013,
  AUTHOR =       {Thomas P. Ryan},
  TITLE =        {Sample Size Determination and Power},
  PUBLISHER =    {Wiley},
  YEAR =         {2013},
  address =      {Hoboken, NJ},
  isbn =         {978-1118437605},
}


@Article{saikkonen:1989,
  author    = {Saikkonen, Pentti},
  year      = {1989},
  title     = {Asymptotic relative efficiency of the classical test
          statistics under misspecification},
  abstract  = {Asymptotic properties of the three classical test
          statistics -- the likelihood ratio, Lagrange multiplier
          (efficient score), and Wald statistics -- are studied in
          the case of two types of misspecification. The first
          misspecification is that the alternative used to construct
          the test is different from the true nonnull model. In the
          second case the alternative hypothesis is correct, but some
          of the nuisance parameters satisfy restrictions which are
          not taken into account in the test. Pitman's local
          alternatives and the concept of asymptotic relative
          efficiency are used to derive the asymptotic distributions
          of the test statistics and to examine the effects of the
          misspecifications. The general results of the paper are
          illustrated by an example on the dynamic specification of a
          regression model.},
  doi       = {10.1016/0304-4076(89)90058-4},
  journal   = {Journal of Econometrics},
  number    = {3},
  pages     = {351--369},
  volume    = {42}
}

@Article{samp:gutt:1992,
  author    = "P. Sampson and P. Guttorp",
  year      = 1992,
  title     = "Nonparametric estimation of nonstationary spatial
          covariance matrix",
  journal   = "Journal of the American Statistical Association",
  volume    = 87,
  pages     = "108--119"
}

@ARTICLE{sankaran:1963,
  AUTHOR =       {Sankaran, M.},
  TITLE =        {Approximations to the non-central chi-square distribution},
  JOURNAL =      {Biometrika},
  YEAR =         {1963},
  volume =       {50},
  number =       {1--2},
  pages =        {199--204},
}

@article{sarndal:2007,
    author = {S{\"{a}}rndal, Carl-Erik},
    journal = {Survey Methodology},
    number = {2},
    pages = {99--119},
    title = {The calibration approach in survey theory and practice},
    volume = {33},
    year = {2007}
}

@article{sbjrh:2005,
   author = {Sanchez, Brisa N and Budtz-Jorgensen, Esben and Ryan, Louise M and Hu, Howard},
   title = {Structural Equation Models: A Review With
            Applications to Environmental Epidemiology},
   journal = {Journal of the American Statistical Association},
   volume = {100},
   number = {472},
   pages = {1443-1455},
   year = {2005},
   doi = {10.1198/016214505000001005},
}

@Article{sand:wret:wald:1988,
  author    = {Sandstrom, Arne and Wretman, Jan H. and Walden, Bertil},
  year      = {1988},
  title     = {Variance Estimators of the {G}ini Coefficient: Probability
          Sampling},
  abstract  = {An estimator of the Gini coefficient (the well-known
          income inequality measure) of a finite population is
          defined for an arbitrary probability sampling design,
          taking the sampling design into consideration. Alternative
          estimators of the variance of the estimated Gini
          coefficient are introduced. The sampling performance of the
          Gini coefficient estimator and its variance estimators is
          studied by means of a Monte Carlo study, using stratified
          sampling from a miniature population of Swedish households
          with authentic income data.},
  doi       = {10.2307/1391424},
  issn      = {07350015},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {1},
  pages     = {113--119},
  publisher = {American Statistical Association},
  volume    = {6}
}

@Article{sargan:1958,
  author    = {Sargan, J. D. },
  year      = {1958},
  title     = {The Estimation of Economic Relationships using
          Instrumental Variables},
  journal   = {Econometrica},
  number    = {3},
  pages     = {393--415},
  volume    = {26}
}

@InCollection{saris:2001,
  author    = {Willem E. Saris},
  editor    = {Robert Cudeck and Stephen du Toit and Dag S{\"o}rbom},
  year      = {2001},
  title     = {Measurement models in sociology and political science},
  booktitle = {Structural Equation Modeling: Present and Future},
  publisher = {SSI Software International},
  address   = {Lincolnwood, IL}
}

@InCollection{saris:andrews:1991,
  title={Evaluation of measurement instruments using a structural modeling approach},
  author={Saris, Willem E and Andrews, Frank M},
  booktitle={Measurement errors in surveys},
  pages={575--597},
  year={1991},
  publisher={John Wiley and Sons},
  editor={Paul P. Biemer and Robert M. Groves and Lars Lyberg and Nancy Mathiowetz and Seymour Sudman},
  address={New York}
}

@InCollection{saris:satorra:1993,
  author    = {Saris, Willem E. and Satorra, Albert},
  year      = {1993},
  title     = {Power Evaluations in Structural Equation Models},
  booktitle = {Testing Structural Equation Models},
  editors   = {Kenneth A Bollen and J Scott Long},
  publisher = {SAGE},
  address   = {Newbury Park, California},
}

@Article{saris:satorra:sorbom:1987,
  author    = {Saris, Willem E. and Satorra, Albert and S{\"o}rbom, Dag },
  year      = {1987},
  title     = {The Detection and Correction of Specification Errors in
          Structural Equation Models},
  comment   = {10.2307/271030},
  journal   = {Sociological Methodology},
  pages     = {105--129},
  volume    = {17},
  abstract  = {Detection of specification errors in structural equation
          models normally requires two steps. The first step
          establishes whether (further) corrections of the model are
          necessary, and the second detects the restrictions that
          have to be corrected. The $\chi^2$ test is frequently used
          in the first step, even though this test has been severely
          criticized. The modification index introduced by Joreskog
          and Sorbom in the LISREL program is commonly used in the
          second step. In this paper, we show that both procedures
          have serious problems and that the suggestions for
          corrections are not necessarily correct. Both procedures
          are affected not only by the size of the misspecification
          in the model but also by the kind and position of the
          misspecification and by other characteristics of the model.
          Thus, some small errors in the model may be detected and
          corrected while large errors go undetected. We propose a
          new procedure to overcome this problem.}
}

@Article{sarndal:1984,
  author    = {S{\"a}rndal, Carl E. },
  year      = {1984},
  title     = {Design-Consistent Versus Model-Dependent Estimation for
          Small Domains},
  journal   = {Journal of the American Statistical Association},
  number    = {387},
  pages     = {624--631},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28198409%2979%3A387%3C624%3ADVMEFS%3E2.0.CO%3B2-3}
          ,
  volume    = {79},
  abstract  = {For small domains, we have a choice between
          design-consistent estimators that make the strongest
          possible use of auxiliary information and model-dependent
          design-biased alternatives such as the synthetic estimator.
          In this article, a design-consistent method is developed
          that borrows strength in the same way that the
          design-biased synthetic estimator does. The latter has a
          design-based mean squared error (MSE) advantage in small
          samples under the supposition that domains resemble each
          other. Under deviations from that model, however, and with
          moderate to large samples, the design-consistent method
          gives a smaller MSE. It also has the advantage that
          design-based variance estimators and design-based
          confidence intervals can be easily obtained. This article
          emphasizes that the choice of estimation method depends on
          a complex interaction of factors that include sample size,
          sampling fraction, domain size, and departures from the
          model.}
}

@Book{sarndal:swens:wret:1992,
  author    = "Carl-Erik S{\"a}rndal and Bengt Swensson and Jan Wretman",
  year      = 1992,
  title     = "Model Assisted Survey Sampling",
  publisher = "Springer",
  address   = "New York"
}

@Article{sato:1987,
  author    = {Sato, Manabu},
  year      = {1987},
  title     = {Pragmatic Treatment of Improper Solutions in Factor
          Analysis},
  journal   = {Annals of the Institute of Statistics and Mathematics,
          part B},
  volume    = {39},
  pages     = {443--455}
}

@article{satorra:saris:1985,
    author = {Satorra, Albert and Saris, Willem E.},
    day = {27},
    doi = {10.1007/BF02294150},
    issn = {0033-3123},
    journal = {Psychometrika},
    month = {March},
    number = {1},
    pages = {83--90},
    title = {Power of the likelihood ratio test in covariance structure analysis},
    volume = {50},
    year = {1985},
    abstract =
       {A procedure for computing the power of the likelihood ratio
       test used in the context of covariance structure analysis is
       derived. The procedure uses statistics associated with the
       standard output of the computer programs commonly used and
       assumes that a specific alternative value of the parameter
       vector is specified. Using the noncentral Chi-square
       distribution, the power of the test is approximated by the
       asymptotic one for a sequence of local alternatives. The
       procedure is illustrated by an example. A Monte Carlo
       experiment also shows how good the approximation is for a
       specific case.},
}

@Article{satorra:1989,
  author    = {Satorra, Albert },
  year      = {1989},
  title     = {Alternative test criteria in covariance structure
          analysis: A unified approach},
  doi       = {10.1007/BF02294453},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {131--151},
  volume    = {54}
}

@Article{satorra:1990,
  author    = {Albert Satorra},
  year      = {1990},
  title     = {Robustness Issues in Structural Equation Modeling: A
          Review of Recent Developments},
  journal   = {Quality and Quantity},
  volume    = {24},
  pages     = {367--386}
}

@article{satorra:saris:de:pijper:1991,
    author = {Satorra, A. and Saris, W. E. and de Pijper, W. M.},
    doi = {10.1111/j.1467-9574.1991.tb01302.x},
    journal = {Statistica Neerlandica},
    number = {2},
    pages = {173--185},
    publisher = {Blackwell Publishing Ltd},
    title = {A comparison of several approximations to the power function of the likelihood ratio test in covariance structure analysis},
    volume = {45},
    year = {1991},
    abstract =
       {In this paper we compare alternative asymptotic
       approximations to the power of the likelihood ratio test used
       in covariance structure analysis for testing the fit of a
       model. Alternative expressions for the noncentrality
       parameter (ncp) lead to different approximations to the power
       function. It appears that for alternative covariance matrices
       close to the null hypothesis, the alternative ncp's lead to
       similar values, while for alternative covariance matrices far
       from Ho the different expressions for the ncp can conflict
       substantively. Monte Carlo evidence shows that the ncp
       proposed in Satorra and Saris (1985) gives the most accurate
       power approximations.},
}

@Article{satorra:1992,
  author    = {Albert Satorra},
  year      = {1992},
  title     = {Asymptotic Robust Inference in the Analysis of Mean and
          Covariance Structures},
  journal   = {Sociological Methodology},
  volume    = {22},
  pages     = {249--278}
}

@Article{satorra:1992:cov,
  author    = {Satorra, Albert},
  year      = {1992},
  title     = {The variance matrix of sample second-order moments in
          multivariate linear relations},
  doi       = {10.1016/0167-7152(92)90286-E},
  journal   = {Statistics \& Probability Letters},
  number    = {1},
  pages     = {63--69},
  volume    = {15},
  abstract  = {We derive an expression for the variance matrix of the
          vector of (uncentered) sample second-order moments under
          multivariate linear relations and an independence
          assumption. An application of the result is presented.}
}

@Article{satorra:1992:robust:sem,
  author    = {Satorra, Albert},
  year      = {1992},
  title     = {Asymptotic Robust Inferences in the Analysis of Mean and
          Covariance Structures},
  abstract  = {Structural equation models are widely used in economic,
          social, and behavioral studies to analyze linear
          interrelationships among variables, some of which may be
          unobservable or subject to measurement error. Alternative
          estimation methods that exploit different distributional
          assumptions are now available. This paper deals with issues
          of asymptotic statistical inferences, such as the
          evaluation of standard errors of estimates and chisquare
          goodness-of-fit statistics, in the general context of mean
          and covariance structures. The emphasis is on drawing
          correct statistical inferences regardless of the
          distribution of the data and the method of estimation
          employed. A simple expression for a consistent estimate
          (given any distribution of the data) of ?, the matrix of
          asymptotic variances of the vector of sample second-order
          moments, is used to compute asymptotic robust standard
          errors and an asymptotic robust chi-square goodness-of-fit
          statistic. Simple modifications of the usual estimate of ?
          also permit correct asymptotic inferences in multistage
          complex samples. We also discuss the conditions under
          which, regardless of the distribution of the data, one can
          rely on the usual (nonrobust) normal-theory inferential
          statistics. Finally, we use a multivariate regression model
          with errors-in-variables to illustrate, by means of
          simulated data, various theoretical aspects of the paper.},
  chapter   = {8},
  journal   = {Sociological Methodology},
  pages     = {249--278},
  volume    = {22}
}

@InCollection{satorra:2000,
  author    = {Albert Satorra},
  editor    = {Heijmans, R.D.H. and Pollock, D.S.G. and Satorra, A.},
  year      = {2000},
  title     = {Scaled and adjusted restricted tests in multi-sample
          analysis of moment structures},
  booktitle = {Innovations in multivariate statistical analysis. A
          Festschrift for Heinz Neudecker},
  pages     = {233--247},
  publisher = {Kluwer Academic Publishers},
  address   = {London}
}

@Article{satorra:2002,
  author    = {Satorra, Albert },
  year      = {2002},
  title     = {Asymptotic Robustness in Multiple Group Linear-Latent
          Variable Models},
  doi       = {http://dx.doi.org/10.1017/S0266466602182041},
  journal   = {Econometric Theory},
  number    = {02},
  pages     = {297--312},
  volume    = {18}
}

@Article{satorra:bentler:1990,
  author    = {Albert Satorra and Peter M. Bentler},
  year      = {1990},
  title     = {Model conditions for asymptotic robustness in the analysis
          of linear relations},
  journal   = {Computational Statistics and Data Analysis},
  volume    = {10},
  number    = {3},
  pages     = {235--249},
  note      = {doi:10.1016/0167-9473(90)90004-2},
  abstract  = {In covariance structure analysis, alternative methods of
          estimation are now regularly available. A variety of
          statistics, such as estimators, test statistics, and
          residuals, are computed. The sampling variability of these
          statistics is known to depend on a matrix $\Gamma$ which is
          based on the fourth-order moments of the data. Estimates of
          these fourth-order moments are expensive to compute,
          require a lot of computer storage, and have high sampling
          variability in small to moderate samples. By exploiting the
          linear relations that typically generate the covariance
          structure, we have developed conditions under which a
          matrix $\Gamma^*$, which depends only on second-order
          moments of the data, can be used as a substitute for
          $\Gamma$ to obtain correct asymptotic distributions for the
          statistics of interest. In contrast to related work on
          asymptotic robustness in covariance structure analysis, our
          theory is developed in the general setting of arbitrary
          discrepancy functions and addresses a broader class of
          statistics that include, for instance, goodness of fit
          statistics that are not necessarily asymptotically $\chi^2$
          distributed, and statistics based on the residuals.
          Basically, our theory shows that the normal theory form
          $\Gamma^*$ for $\Gamma$ can be used whenever an
          independence assumption (not only uncorrelatedness), which
          will always hold under normality, carries over to the model
          with nonnormal variables. This theory is spelled out in
          sufficient detail and simplicity so that it can be used in
          every day practice.}
}

@InCollection{satorra:bentler:1994,
  author    = {Satorra, Albert and Bentler, Peter M.},
  editor    = {{von Eye}, Alexander and Clogg, Clifford C.},
  title     = {Corrections to Test Statistics and Standard Errors in
          Covariance Structure Analysis},
  booktitle = {Latent Variable Analysis},
  year      = {1994},
  chapter   = {16},
  pages     = {399--419},
  comment   = {Satorra and Bentler (1994) derive corrections for test
          statistics and standard errors taking into account
          non-normality of the data.},
  publisher = {Sage},
  address   = {Thousands Oaks, CA}
}

@Article{satorra:bentler:2001,
  author    = {Satorra, Albert and Bentler, Peter M.},
  year      = {2001},
  title     = {A scaled difference chi-square test statistic for moment
          structure analysis},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {507--514},
  comment   = {http://dx.doi.org/10.1007/BF02296192},
  volume    = {66}
}

@Article{satterthwaite:1946,
  author    = {Satterthwaite, F. E. },
  year      = {1946},
  title     = {An Approximate Distribution of Estimates of Variance Components},
  journal   = {Biometrics Bulletin},
  number    = {6},
  pages     = {110--114},
  comment   = {http://links.jstor.org/sici?sici=0099-4987%28194612%292%3A6%3C110%3AAADOEO%3E2.0.CO%3B2-A},
  volume    = {2}
}

@Article{savalei:2008,
  author    = {Savalei, Victoria},
  year      = {2008},
  title     = {Is the {ML} Chi-Square Ever Robust to Nonnormality? {A}
          Cautionary Note With Missing Data},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {1},
  pages     = {1--22},
  publisher = {Psychology Press},
  volume    = {15},
  abstract  = {Normal theory maximum likelihood (ML) is by far the most
          popular estimation and testing method used in structural
          equation modeling (SEM), and it is the default in most SEM
          programs. Even though this approach assumes multivariate
          normality of the data, its use can be justified on the
          grounds that it is fairly robust to the violations of the
          distributional assumptions under some conditions. In
          support of this claim, a large literature exists outlining
          conditions under which the ML chi-square retains its
          asymptotic distribution even under nonnormality. The most
          important of these conditions is specifying a model in
          which the exogenous variables presumed to be uncorrelated
          (e.g., factors and errors in a confirmatory factor analysis
          model) are also statistically independent. The goal of this
          article is to point out that these conditions were
          developed for complete data, and in fact no longer ensure
          robustness when the data are both nonnormal and incomplete.
          This lack of robustness is illustrated both mathematically
          and empirically. Violation becomes more severe when the
          data are highly nonnormal and when a higher proportion of
          data is missing. It is concluded that if the proportion of
          missing data is greater than about 10\%, robustness of the
          ML chi-square with incomplete nonnormal data cannot be
          counted on, even if the necessary assumptions such as
          independence are made. Other approaches to model testing
          are to be preferred in this case.}
}

@Article{savalei:2010,
  author    = {Victoria Savalei},
  year      = {2010},
  title     = {Expected versus observed information in {SEM} with incomplete
          normal and nonnormal data},
  journal   = {Psychological Methods},
  volume    = 15,
  number    = 4,
  pages     = {352--367},
  doi       = {10.1037/a0020143},
}

@article{savalei:rhemtulla:2012,
    abstract =
        {Fraction of missing information $\lambda_j$ is a useful measure of the
        impact of missing data on the quality of estimation of a particular
        parameter. This measure can be computed for all parameters in the
        model, and it communicates the relative loss of efficiency in the
        estimation of a particular parameter due to missing data. It has been
        recommended that researchers working with incomplete data sets
        routinely report this measure, as it is more informative than percent
        missing data (Bodner, 2008; Schafer, 1997). However, traditional
        estimates of $\lambda_j$ require the implementation of multiple imputation
        ({MI}). Many researchers prefer to fit structural equation models
        using full information maximum likelihood rather than {MI}. This
        article demonstrates how to obtain an estimate of $\lambda_j$ using maximum
        likelihood estimation routines only and argues that this estimate is
        superior to the estimate obtained via {MI} when the number of
        imputations is small. Interpretation of $\lambda_j$ is also addressed.},
    author = {Savalei, Victoria and Rhemtulla, Mijke},
    doi = {10.1080/10705511.2012.687669},
    journal = {Structural Equation Modeling: A Multidisciplinary Journal},
    number = {3},
    pages = {477--494},
    title = {On Obtaining Estimates of the Fraction of Missing Information From Full Information Maximum Likelihood},
    volume = {19},
    year = {2012}
}

@article{savalei:falk:2014,
    abstract =
        {This article builds on the work of Savalei and Bentler (2009), who
        proposed and evaluated a statistically justified two-stage ({TS})
        approach for fitting structural equation models with incomplete
        normally distributed data. The {TS} approach first obtains saturated
        maximum likelihood ({ML}) estimates of the population means and
        covariance matrix and then uses these saturated estimates in the
        complete data {ML} fitting function. Standard errors and test
        statistics are then adjusted to reflect uncertainty due to missing
        data. This work presents an extension of the {TS} methodology to
        nonnormal incomplete data (robust {TS}) and conducts an empirical
        evaluation of its performance relative to the full information
        maximum likelihood ({FIML}) approach with robust standard errors and
        a scaled chi-square statistic. The results indicate that although
        {TS} parameter estimates are slightly lower in efficiency, the {TS}
        approach performs better than {FIML} in terms of coverage and the
        rejection rate of the scaled chi-square across a wide variety of
        conditions. Its wide implementation and further study are
        encouraged.},
    author = {Savalei, Victoria and Falk, Carl F.},
    doi = {10.1080/10705511.2014.882692},
    journal = {Structural Equation Modeling: A Multidisciplinary Journal},
    number = {2},
    pages = {280--302},
    title = {Robust {Two-Stage} Approach Outperforms Robust Full Information Maximum Likelihood With Incomplete Nonnormal Data},
    volume = {21},
    year = {2014}
}

@Article{savalei:kolenikov:2008,
  author    = {Victoria Savalei and Stanislav Kolenikov},
  year      = {2008},
  title     = {Constrained vs.\ unconstrained estimation in structural
          equation modeling},
  journal   = {Psychological Methods},
  pages     = {150--170},
  volume    = {13}
}

@Book{schabenberger2004statistical,
  author    = {Schabenberger, Oliver and Gotway, Carol A.},
  year      = {2004},
  title     = {Statistical Methods for Spatial Data Analysis (Texts in
          Statistical Science Series)},
  abstract  = {{Statistical Methods for Spatial Data Analysis is a
          comprehensive treatment of statistical theory and methods
          for spatial data analysis, employing a model-based and
          frequentist approach that emphasizes the spatial domain.
          The volume delivers an up-to-date treatment of
          semivariogram estimation and modeling and spatial analysis
          in the spectral domain, as well as a thorough analysis of
          spatial regression, covering linear models with
          uncorrelated errors, linear models with
          spatially-correlated errors and generalized linear mixed
          models for spatial data and succinctly discussing Bayesian
          hierarchical models. It concludes with a review of
          simulation, non-stationary covariance and spatio-temporal
          processes. }},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {1584883227},
  publisher = {Chapman \& Hall/CRC}
}

@Book{schafer:1997,
  author    = {Joe L. Schafer},
  year      = {1997},
  title     = {Analysis of Incomplete Multivariate Data},
  publisher = {Chapman \& Hall/CRC}
}

@Article{scharfstein:rotnitzky:robins:1999,
  author    = {Scharfstein, Daniel O. and Rotnitzky, Andrea and Robins,
          James M.},
  year      = {1999},
  title     = {Adjusting for Nonignorable Drop-Out Using Semiparametric
          Nonresponse Models: Rejoinder},
  doi       = {10.2307/2669930},
  issn      = {01621459},
  journal   = {Journal of the American Statistical Association},
  number    = {448},
  pages     = {1135--1146},
  publisher = {American Statistical Association},
  volume    = {94}
}

@Book{scheaffer:mendenhall:ott:2006,
  author    = {Richard L. Scheaffer and William {III} Mendenhall and R.
          Lyman Ott },
  year      = {2006},
  title     = {Elementary Survey Sampling},
  isbn      = {0534418058},
  publisher = {{Duxbury Press}},
  edition   = {6th}
}

@Article{scheines1999bayesian,
  author    = {Scheines, Richard and Hoijtink, Herbert and Boomsma,
          Anne},
  year      = {1999},
  title     = {Bayesian estimation and testing of structural equation
          models},
  abstract  = {Abstract\&nbsp;\&nbsp;The Gibbs sampler can be used to
          obtain samples of arbitrary size from the posterior
          distribution over the parameters of a structural equation
          model (SEM) given covariance data and a prior distribution
          over the parameters. Point estimates, standard deviations
          and interval estimates for the parameters can be computed
          from these samples. If the prior distribution over the
          parameters is uninformative, the posterior is proportional
          to the likelihood, and asymptotically the inferences based
          on the Gibbs sample are the same as those based on the
          maximum likelihood solution, for example, output from
          LISREL or EQS. In small samples, however, the likelihood
          surface is not Gaussian and in some cases contains local
          maxima. Nevertheless, the Gibbs sample comes from the
          correct posterior distribution over the parameters
          regardless of the sample size and the shape of the
          likelihood surface. With an informative prior distribution
          over the parameters, the posterior can be used to make
          inferences about the parameters underidentified models, as
          we illustrate on a simple errors-in-variables model.},
  doi       = {10.1007/BF02294318},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {37--52},
  volume    = {64}
}

@Article{scheines:hoijtink:boomsma:1999,
  author    = {Scheines, Richard and Hoijtink, Herbert and Boomsma, Anne
          },
  year      = {1999},
  title     = {Bayesian estimation and testing of structural equation
          models},
  doi       = {10.1007/BF02294318},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {37--52},
  volume    = {64}
}

@article{schlier:2008,
   title   = "On scrambled Halton sequences",
   journal = "Applied Numerical Mathematics",
   volume  = "58",
   number  = "10",
   pages   = "1467--1478",
   year    = "2008",
   doi     = "10.1016/j.apnum.2007.09.001",
   author  = "Christoph Schlier",
}



@Article{schm:1996,
  author    = {Schmid, Christopher H.},
  year      = 1996,
  title     = {An {EM} Algorithm Fitting First-order Conditional
          Autoregressive Models to Longitudinal Data},
  journal   = {Journal of the American Statistical Association},
  volume    = 91,
  pages     = {1322--1330}
}

@ARTICLE{schouten:cobben:bethlehem:2009,
  AUTHOR =       {Barry Schouten and Fannie Cobben and Jelke Bethlehem},
  TITLE =        {Indicators for the representativeness of survey response},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2009},
  volume =       {35},
  number =       {1},
  pages =        {101--113},
}


@Book{schum:marc:1998,
  editor    = {Randall E. Schumacker and George A. Marcoulides},
  year      = {1998},
  title     = {Interaction and Nonlinear Effects in Structural Equation
          Modeling},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, New Jersey}
}

@Article{schwarz:1978,
  author    = "Gideon Schwarz",
  year      = 1978,
  title     = "Estimating the dimension of a model",
  journal   = "Annals of Statistics",
  volume    = 6,
  pages     = "461--464"
}

@Article{sclove:morris:rad:1972,
  author    = {S. L. Sclove and C. Morris and R. Radhakrishnan},
  year      = {1972},
  title     = {Non-optimality of preliminary-test estimators for the mean
          of a multivariate normal distribution},
  journal   = {Annals of Mathematical Statistics},
  volume    = {43},
  pages     = {1481--1490}
}

@article{scott:holt:1982,
     title = {The Effect of Two-Stage Sampling on Ordinary Least Squares Methods},
     author = {Scott, A. J. and Holt, D.},
     journal = {Journal of the American Statistical Association},
     volume = {77},
     number = {380},
     pages = {848--854},
     comment = {http://www.jstor.org/stable/2287317},
     year = {1982},
     abstract =
        {We look at the effect of intracluster correlation on
        standard procedures in linear regression. The ordinary least
        squares estimator of the coefficient vector
        performs well in most cases but the usual estimator of
        <tex-math>$\operatorname{cov}(\hat{\beta})$</tex-math> and
        procedures based on this such as confidence intervals and
        hypothesis tests can be seriously misleading. The size of
        the effect, however, tends to be smaller than the
        corresponding effect on the variance of an estimated mean in
        two-stage sampling provided that the cluster sample sizes
        are approximately equal.},
}

@Article{scott:1998,
  author    = "Charles T. Scott",
  year      = "1998",
  title     = "Sampling Methods for Estimating Change in Forest
          Resources",
  journal   = "Ecological Applications",
  volume    = "8",
  number    = "2",
  pages     = "228--233"
}

@Book{scott:2000,
  author    = {John P. Scott},
  year      = {2000},
  title     = {Social Network Analysis: A Handbook},
  publisher = {SAGE},
  address   = {Thousand Oaks, CA}
}

@TechReport{scott:amen:1990,
  author    = {Scott, C. and B. Amenuvegbe},
  year      = {1990},
  title     = {Effect of Recall Duration on Reporting Household
          Expenditures: An Experimental Study in {G}hana},
  institution   = {The World Bank},
  type      = {Social Dimensions of Adjustment in Africa Working Paper},
  number    = {6}
}

@InCollection{scott:wild:2003,
  author    = {Alastair Scott and Chris Wild},
  editor    = {R. Chambers and C. J. Skinner},
  year      = {2003},
  title     = {Fitting Logistic Regression Models in Case-Control Studies
          with Complex Sampling},
  booktitle = {Analysis of Survey Data},
  publisher = {John Wiley and Sons},
  chapter   = {8}
}

@Book{seber:lee:2003,
  author    = {George A. F. Seber and Alan J. Lee},
  year      = {2003},
  title     = {Linear Regression Analysis},
  publisher = {Wiley},
  address   = {Hoboken, NJ},
  edition   = {2nd}
}

@article{seid:west:moser:2023,
  author  = {Seidenberg, A. and West, B.T. and Moser, R.},
  title   = {Preferred Reporting Items for Complex Sample Survey Analysis ({PRICSSA})},
  journal = {Journal of Survey Statistics and Methodology},
  year    = 2023,
  note    = {in press}
}

@Article{self:liang:1987,
  author    = {Self, Steven G. and Liang, Kung-Yee },
  year      = {1987},
  title     = {Asymptotic Properties of Maximum Likelihood Estimators and
          Likelihood Ratio Tests Under Nonstandard Conditions},
  journal   = {Journal of the American Statistical Association},
  number    = {398},
  pages     = {605--610},
  volume    = {82},
  abstract  = {Large sample properties of the likelihood function when
          the true parameter value may be on the boundary of the
          parameter space are described. Specifically, the asymptotic
          distribution of maximum likelihood estimators and
          likelihood ratio statistics are derived. These results
          generalize the work of Moran (1971), Chant (1974), and
          Chernoff (1954). Some of Chant's results are shown to be
          incorrect. The approach used in deriving these results
          follows from comments made by Moran and Chant. The problem
          is shown to be asymptotically equivalent to the problem of
          estimating the restricted mean of a multivariate Gaussian
          distribution from a sample of size 1. In this
          representation the Gaussian random variable corresponds to
          the limit of the normalized score statistic and the
          estimate of the mean corresponds to the limit of the
          normalized maximum likelihood estimator. Thus the limiting
          distribution of the maximum likelihood estimator is the
          same as the distribution of the projection of the Gaussian
          random variable onto the region of admissible values for
          the mean. A variety of examples is provided for which the
          limiting distributions of likelihood ratio statistics are
          mixtures of chi-squared distributions. One example is
          provided with a nuisance parameter on the boundary for
          which the asymptotic distribution is not a mixture of
          chi-squared distributions.}
}

@article{sen:1976,
  author    = "Amartya Kumar Sen",
  year      = "1976",
  title     = "Poverty: An Ordinal Approach to Measurement",
  journal   = {Econometrica},
  number    = {2},
  pages     = {219--231},
  volume    = {44},
}


@Book{sen:1997,
  author    = "Amartya Kumar Sen",
  year      = "1997",
  title     = "On Economic Inequality",
  series    = "Radcliffe Lectures",
  publisher = "Oxford University Press",
  edition   = "2nd",
  address   = "Oxford, UK"
}

@Book{sen:foster:1997,
  author    = {Sen, Amartya and Foster, James },
  year      = {1997},
  title     = {On Economic Inequality},
  publisher = {Oxford University Press},
  series    = {Radcliffe Lectures}
}

@Book{serfling1981approximation,
  author    = {Serfling, Robert J.},
  year      = {1981},
  title     = {Approximation Theorems of Mathematical Statistics (Wiley
          Series in Probability and Statistics)},
  abstract  = {{This paperback reprint of one of the best in the field
          covers a broad range of limit theorems useful in
          mathematical statistics, along with methods of proof and
          techniques of application. The manipulation of
          "probability" theorems to obtain "statistical" theorems is
          emphasized.}},
  howpublished  = {Hardcover},
  isbn      = {0471024031},
  publisher = {Wiley-Interscience}
}

@Book{serfling:1980,
  author    = {Robert J. Serfling},
  year      = {1980},
  title     = {Approximation Theorems of Mathematical Statistics},
  publisher = {John Wiley and Sons},
  address   = {New York}
}

@Article{shao:wu:1992,
  author    = {Shao, Jun and Wu, C. F. J.},
  year      = {1992},
  title     = {Asymptotic Properties of the Balanced Repeated Replication
          Method for Sample Quantiles},
  abstract  = {Inference, including variance estimation, can be made from
          stratified samples by selecting a balanced set of
          subsamples. This balanced subsampling method is generically
          called the balanced repeated replication method in survey
          data analysis, which includes McCarthy's balanced
          half-samples method and its extensions for more general
          stratified designs. We establish the asymptotic consistency
          of the balanced repeated replication variance estimators
          when the parameter of interest is the population quantile.
          The consistency results also hold when balanced subsampling
          is replaced by random subsampling. As a key technical
          prerequisite, we prove a Bahadur-type representation for
          sample quantiles in stratified random sampling.},
  journal   = {The Annals of Statistics},
  number    = {3},
  pages     = {1571--1593},
  volume    = {20}
}

@Article{shao:1996,
  author    = {Jun Shao},
  year      = {1996},
  title     = {Resampling Methods in Sample Surveys (with discussion)},
  journal   = {Statistics},
  volume    = {27},
  pages     = {203--254}
}

@Article{shao:2003,
  author    = {Shao, Jun },
  year      = {2003},
  title     = {Impact of the Bootstrap on Sample Surveys},
  journal   = {Statistical Science},
  pages     = {191--198},
  comment   = {http://projecteuclid.org/DPubS?service=UI\&version=1.0\&verb=Display\&handle=euclid.ss/1063994974},
  volume    = {18},
  abstract  = {This article discusses the impact of the bootstrap on
          sample surveys and introduces some of the main developments
          of the bootstrap methodology for sample surveys in the last
          twenty five years.}
}

@Article{shao:chen:1998,
  author    = {Shao, Jun and Chen, Yinzhong},
  year      = {1998},
  title     = {BOOTSTRAPPING SAMPLE QUANTILES BASED ON COMPLEX SURVEY
          DATA UNDER HOT DECK IMPUTATION},
  abstract  = {The bootstrap method works for both smooth and nonsmooth
          statistics, and replaces theoretical derivations by routine
          computations. With survey data sampled using a stratified
          multistage sampling design, the consistency of the
          bootstrap variance estimators and bootstrap confidence
          intervals was established for smooth statistics such as
          functions of sample means (Rao and Wu (1988)). However,
          similar results are not available for nonsmooth statistics
          such as the sample quantiles and the sample low income
          proportion. We consider a more complicated situation where
          the data set contains nonrespondents imputed using a random
          hot deck method. We establish the consistency of the
          bootstrap procedures for the sample quantiles and the
          sample low income proportion. Some empirical results are
          also presented.},
  journal   = {Statistica Sinica},
  pages     = {1071--1085},
  volume    = {8}
}

@Article{shao:sitter:1996,
  author    = {Shao, Jun and Sitter, Randy R. },
  year      = {1996},
  title     = {Bootstrap for Imputed Survey Data},
  journal   = {Journal of the American Statistical Association},
  number    = {435},
  pages     = {1278--1288},
  comment   = {http://links.jstor.org/sici?sici=0162-1459%28199609%2991%3A435%3C1278%3ABFISD%3E2.0.CO%3B2-G}
          ,
  volume    = {91},
  abstract  = {Most surveys use imputation to compensate for missing
          data. However, treating the imputed data set as the
          complete data set and directly applying existing methods
          (e.g., the linearization, the jackknife, and the bootstrap)
          for variance estimation and/or statistical inference does
          not produce valid results, because these methods do not
          account for the effect of missing data and/or imputation.
          In this article we show that correct bootstrap estimates
          can be obtained by imitating the process of imputing the
          original data set in the bootstrap resampling; that is, by
          imputing the bootstrap data sets in exactly the same way
          that the original data set is imputed. The proposed
          bootstrap is asymptotically valid irrespective of the
          sampling design, the imputation method, or the type of
          statistic used in inference. This enables us to use a
          unified method in a variety of problems, and in fact this
          is the only method that works without any restriction on
          the sampling design, the imputation method, or the type of
          statistic.}
}

@Book{shao:tu:1995,
  author    = {Jun Shao and Dongsheng Tu},
  year      = {1995},
  title     = {The Jackknife and Bootstrap},
  publisher = {Springer},
  address   = {New York},
  isbn      = {0387945156}
}

@Book{shao:tu:1996,
  author    = {Shao, Jun and Tu, Dongsheng},
  year      = {1995},
  title     = {The Jackknife and Bootstrap (Springer Series in
          Statistics)},
  abstract  = {{The Jackknife and bootstrap are the most popular
          data-resampling methods used in statistical analysis. This
          book provides a systematic introduction to the theory of
          the jackknife, bootstrap and other resampling methods that
          have been developed in the last twenty years. It aims to
          provide a guide to using these methods which will enable
          applied statisticians to feel comfortable in applying them
          to data in their own research. The authors have included
          examples of applying these methods in various applications
          in both the independent and identically distributed (iid)
          case and in more complicated cases with non-iid data sets.
          Readers are assumed to have a reasonable knowledge of
          mathematical statistics and so this will be made suitable
          reading for graduate students, researchers and
          practitioners seeking a wide-ranging survey of this
          important area of statistical theory and application.}},
  howpublished  = {Hardcover},
  isbn      = {0387945156},
  publisher = {Springer}
}

@Article{shao:wu:1992,
  author    = {Shao, Jun and Wu, C. F. J. },
  year      = {1992},
  title     = {Asymptotic Properties of the Balanced Repeated Replication
          Method for Sample Quantiles},
  journal   = {The Annals of Statistics},
  number    = {3},
  pages     = {1571--1593},
  comment   = {http://links.jstor.org/sici?sici=0090-5364%28199209%2920%3A3%3C1571%3AAPOTBR%3E2.0.CO%3B2-X}
          ,
  volume    = {20},
  abstract  = {Inference, including variance estimation, can be made from
          stratified samples by selecting a balanced set of
          subsamples. This balanced subsampling method is generically
          called the balanced repeated replication method in survey
          data analysis, which includes McCarthy's balanced
          half-samples method and its extensions for more general
          stratified designs. We establish the asymptotic consistency
          of the balanced repeated replication variance estimators
          when the parameter of interest is the population quantile.
          The consistency results also hold when balanced subsampling
          is replaced by random subsampling. As a key technical
          prerequisite, we prove a Bahadur-type representation for
          sample quantiles in stratified random sampling.}
}

@Article{shapiro:1983,
  author    = "Alexander Shapiro",
  year      = 1985,
  title     = "Asymptotic Distribution Theory in the Analysis
               of Covariance Structures (a unified approach)",
  journal   = "South African Statistical Journal",
  volume    = 17,
  pages     = "33--81"
}

@Article{shapiro:1985,
  author    = "Alexander Shapiro",
  year      = 1985,
  title     = "Asymptotic Distribution of Test Statistic in the Analysis
          of Moment Structures Under Inequality Constraints",
  journal   = "Biometrika",
  volume    = 72,
  number    = 1,
  pages     = "133--144"
}

@Article{shapiro:1988,
  author    = {Shapiro, Alexander },
  year      = {1988},
  title     = {Towards a Unified Theory of Inequality Constrained Testing
          in Multivariate Analysis},
  abstract  = { In this paper a distributional theory of test statistics
          in various problems of multivariate analysis involving
          inequality constraints is examined. A unified point of view
          based on geometrical properties of convex cones is
          presented. Chi-bar-squared and E-bar-squared test
          statistics are introduced. Their applications to hypothesis
          testing problems are discussed. },
  journal   = {International Statistical Review},
  number    = {1},
  pages     = {49--62},
  volume    = {56}
}

@article{sheather:jones:1991,
     title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
     author = {Sheather, S. J. and Jones, M. C.},
     journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
     volume = {53},
     number = {3},
     pages = {683--690},
     year = {1991},
    }

@Article{shimizu:kano:2008,
  author    = {Shimizu, S. and Kano, Y.},
  year      = {2008},
  title     = {Use of non-normality in structural equation modeling:
          Application to direction of causation},
  abstract  = {Structural equation modeling (SEM) typically utilizes
          first- and second-order moment structures. This limits its
          applicability since many unidentified models and many
          equivalent models that researchers would like to
          distinguish are created. In this paper, we relax this
          restriction and assume non-normal distributions on
          exogenous variables. We shall provide a solution to the
          problems of underidentifiability and equivalence of SEM
          models by making use of non-normality (higher-order moment
          structures). The non-normal SEM is applied to finding the
          possible direction of a path in simple regression models.
          The method of (generalized) least squares is employed to
          estimate model parameters. A test statistic for examining a
          fit of a model is proposed. A simulation result and a real
          data example are reported to study how the non-normal SEM
          approach works empirically.},
  doi       = {10.1016/j.jspi.2006.01.017},
  issn      = {03783758},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {11},
  pages     = {3483--3491},
  volume    = {138}
}

@Book{ship:2000,
  author    = {Bill Shipley},
  year      = {2000},
  title     = {Cause and correlation in Biology: A user's guide to path
          analysis, structural equations and causal inference},
  publisher = {Cambridge Unversity Press},
  address   = {Cambridge, UK}
}

@BOOK{shoemaker:1972,
  author =       {Shoemaker, D.M.},
  title =        {Principles and Procedures of Multiple Matrix Sampling},
  publisher =    {Ballinger Publishing Company},
  year =         {1973},
  address =      {Cambridge, Massachusetts},
}

@article{shor:bafumi:keele:park:2007,
    abstract =
        {The analysis of time-series cross-sectional ({TSCS}) data has become
        increasingly popular in political science. Meanwhile, political
        scientists are also becoming more interested in the use of multilevel
        models ({MLM}). However, little work exists to understand the
        benefits of multilevel modeling when applied to {TSCS} data. We
        employ Monte Carlo simulations to benchmark the performance of a
        Bayesian multilevel model for {TSCS} data. We find that the {MLM}
        performs as well or better than other common estimators for such
        data. Most importantly, the {MLM} is more general and offers
        researchers additional advantages.},
    author = {Shor, Boris and Bafumi, Joseph and Keele, Luke and Park, David},
    doi = {10.1093/pan/mpm006},
    journal = {Political Analysis},
    number = {2},
    pages = {165--181},
    title = {A {Bayesian} Multilevel Modeling Approach to Time-Series Cross-Sectional Data},
    volume = {15},
    year = {2007}
}

@Article{shor:1980,
  author    = "Anthony F. Shorrocks",
  year      = 1980,
  title     = "The Class of Additively Decomposable Inequality Measures",
  journal   = "Econometrica",
  volume    = 48,
  number    = 3,
  pages     = "613--626"
}

@Unpublished{shor:1999,
  author    = "Anthony F. Shorrocks",
  year      = 1999,
  title     = "Decomposition Procedures for Distributional Analysis: A
          Unified Framework Based on the Shapley Value",
  note      = "Mimeo, University of Essex"
}

@Book{shor:hoev:2005,
  editor    = "Anthony Shorrocks and Rolph van der Hoeven",
  year      = 2005,
  title     = "Growth, Inequality, and Poverty Prospects for Pro-poor
          Economic Development",
  publisher = "Oxford University Press",
  series    = "WIDER Studies in Development Economics"
}

@Article{shorrocks:1984,
  author    = {Shorrocks, Anthony F. },
  year      = {1984},
  title     = {Inequality Decomposition by Population Subgroups},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1369--1385},
  volume    = {52}
}

@Book{siegel:cast:1988,
  author    = "S. Siegel and N. J. Castellan",
  year      = 1988,
  title     = "Nonparametric Statistics for the Behavioral Sciences",
  edition   = "2nd",
  publisher = "McGraw-Hill",
  address   = "New York"
}

@Article{siemsen:bollen:2007,
  author    = {Siemsen, Enno and Bollen, Kenneth A. },
  year      = {2007},
  title     = {Least Absolute Deviation Estimation in Structural Equation
          Modeling},
  doi       = {10.1177/0049124107301946},
  journal   = {Sociological Methods Research},
  number    = {2},
  pages     = {227--265},
  volume    = {36},
  abstract  = {Least absolute deviation (LAD) is a well-known criterion
          to fit statistical models, but little is known about LAD
          estimation in structural equation modeling (SEM). To
          address this gap, the authors use the LAD criterion in SEM
          by minimizing the sum of the absolute deviations between
          the observed and the model-implied covariance matrices.
          Using Monte Carlo simulations, the authors compare the
          performance of this LAD estimator along several dimensions
          (bias, efficiency, convergence, frequencies of improper
          solutions, and absolute percentage deviation) to the full
          information maximum likelihood (ML) and unweighted least
          squares (ULS) estimators in structural equation modeling.
          The results for LAD are mixed: There are special conditions
          under which the LAD estimator outperforms ML and ULS, but
          the simulation evidence does not support a general claim
          that LAD is superior to ML and ULS in small samples.
          10.1177/0049124107301946}
}

@Book{silva:sen:2004,
  author    = {Silvapulle, Mervyn J. and Sen, Pranab K. },
  year      = {2004},
  title     = {Constrained Statistical Inference: Inequality, Order, and
          Shape Restrictions},
  isbn      = {0471208272},
  publisher = {Wiley-Interscience},
  address   = {New York}
}

@Book{silva:sen:2005,
  author    = {Silvapulle, Mervyn J. and Sen, Pranab K.},
  year      = {2004},
  title     = {Constrained Statistical Inference: Inequality, Order, and
          Shape Restrictions},
  abstract  = {This volumes focuses on the theory of statistical
          inference under inequality constraints, providing a unified
          and up-to-date treatment of the methodology. The scope of
          applications of the presented methodology and theory in
          different fields is clearly illustrated by using examples
          from several areas, especially sociology, econometrics, and
          biostatistics. The authors also discuss a broad range of
          other inequality constrained inference problems, which do
          not fit well in the contemplated unified framework,
          providing meaningful access to comprehend methodological
          resolutions.},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {0471208272},
  publisher = {Wiley-Interscience}
}

@article{silverman:1984,
 abstract =
    {The spline smoothing approach to nonparametric regression and curve
    estimation is considered. It is shown that, in a certain sense, spline
    smoothing corresponds approximately to smoothing by a kernel method with
    bandwidth depending on the local density of design points. Some exact
    calculations demonstrate that the approximation is extremely close in
    practice. Consideration of kernel smoothing methods demonstrates that the
    way in which the effective local bandwidth behaves in spline smoothing
    has desirable properties. Finally, the main result of the paper is
    applied to the related topic of penalized maximum likelihood probability
    density estimates; a heuristic discussion shows that these estimates
    should adapt well in the tails of the distribution.},
 author = {B. W. Silverman},
 journal = {The Annals of Statistics},
 number = {3},
 pages = {898-916},
 title = {Spline Smoothing: The Equivalent Variable Kernel Method},
 volume = {12},
 year = {1984}
}



@Article{silvey:1959,
  author    = {Silvey, S. D.},
  year      = {1959},
  title     = {The Lagrangian Multiplier Test},
  journal   = {The Annals of Mathematical Statistics},
  number    = {2},
  pages     = {389--407},
  volume    = {30}
}

@Article{singh:1968,
  author    = "Singh, D.",
  year      = "1968",
  title     = "Estimates in Successive Sampling Using a Multi-Stage
          Design",
  journal   = "Journal of the American Statistical Association",
  volume    = "63",
  number    = "321",
  pages     = "99--112"
}

@Article{singh:singh:1977,
  author    = {Singh, D. and Singh, Padam},
  year      = {1977},
  title     = {New Systematic Sampling},
  journal   = {Journal of Statistical Planning and Inference},
  pages     = {163--177},
  volume    = {1}
}

@article{singh:mohl:1996,
    author = {Singh, Avi and Mohl, Chris},
    journal = {Survey Methodology},
    number = {2},
    pages = {107--115},
    title = {Understanding calibration estimators in survey sampling},
    volume = {22},
    year = {1996}
}

@Article{sitter:1992,
  author    = {Sitter, R. R. },
  year      = {1992},
  title     = {A Resampling Procedure for Complex Survey Data},
  doi       = {10.2307/2290213},
  journal   = {Journal of the American Statistical Association},
  number    = {419},
  pages     = {755--765},
  volume    = {87}
}

@Article{sitter:1992:3bs,
  author    = {Sitter, R. R.},
  year      = {1992},
  title     = {Comparing Three Bootstrap Methods for Survey Data},
  doi       = {10.2307/3315464},
  issn      = {03195724},
  journal   = {The Canadian Journal of Statistics},
  number    = {2},
  pages     = {135--154},
  publisher = {Statistical Society of Canada},
  volume    = {20}
}

@Article{sitter:1993,
  author    = {Sitter, R. R. },
  year      = {1993},
  title     = {Balanced repeated replications based on orthogonal
          multi-arrays},
  doi       = {10.1093/biomet/80.1.211},
  journal   = {Biometrika},
  number    = {1},
  pages     = {211--221},
  volume    = {80},
  abstract  = {To obtain variance estimates in stratified sampling with
          possibly unequal numbers of primary sampling units per
          stratum, one can either randomly divide the units in each
          stratum into two groups and apply the balanced half-samples
          method to these groups, or use mixed orthogonal arrays to
          form a set of balanced subsamples. With the first
          technique, resulting variance estimates can be very
          inefficient, and with the second technique it can be
          difficult to construct orthogonal arrays with a small
          number of replicates for a given stratified design. The
          proposed extension of the orthogonal array method to
          orthogonal multi-arrays retains all of the orthogonal array
          method's desirable properties but allows greater
          flexibility in construction, and thus can handle more
          complex stratified designs using fewer replicates.
          10.1093/biomet/80.1.211}
}

@Article{skin:holm:smith:1986,
  author    = "C. J. Skinner and D. J. Holmes and T. M. F. Smith",
  year      = 1986,
  title     = "The Effect of Sample Design on Principal Component
          Analysis",
  journal   = "Journal of the American Statistical Association",
  volume    = 81,
  issue     = 395,
  pages     = "789--798"
}

@Article{skinner1986effect,
  author    = {Skinner, C. J. and Holmes, D. J. and Smith, T. M. F.},
  year      = {1986},
  title     = {The Effect of Sample Design on Principal Component
          Analysis},
  abstract  = {Most sample surveys are multivariate and many lend
          themselves to multivariate methods of analysis. The most
          usual mode of such analysis is a standard statistical
          package, such as BMDP or SPSS, in which the multivariate
          analyses are based on the underlying assumption that the
          data are generated as independent observations from a
          common probability distribution. This assumption ignores
          the sample selection procedure involved in the survey,
          which leads to the following basic questions. What effects
          can the sample design have on methods of multivariate
          analysis? How should such effects be taken into account?
          This article considers the case of principal component
          analysis and, in particular, the point estimation of the
          eigenvalues and eigenvectors of a covariance matrix. It is
          assumed that the selection of the sample depends on the
          population values of auxiliary variables as, for example,
          in stratified sampling. The conventional estimators, based
          on the assumption of simple random sampling, are compared
          with alternative probability-weighted and maximum
          likelihood estimators. Under a multivariate normal model,
          simple expressions are presented for the approximate model
          bias of the different estimators. The validity of these
          results is assessed in a simulation study involving a
          disproportionate stratified design.},
  journal   = {Journal of the American Statistical Association},
  number    = {395},
  pages     = {789--798},
  volume    = {81}
}

@Article{skinner:1986,
  author    = {Skinner, C. },
  year      = {1986},
  title     = {Regression estimation and post-stratification in factor
          analysis},
  doi       = {http://dx.doi.org/10.1007/BF02294059},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {347--356},
  volume    = {51},
  abstract  = {Regression estimation and poststratification are methods
          used in survey sampling to estimate a population mean, when
          additional information is available for some auxiliary
          variables. The extension of these methods to factor
          analysis is considered. These methods may be used either to
          improve the efficiency of estimation or to adjust for the
          effects of nonrandom selection. The estimation procedure
          may be formulated in a LISREL framework.}
}

@InCollection{skinner:1989,
  author    = {Chris J. Skinner},
  editor    = {C. J. Skinner and D. Holt and T. M. Smith},
  year      = {1989},
  title     = {Domain Means, Regression and Multivariate Analysis},
  booktitle = {Analysis of Complex Surveys},
  publisher = {Wiley},
  chapter   = {3},
  pages     = {59--88},
  address   = {New York},
  comment   = {This work introduced the concept of pseudo-MLE with
          corrections for complex designs.}
}

@Misc{skinner:2004:samsi,
  author    = {Skinner, Chris J},
  year      = {2004},
  title     = {Estimation for Covariance Structure Models with Survey
          Data: Pseudo Maximum Likelihood and Generalized Least
          Squares Approaches},
  howpublished  = {Mimeo in group on Complex Survey Data within Structural
          Equations with Latent Variables SAMSI 2004--05 Program}
}

@Article{skinner:carter:2003,
  author    = {C.J. Skinner and R.G. Carter},
  year      = {2003},
  title     = {Estimation of a Measure of Disclosure Risk for Survey
          Microdata Under Unequal Probability Sampling},
  journal   = {Survey Methodology},
  volume    = {29},
  number    = {2},
  pages     = {177--180},
  abstract  = { Skinner and Elliot (2002) proposed a simple measure of
          disclosure risk for survey microdata and showed how to
          estimate this measure under sampling with equal
          probabilities. In this paper we show how their results on
          point estimation and variance estimation may be extended to
          handle unequal probability sampling. Our approach assumes a
          Poisson sampling design. Comments are made about the
          possible impact of departures from this assumption. }
}

@Article{skinner:holmes:smith:1986,
  author    = {Skinner, C. J. and Holmes, D. J. and Smith, T. M. F. },
  year      = {1986},
  title     = {The Effect of Sample Design on Principal Component
          Analysis},
  journal   = {Journal of the American Statistical Association},
  number    = {395},
  pages     = {789--798},
  volume    = {81},
  abstract  = {Most sample surveys are multivariate and many lend
          themselves to multivariate methods of analysis. The most
          usual mode of such analysis is a standard statistical
          package, such as BMDP or SPSS, in which the multivariate
          analyses are based on the underlying assumption that the
          data are generated as independent observations from a
          common probability distribution. This assumption ignores
          the sample selection procedure involved in the survey,
          which leads to the following basic questions. What effects
          can the sample design have on methods of multivariate
          analysis? How should such effects be taken into account?
          This article considers the case of principal component
          analysis and, in particular, the point estimation of the
          eigenvalues and eigenvectors of a covariance matrix. It is
          assumed that the selection of the sample depends on the
          population values of auxiliary variables as, for example,
          in stratified sampling. The conventional estimators, based
          on the assumption of simple random sampling, are compared
          with alternative probability-weighted and maximum
          likelihood estimators. Under a multivariate normal model,
          simple expressions are presented for the approximate model
          bias of the different estimators. The validity of these
          results is assessed in a simulation study involving a
          disproportionate stratified design.}
}

@Book{skinner:holt:smith:1989,
  editor    = {C. J. Skinner and D. Holt and T. M. Smith},
  year      = {1989},
  title     = {Analysis of Complex Surveys},
  publisher = {Wiley},
  address   = {New York}
}

@TechReport{skinner:vieira:2005,
  author    = {Chris J. Skinner and Marcel de Toledo Vieira},
  year      = {2005},
  title     = {Design Effects in the Analysis of Longitudinal Survey
          Data},
  institution   = {Southampton Statistical Sciences Research Institute},
  type      = {Methodology Working Paper},
  number    = {M05/13}
}

@Article{skinner:vieira:2007,
  author    = {Chris J. Skinner and Vieira, Marcel de Toledo },
  year      = {2007},
  title     = {Variance estimation in the analysis of clustered
          longitudinal survey data},
  abstract  = {Skinner and Vieira investigate the effect of clustered
          sampling on variance estimation in longitudinal surveys.
          They present theoretical arguments and empirical evidence
          of the effects of ignoring clustering in longitudinal
          analyses, and find that these effects tend to be larger
          than for corresponding cross-sectional analyses. They also
          compare traditional survey sampling based methods to
          account for clustering in variance estimation to a
          multi-level modeling approach.},
  journal   = {Survey Methodology},
  number    = {1},
  pages     = {3--12},
  volume    = {33}
}



@article{skinner:wakefield:2017,
  title={Introduction to the design and analysis of complex survey data},
  author={Skinner, Chris and Wakefield, Jon},
  journal={Statistical Science},
  volume={32},
  number={2},
  pages={165--175},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@ARTICLE{skorohod:1977,
  AUTHOR =       {A. V. Skorohod},
  TITLE =        {On a representation of random variables},
  JOURNAL =      {Probability Theory and Applications},
  YEAR =         {1977},
  volume =       {21},
  number =       {3},
  pages =        {628--632},
  doi =          {10.1137/1121076},
}



@Article{skrondal:2000,
  author    = {Skrondal, Anders },
  year      = {2000},
  title     = {Design and Analysis of {Monte Carlo} Experiments:
          Attacking the Conventional Wisdom},
  comment   = {doi:10.1207/S15327906MBR3502_1},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {137--167},
  volume    = {35},
  abstract  = {The design and analysis of Monte Carlo experiments, with
          special reference to structural equation modelling, is
          discussed in this article. These topics merit
          consideration, since the validity of the conclusions drawn
          from a Monte Carlo study clearly hinges on these features.
          It is argued that comprehensive Monte Carlo experiments can
          be implemented on a PC if the experiments are adequately
          designed. This is especially important when investigating
          modern computer intensive methodologies like resampling and
          Markov Chain Monte Carlo methods. We are faced with three
          fundamental challenges in Monte Carlo experimentation. The
          first problem is statistical precision, which concerns the
          reliability of the obtained results. External validity, on
          the other hand, depends on the number of experimental
          conditions, and is crucial for the prospects of
          generalising the results beyond the specific experiment.
          Finally, we face the constraint on available computer
          resources. The conventional wisdom in designing and
          analysing Monte Carlo experiments embodies no explicit
          specification of meta-model for analysing the output of the
          experiment, the use of case studies or full factorial
          designs as experimental plans, no use of variance reduction
          techniques, a large number of replications, and
          "eyeballing" of the results. A critical examination of the
          conventional wisdom is presented in this article. We
          suggest that the following alternative procedures should be
          considered. First of all, we argue that it is profitable to
          specify explicit meta-models, relating the chosen
          performance statistics and experimental conditions.
          Regarding the experimental plan, we recommend the use of
          incomplete designs, which will often result in considerable
          savings. We also consider the use of common random numbers
          in the simulation phase, since this may enhance the
          precision in estimating meta-models. The use of fewer
          replications per trial, enabling us to investigate an
          increased number of experimental conditions, should also be
          considered in order to improve the external validity at the
          cost of the conventionally excessive precision.}
}

@article{skrondal:laake:2001,
    abstract =
        {Structural equation models with latent variables are sometimes
        estimated using an intuitive three-step approach, here denoted factor
        score regression. Consider a structural equation model composed of an
        explanatory latent variable and a response latent variable related by
        a structural parameter of scientific interest. In this simple example
        estimation of the structural parameter proceeds as follows: First,
        common factor models areseparately estimated for each latent
        variable. Second, factor scores areseparately assigned to each latent
        variable, based on the estimates. Third, ordinary linear regression
        analysis is performed among the factor scores producing an estimate
        for the structural parameter. We investigate the asymptotic and
        finite sample performance of different factor score regression
        methods for structural equation models with latent variables. It is
        demonstrated that the conventional approach to factor score
        regression performs very badly. Revised factor score regression,
        using Regression factor scores for the explanatory latent variables
        and Bartlett scores for the response latent variables, produces
        consistent estimators for all parameters.},
    author = {Skrondal, Anders and Laake, Petter},
    journal = {Psychometrika},
    number = {4},
    pages = {563--575},
    title = {Regression among factor scores},
    doi = {10.1007/bf02296196},
    volume = {66},
    year = {2001}
}

@Book{skrondal:rabeh:glvm:2004,
  author    = {Anders Skrondal and Sophia Rabe-Hesketh},
  year      = {2004},
  title     = {Generalized Latent Variable Modeling},
  publisher = {Chapman and Hall/CRC},
  address   = {Boca Raton, Florida}
}

@Misc{sloane:hadamard:library,
  author    = {N. J. A. Sloane},
  year      = {2004},
  title     = {A Library of {H}adamard Matrices},
  howpublished  = {available at http://research.att.com/$\sim$njas/hadamard/}
}

@Article{smit:kole:cox:2003,
  author    = "Richard Smith and Stanislav Kolenikov and Lawrence H. Cox",
  year      = "2003",
  title     = "Spatio-Temporal Modeling of {PM}$_{2.5}$ Data with Missing
          Values",
  journal   = "Journal of Geophysical Research -- Atmospheres",
  volume    = 108,
  number    = "D24",
  note      = {9004, doi:10.1029/2002JD002914}
}

@Article{smith:1994:nonregular,
  author    = {Smith, Richard L.},
  year      = {1994},
  title     = {Nonregular regression},
  abstract  = {In an earlier paper, the author has studied asymptotic
          properties of maximum likelihood estimates for a class of
          nonregular models in which the range of the distribution
          depends on unknown parameters. Examples include Weibull and
          extreme value distributions. The present paper is concerned
          with the extension of this theory to the case when
          covariates are present. The proposed solution involves
          solving a linear programming problem for the regression
          parameters, followed by maximisation of a pseudo-likelihood
          function for auxiliary parameters. Asymptotic properties
          are derived, and the results illustrated by two examples
          involving extreme value data with a trend.
          10.1093/biomet/81.1.173},
  doi       = {10.1093/biomet/81.1.173},
  journal   = {Biometrika},
  number    = {1},
  pages     = {173--183},
  volume    = {81}
}

@InProceedings{smith:1989:nonregular,
  author    = {Richard L. Smith},
  year      = {1989},
  title     = {A Survey of Nonregular Problems},
  booktitle = {Proceedings of I.S.I., 47th session},
  pages     = {353--372}
}

@Article{smith:1997:gel,
  author    = {Smith, Richard J. },
  year      = {1997},
  title     = {Alternative Semi-Parametric Likelihood Approaches to
          Generalised Method of Moments Estimation},
  doi       = {10.2307/2957959},
  journal   = {The Economic Journal},
  number    = {441},
  pages     = {503--519},
  volume    = {107},
  abstract  = {Since Hansen's (1982) seminal paper, the generalised
          method of moments (GMM) has become an increasingly
          important method for estimation and inference in
          econometrics. This paper examines alternative
          semi-parametric quasi-likelihood approaches. Essentially,
          these methods embed sample versions of the moment
          conditions used in GMM in a non-parametric quasi-likelihood
          function by use of additional parameters associated with
          these moment conditions. Specification and misspecification
          tests may be defined which are similar in nature to the
          classical tests and are first-order equivalent to the
          corresponding GMM statistics. The structure of the
          semi-parametric quasi-maximum likelihood estimator is
          explored for models estimated by instrumental variables.}
}

@Article{smith:2006,
  author    = {Smith, Richard J. },
  year      = {2007},
  title     = {Efficient information theoretic inference for conditional
          moment restrictions},
  abstract  = {The generalised method of moments estimator may be
          substantially biased in finite samples, especially so when
          there are large numbers of unconditional moment conditions.
          This paper develops a class of first-order equivalent
          semi-parametric efficient estimators and tests for
          conditional moment restrictions models based on a local or
          kernel-weighted version of the Cressie-Read power
          divergence family of discrepancies. This approach is
          similar in spirit to the empirical likelihood methods of
          Kitamura et al. [2004. Empirical likelihood-based inference
          in conditional moment restrictions models. Econometrica 72,
          1667-1714] and Tripathi and Kitamura [2003. Testing
          conditional moment restrictions. Annals of Statistics 31,
          2059-2095]. These efficient local methods avoid the
          necessity of explicit estimation of the conditional
          Jacobian and variance matrices of the conditional moment
          restrictions and provide empirical conditional
          probabilities for the observations.},
  booktitle = {'Information and Entropy Econometrics' - A Volume in Honor
          of Arnold Zellner},
  doi       = {10.1016/j.jeconom.2006.05.004},
  journal   = {Journal of Econometrics},
  number    = {2},
  pages     = {430--460},
  volume    = {138}
}

@Misc{smith:consistency,
  author    = "R. L. Smith",
  year      = 2005,
  note      = "Private communication"
}

@Unpublished{smith:envnotes,
  author    = "Richard L. Smith",
  year      = 2003,
  title     = "Environmental statistics",
  note      = "Unpublished manuscript under revision for publication as a
          book.\\ http://www.stat.unc.edu/postscript/rs/envnotes.ps"
}

@InProceedings{smith:kolen:jsm2002,
  author    = "Richard Smith and Stanislav Kolenikov",
  year      = "2002",
  title     = "Spatio-Temporal Modelling of the Longitudinal {PM}$_{2.5}$
          Data with Missing Values",
  booktitle = "2002 Proceedings of American Statistical Association"
}

@Book{smith:young:2005,
  author    = "R. L. Smith and G. A. Young",
  year      = "2005",
  title     = "Essentials of Statistical Inference",
  publisher = "Cambridge University Press"
}

@Book{smith:young:forth,
  author    = "R. L. Smith and K. D. S. Young",
  year      = "forthcoming",
  title     = "Linear regression",
  publisher = "Cambridge University Press"
}

@Article{sorbom:1989,
  author    = {S\"{o}rbom, Dag},
  year      = {1989},
  title     = {Model modification},
  doi       = {10.1007/BF02294623},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {371--384},
  volume    = {54},
  abstract  = {An analysis of empirical data often leads to a rejection
          of a hypothesized model, even if the researcher has spent
          considerable efforts in including all available information
          in the formulation of the model. Thus, the researcher must
          reformulate the model in some way, but in most instances
          there is, at least theoretically, an overwhelming number of
          possible actions that could be taken. In this paper a
          modification index will be discussed which should serve as
          a guide in the search for a better model. In statistical
          terms, the index measures how much we will be able to
          reduce the discrepancy between model and data, as defined
          by a general fit function, when one parameter is added or
          freed or when one equality constraint is relaxed. The
          modification index discussed in this paper is an
          improvement of the one incorporated in the LISREL V
          computer program in that it takes into account changes in
          all the parameters of the model when one particular
          parameter is freed.}
}

@article{speizer:bollen:2000,
    author = {Speizer, Ilene S. and Bollen, Kenneth A.},
    doi = {10.1111/j.1728-4465.2000.00163.x},
    journal = {Studies in Family Planning},
    number = {2},
    pages = {163--177},
    title = {How Well Do Perceptions of Family Planning Service Quality Correspond to Objective Measures? Evidence from Tanzania},
    volume = {31},
    year = {2000}
}

@inproceedings{srinath:2013,
    author = {Srinath, K. P.},
    publisher = {The American Statistical Association},
    booktitle = {Proceedings of Survey Research Methods Section},
    title = {Design Effects in Surveys that Require Oversampling of Certain Subpopulations},
    year = {2013},
    address = {Alexandria, VA}
}

@Manual{ssi:lisrel8,
  author    = "SSI",
  year      = 2004,
  title     = "{LISREL} software, Release 8.52 for Windows",
  organization  = "Scientific Software International",
  address   = "Lincolnwood, IL"
}

@Article{staiger1997instrumental,
  author    = {Staiger, Douglas and Stock, James H.},
  year      = {1997},
  title     = {Instrumental Variables Regression with Weak Instruments},
  abstract  = {This paper develops asymptotic distribution theory for
          single-equation instrumental variables regression when the
          partial correlations between the instruments and the
          endogenous variables are weak, here modeled as local to
          zero. Asymptotic representations are provided for various
          statistics, including two-stage least squares (TSLS) and
          limited information maximum likelihood (LIML) estimators,
          Wald statistics, and statistics testing overidentification
          and endogeneity. The asymptotic distributions are found to
          provide good approximations to sampling distributions with
          10-20 observations per instrument. The theory suggests
          concrete guidelines for applied work, including using
          nonstandard methods for construction of confidence regions.
          These results are used to interpret Angrist and Krueger's
          (1991) estimates of the returns to education: whereas TSLS
          estimates with many instruments approach the OLS estimate
          of 6\%, the more reliable LIML estimates with fewer
          instruments fall between 8\% and 10\%, with a typical 95\%
          confidence interval of (5\%, 15\%).},
  journal   = {Econometrica},
  number    = {3},
  pages     = {557--586},
  volume    = {65}
}

@Article{staiger:stock:1997,
  author    = {Staiger, Douglas and Stock, James H. },
  year      = {1997},
  title     = {Instrumental Variables Regression with Weak Instruments},
  journal   = {Econometrica},
  number    = {3},
  pages     = {557--586},
  volume    = {65},
  abstract  = {This paper develops asymptotic distribution theory for
          single-equation instrumental variables regression when the
          partial correlations between the instruments and the
          endogenous variables are weak, here modeled as local to
          zero. Asymptotic representations are provided for various
          statistics, including two-stage least squares (TSLS) and
          limited information maximum likelihood (LIML) estimators,
          Wald statistics, and statistics testing overidentification
          and endogeneity. The asymptotic distributions are found to
          provide good approximations to sampling distributions with
          10-20 observations per instrument. The theory suggests
          concrete guidelines for applied work, including using
          nonstandard methods for construction of confidence regions.
          These results are used to interpret Angrist and Krueger's
          (1991) estimates of the returns to education: whereas TSLS
          estimates with many instruments approach the OLS estimate
          of 6%, the more reliable LIML estimates with fewer
          instruments fall between 8% and 10%, with a typical 95%
          confidence interval of (5%, 15%).}
}

@Article{stapleton:2002,
  author    = {Laura M. Stapleton},
  year      = {2002},
  title     = {The Incorporation of Sample Weights Into Multilevel
          Structural Equation Models},
  journal   = {Structural Equation Modeling},
  volume    = {9},
  number    = {4},
  pages     = {475--502},
  abstract  = {Over the past decade and a half, methodologists working
          with structural equation modeling (SEM) have developed
          approaches for accommodating multilevel data. These
          approaches are particularly helpful when modeling data that
          come from complex sampling designs. However, most data sets
          that are associated with complex sampling designs also
          include observation weights, and methods to incorporate
          these sampling weights into multilevel SEM analyses have
          not been addressed. This article investigates the use of
          different weighting techniques and finds, through a
          simulation study, that the use of an effective sample size
          weight provides unbiased estimates of key parameters and
          their sampling variances. Also, a popular normalization
          technique of scaling weights to reflect the actual sample
          size is shown to produce negatively biased sampling
          variance estimates, as well as negatively biased
          within-group variance parameter estimates in the small
          group size case.},
  doi       = {doi:10.1207/S15328007SEM0904_2}
}

@Article{stapleton:2005,
  author    = {Stapleton, Laura M. and Leite, Walter L.},
  year      = {2005},
  title     = {Teacher's Corner: A Review of Syllabi for a Sample of
          Structural Equation Modeling Courses},
  abstract  = {With increases in the use of structural equation modeling
          (SEM) in the social sciences, graduate course offerings in
          this statistical technique can be expected to increase.
          Knowledge of the content of current SEM course offerings
          may provide ideas to instructors developing new courses or
          enhancing current courses. This article discusses results
          from a review of 55 SEM course syllabi, mostly from the
          disciplines of education and psychology. Areas that were
          reviewed include course content, software use, required and
          recommended readings, and techniques used to assess student
          learning. Summary descriptive statistics, examples of
          specific strategies, and a list of selected readings used
          in these courses are provided. This review is intended to
          provide information to instructors who are creating a new
          course and instructors seeking ideas to change or augment
          their courses.},
  doi       = {10.1207/s15328007sem1204\_7},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {4},
  pages     = {642--664},
  publisher = {Psychology Press},
  volume    = {12}
}

@Article{stapleton:2006,
  author    = {Laura M. Stapleton},
  year      = {2006},
  title     = {An Assessment of Practical Solutions for Structural
          Equation Modeling with Complex Sample Data},
  journal   = {Structural Equation Modeling},
  volume    = {13},
  number    = {1},
  pages     = {28--58},
  abstract  = {This article discusses 5 approaches that secondary
          researchers might use to obtain robust estimates in
          structural equation modeling analyses when using data that
          come from large survey programs. These survey programs
          usually collect data using complex sampling designs and
          estimates obtained from conventional analyses that ignore
          the dependencies in complex sample data may not be robust.
          The results from a simulation study that examined 5 methods
          of estimation under 6 types of sampling designs and
          different population conditions are shared and applied
          analysts are encouraged to consider using the pseudomaximum
          likelihood for linearization estimation of asymptotic
          covariance matrices currently available in some software
          programs for structural equation modeling analyses.},
  doi       = {doi:10.1207/s15328007sem1301_2}
}

@Article{stapleton:leite:2005,
  author    = {Stapleton, Laura M. and Leite, Walter L. },
  year      = {2005},
  title     = {Teacher's Corner: A Review of Syllabi for a Sample of
          Structural Equation Modeling Courses},
  abstract  = {With increases in the use of structural equation modeling
          (SEM) in the social sciences, graduate course offerings in
          this statistical technique can be expected to increase.
          Knowledge of the content of current SEM course offerings
          may provide ideas to instructors developing new courses or
          enhancing current courses. This article discusses results
          from a review of 55 SEM course syllabi, mostly from the
          disciplines of education and psychology. Areas that were
          reviewed include course content, software use, required and
          recommended readings, and techniques used to assess student
          learning. Summary descriptive statistics, examples of
          specific strategies, and a list of selected readings used
          in these courses are provided. This review is intended to
          provide information to instructors who are creating a new
          course and instructors seeking ideas to change or augment
          their courses.},
  doi       = {10.1207/s15328007sem1204\_7},
  journal   = {Structural Equation Modeling: A Multidisciplinary
          Journal},
  number    = {4},
  pages     = {642--664},
  volume    = {12}
}

@Manual{stata10,
  author    = "{Stata~Corp.}",
  year      = 2007,
  title     = "Stata Statistical Software: Release 10",
  address   = "College Station, TX, USA"
}

@Article{stata10:mata,
  author    = {John Doe},
  year      = {current},
  title     = {Stata 10: Review of Programming Capabilities},
  journal   = {Journal of Statistical Software}
}

@Article{stata10:svy,
  author    = {John Doe},
  year      = {current},
  title     = {Stata 10: Review of Complex Survey Data Analysis},
  journal   = {Journal of Statistical Software}
}

@Manual{stata11,
  author    = "{Stata~Corp.}",
  year      = 2009,
  title     = "Stata Statistical Software: Release 11",
  address   = "College Station, TX, USA"
}

@Manual{stata7,
  author    = "{Stata~Corp.}",
  year      = 2001,
  title     = "Stata Statistical Software: Release 7",
  address   = "College Station"
}

@Manual{stata8,
  author    = "{Stata~Corp.}",
  year      = 2003,
  title     = "Stata Statistical Software: Release 8",
  address   = "College Station"
}

@Manual{stata9,
  author    = "{Stata~Corp.}",
  year      = 2005,
  title     = "Stata Statistical Software: Release 9",
  address   = "College Station, TX, USA"
}

@Manual{stata:9:svy,
  author    = {{Stata Corp.}},
  year      = {2005},
  title     = {[SVY] Survey Data Reference Manual},
  address   = {College Station, TX},
  publisher = {Stata Press}
}

@Manual{stata:12:svy,
  author    = {{Stata Corp.}},
  year      = {2012},
  title     = {[SVY] Survey Data Reference Manual: Release 12},
  address   = {College Station, TX},
  publisher = {Stata Press}
}

@Manual{stata13,
  author    = "{Stata~Corp.}",
  year      = 2013,
  title     = "Stata Statistical Software: Release 13",
  address   = "College Station, TX, USA"
}

@Manual{stata14,
  author    = "{Stata~Corp.}",
  year      = 2015,
  title     = "Stata Statistical Software: Release 14",
  address   = "College Station, TX, USA"
}

@Manual{stata15,
  author    = "{Stata~Corp.}",
  year      = 2017,
  title     = "Stata Statistical Software: Release 15",
  address   = "College Station, TX, USA"
}

@Manual{statcan:2003:methods,
  author    = {{Statistics Canada}},
  year      = {2003},
  title     = {Survey Methods and Practices},
  note      = {Catalogue No. 12-587-XPE},
  address   = {Ottawa}
}

@Article{stee:1996,
  author    = {Steele, Brian M.},
  year      = 1996,
  title     = {A Modified {E}M Algorithm for Estimation in Generalized
          Mixed Models},
  journal   = {Biometrics},
  volume    = 52,
  pages     = {1295--1310}
}

@article{stefan:hidiroglou:2021,
  title={Small area benchmarked estimation under the basic unit level model 
         when the sampling rates are non-negligible.},
  author={Stefan, Marius and Hidiroglou, Michael A},
  journal={Survey Methodology},
  volume={47},
  number={1},
  pages={123--150},
  year={2021},
  publisher={Statistics Canada}
}

@Article{stefanski:2007,
  author    = {Stefanski, Leonard A.},
  title     = {Residual (Sur)Realism},
  abstract  = {We show how to construct multiple linear regression
          datasets with the property that the plot of residuals
          versus predicted values from the least squares fit of the
          correct model reveals a hidden image or message. In the
          full PDF version of this article, the abstract itself is
          one such plot.},
  journal   = {The American Statistician},
  number    = {2},
  pages     = {163--177},
  volume    = {61}
}

@UNPUBLISHED{steiger:lind:1980,
  AUTHOR =       {James H. Steiger and John C. Lind},
  TITLE =        {Statistically-Based Tests for the Number of Common Factors},
  NOTE =         {Paper presented at the Annual Meeting of the Psychometric Society, Iowa City, Iowa, USA.
                  Available at http://www.statpower.net/Steiger Biblio/Steiger-Lind 1980.pdf},
  year =         {1980},
}


@UNPUBLISHED{steiger:sem:handout,
  AUTHOR =       {James Steiger},
  TITLE =        {Statistical Models in Structural Equation Modeling},
  year =         {2009},
  NOTE =         {http://www.statpower.net/Content/GCM/Handouts/Statistical Models in Structural Equation Modeling.pdf},
}


@Article{steiger:1990,
  author    = {Steiger, James H.},
  year      = {1990},
  title     = {Structural Model Evaluation and Modification: An Interval
          Estimation Approach},
  doi       = {10.1207/s15327906mbr2502\_4},
  journal   = {Multivariate Behavioral Research},
  number    = {2},
  pages     = {173--180},
  publisher = {Psychology Press},
  volume    = {25}
}

@Article{steiger:2001,
  author    = {Steiger, James H.},
  year      = {2001},
  title     = {Driving Fast in Reverse: The Relationship between Software
          Development, Theory, and Education in Structural Equation
          Modeling},
  doi       = {10.2307/2670370},
  journal   = {Journal of the American Statistical Association},
  number    = {453},
  pages     = {331--338},
  publisher = {American Statistical Association},
  volume    = {96},
  abstract  = {Structural equation modeling is one of the most widely
          used statistical techniques in the social sciences,
          especially psychology. Its popularity and complexity have
          spawned a large number of "user-friendly" computer
          programs, training seminars, introductory textbooks, edited
          volumes, and an internet discussion group (SEMNET). A
          review of several introductory textbooks and an edited
          volume raises disturbing questions about the interplay
          between commercial development, statistical theory, and
          "practical" statistical education in this field.}
}

@Article{steiger:shapiro:browne:1985,
  author    = {Steiger, James and Shapiro, Alexander and Browne, Michael},
  year      = {1985},
  title     = {On the multivariate asymptotic distribution of sequential
          {C}hi-square statistics},
  doi       = {10.1007/BF02294104},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {253--263},
  volume    = {50},
  abstract  = {The multivariate asymptotic distribution of sequential
          Chi-square test statistics is investigated. It is shown
          that: (a) when sequential Chi-square statistics are
          calculated for nested models on the same data, the
          statistics have an asymptotic intercorrelation which may be
          expressed in closed form, and which is, in many cases,
          quite high; and (b) sequential Chi-squaredifference tests
          are asymptotically independent. Some Monte Carlo evidence
          on the applicability of the theory is provided.}
}

@InCollection{stein:1956,
  author    = {Stein, C.},
  year      = {1956},
  title     = {Efficient nonparametric testing and estimation},
  booktitle = {Proceedings of the Third Berkeley Symposium on
          Mathematical Statistics and Probability},
  publisher = {University of California Press},
  volume    = {1},
  address   = {Berkeley}
}

@Book{stein:1999,
  author    = "Michael L. Stein",
  year      = "1999",
  title     = "Interpolation of Spatial Data: Some Theory for Kriging",
  publisher = "Springer",
  address   = "New York"
}

@Misc{stein:2002talk,
  author    = "Michael Stein",
  year      = 2002,
  title     = "Models for Spatial-Temporal Covariances",
  note      = "Pre\-sen\-ta\-tion at SAMSI/GSP Workshop on
          Spatio-Temporal Modeling.
          http://www.cgd.ucar.edu/stats/Workshop2003/stein.pdf"
}

@TechReport{stein:chi:welty:2003,
  author    = "Michael L. Stein and Zhiyi Chi and Leah J. Welty",
  year      = 2003,
  title     = "Approximating Likelihoods for Large Spatial Datasets",
  institution   = "Department of Statistics, University of Chicago"
}

@Article{stein:chi:welty:2004,
  author    = "Michael L. Stein and Zhiyi Chi and Leah J. Welty",
  year      = 2004,
  title     = "Approximating Likelihoods for Large Spatial Datasets",
  journal   = "Journal of the Royal Statistical Society",
  series    = "B",
  volume    = 66,
  pages     = "275--296"
}

@Article{stinch:white:1998,
  author    = {Stinchcombe, Maxwell B. and White, Halbert },
  year      = {2000},
  title     = {Consistent Specification Testing with Nuisance Parameters
          Present Only under the Alternative},
  doi       = {10.1017/S0266466698143013},
  journal   = {Econometric Theory},
  number    = {03},
  pages     = {295--325},
  volume    = {14}
}

@Article{stock:watson:2002,
  author    = "James H. Stock and Mark W. Watson",
  year      = 2002,
  title     = "Forecasting Using Principal Components from a Large Number
          of Predictors",
  journal   = "Journal of the American Statistical Association",
  volume    = 97,
  issue     = 460,
  pages     = "1167--1179"
}

@InBook{stock:yogo:2005:ch5,
  author    = {Stock, James H. and Yogo, Motohiro },
  editor    = {Andrews, Donald W. K. and Stock, James H. },
  year      = {2005},
  title     = {Testing for weak instruments in linear {IV} regression},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {5},
  publisher = {Cambridge University Press},
  abstract  = {Weak instruments can produce biased IV estimators and
          hypothesis tests with large size distortions. But what,
          precisely, are weak instruments, and how does one detect
          them in practice? This paper proposes quantitative
          definitions of weak instruments based on the maximum IV
          estimator bias, or the maximum Wald test size distortion,
          when there are multiple endogenous regressors. We tabulate
          critical values that enable using the first-stage
          F-statistic (or, when there are multiple endogenous
          regressors, the Cragg-Donald (1993) statistic) to test
          whether given instruments are weak. A technical
          contribution is to justify sequential asymptotic
          approximations for IV statistics with many weak
          instruments.}
}

@InBook{stock:yogo:2005:ch6,
  author    = {Stock, James H. and Yogo, Motohiro},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {Asymptotic distributions of instrumental variables
          statistics with many instruments},
  abstract  = {This paper extends Staiger and Stock's (1997) weak
          instrument asymptotic approximations to the case of many
          weak instruments by modeling the number of instruments as
          increasing slowly with the number of observations. It is
          shown that the resulting ” many weak instrument”
          approximations can be calculated sequentially by letting
          first the sample size, then the number of instruments, tend
          to infinity. The resulting distributions are given for
          k-class estimators and test statistics.},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {6},
  publisher = {Cambridge University Press}
}

@Article{stoel:garre:dolan:wittenboer:2006,
  author    = {Reinoud D. Stoel and Francisca Galindo Garre and Conor
          Dolan and Godfried van den Wittenboer},
  year      = {2006},
  title     = {On the Likelihood Ratio test in structural equation
          modeling when parameters are subject to boundary
          constraints},
  journal   = {Psychological Methods},
  volume    = 11,
  number    = 4,
  pages     = {439--455},
}

@Article{stram:lee:1994,
  author    = {Stram, Daniel O. and Lee, Jae W. },
  year      = {1994},
  title     = {Variance Components Testing in the Longitudinal Mixed
          Effects Model},
  abstract  = { This article discusses the asymptotic behavior of
          likelihood ratio tests for nonzero variance components in
          the longitudinal mixed effects linear model described by
          Laird and Ware (1982, Biometrics 38, 963-974). Our
          discussion of the large-sample behavior of likelihood ratio
          tests for nonzero variance components is based on the
          results for nonstandard testing situations by Self and
          Liang (1987, Journal of the American Statistical
          Association 82, 605-610). },
  journal   = {Biometrics},
  number    = {4},
  pages     = {1171--1177},
  volume    = {50}
}


@ARTICLE{stratton:otoole:wetzel:2007,
  AUTHOR =       {Stratton, L. S. and O'Toole, D. M. and Wetzel, J. N.},
  TITLE =        {Are the Factors Affecting Dropout Behavior Related
                  to Initial Enrollment Intensity for College Undergraduates?},
  JOURNAL =      {Research in Higher Education},
  YEAR =         {2007},
}


@Article{stromberg:1997,
  author    = {Stromberg, Arnold J.},
  year      = {1997},
  title     = {Robust covariance estimates based on resampling},
  abstract  = {Many asymptotic covariance estimates are generated using
          uncontaminated model distributions and thus are often based
          in part on the information matrix. Such covariance
          estimators have a low breakdown point (Donoho and Huber,
          1983; Huber, 1981; Hampel et al., 1986, p. 98; Lopuhaa and
          Rousseeuw, 1991), even if the estimate itself has a high
          breakdown point. These covariance estimates will not be
          reliable when there are outliers present. As alternative
          estimates of variability for robust estimators, we consider
          using the bootstrapped or jackknifed sample covariance
          matrix. As will be shown in this paper, the bootstrapped
          sample covariance matrix can have a breakdown point of 1/n
          regardless of the breakdown point of the estimator; thus
          both bootstrapped and asymptotic covariance estimates may
          be heavily influenced by outliers even if the original
          estimate is not. This is not the case for the jackknifed
          sample covariance matrix. If an estimate is not heavily
          influenced by outliers, its jackknifed sample covariance
          estimate is not likely to be heavily influenced by
          outliers. On the other hand, the jackknife, but not the
          bootstrap, may have a low breakdown point because the
          covariance estimate can often be shifted to zero by
          shifting a few points.},
  booktitle = {Robust Statistics and Data Analysis, Part II},
  doi       = {10.1016/S0378-3758(96)00051-1},
  journal   = {Journal of Statistical Planning and Inference},
  number    = {2},
  pages     = {321--334},
  volume    = {57}
}

@Article{sugden:smith:1984,
  author    = {Sugden, R. A. and Smith, T. M. F. },
  year      = {1984},
  title     = {Ignorable and informative designs in survey sampling
          inference},
  journal   = {Biometrika},
  number    = {3},
  pages     = {495--506},
  comment   = {http://dx.doi.org/10.1093/biomet/71.3.495},
  volume    = {71},
  abstract  = {The role of the sample selection mechanism in a
          model-based approach to finite population inference is
          examined. When the data analyst has only partial
          information on the sample design then a design which is
          ignorable when known fully may become informative.
          Conditions under which partially known designs can be
          ignored are established and examined for some standard
          designs. The results are illustrated by an example used by
          Scott (1977). 10.1093/biomet/71.3.495},
}

@Article{suma:sury:sury:2007,
  author    = {Sumarto, S. and Suryadarma, D. and Suryahadi, A.},
  year      = {2007},
  title     = {Predicting consumption poverty using non-consumption
          indicators: Experiments using {I}ndonesian data},
  journal   = {Social Indicators Research},
  volume    = {81},
  number    = {3},
  pages     = {543--578}
}

@ARTICLE{tchuprow:1923,
  author =       {A. A. Tchuprow},
  title =        {On the Mathematical Expectation of the Moments of Frequency Distributions
    in the Case of Correlated Observations},
  journal =      {Metron},
  year =         {1923},
  volume =       {2},
  pages =        {646--680}
}


@Article{tenh:kuns:pulk:land:1998,
  author    = {Ten Have, Thomas R. and Kunselman, Allen R. and
          Pulkstenis, Erik P. and Landis, J. Richard},
  year      = {1998},
  title     = {Mixed Effects Logistic Regression Models for Longitudinal
          Binary Response Data with Informative Drop-Out},
  abstract  = {A shared parameter model with logistic link is presented
          for longitudinal binary response data to accommodate
          informative drop-out. The model consists of observed
          longitudinal and missing response components that share
          random effects parameters. To our knowledge, this is the
          first presentation of such a model for longitudinal binary
          response data. Comparisons are made to an approximate
          conditional logit model in terms of a clinical trial
          dataset and simulations. The naive mixed effects logit
          model that does not account for informative drop-out is
          also compared. The simulation-based differences among the
          models with respect to coverage of confidence intervals,
          bias, and mean squared error (MSE) depend on at least two
          factors: whether an effect is a between-or within-subject
          effect and the amount of between-subject variation as
          exhibited by variance components of the random effects
          distributions. When the shared parameter model holds, the
          approximate conditional model provides confidence intervals
          with good coverage for within-cluster factors but not for
          between-cluster factors. The converse is true for the naive
          model. Under a different drop-out mechanism, when the
          probability of drop-out is dependent only on the current
          unobserved observation, all three models behave similarly
          by providing between-subject confidence intervals with good
          coverage and comparable MSE and bias but poor
          within-subject confidence intervals, MSE, and bias. The
          naive model does more poorly with respect to the
          within-subject effects than do the shared parameter and
          approximate conditional models. The data analysis, which
          entails a comparison of two pain relievers and a placebo
          with respect to pain relief, conforms to the simulation
          results based on the shared parameter model but not on the
          simulation based on the outcome-driven drop-out process.
          This comparison between the data analysis and simulation
          results may provide evidence that the shared parameter
          model holds for the pain data.},
  doi       = {10.2307/2534023},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {1},
  pages     = {367--383},
  publisher = {International Biometric Society},
  volume    = {54}
}

@BOOK{templ:2017,
  author =       {Matthias Templ},
  title =        {Statistical Disclosure Control for Microdata: Methods and Applications in R},
  publisher =    {Springer},
  year =         {2017},
  address =      {New York, NY},
}

@Book{teo:khine:2009,
  editor    = {Timothy Teo and Myint Swe Khine},
  year      = {2009},
  title     = {Structural Equation Modeling in Educational Research:
          Concepts and Aplications},
  howpublished  = {Hardcover},
  isbn      = {9087907885},
  publisher = {Sense Publishers}
}

@Book{thei:1958,
  author    = {Henry Theil},
  year      = {1958},
  title     = {Economic forecasts and policy},
  publisher = {North-Holland},
  address   = {Amsterdam}
}

@Book{this:wain:2001,
  editor    = {David Thissen and Howard Wainer},
  year      = {2001},
  title     = {Test Scoring},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, New Jersey}
}

@ARTICLE{theberge:2000,
  AUTHOR =       {Alain Th{\'e}berge},
  TITLE =        {Calibration and restricted weights},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2000},
  volume =       {26},
  number =       {1},
  pages =        {99--107},
}


@Article{thomas:2007,
  author    = {K.J.A. Thomas},
  year      = {2007},
  title     = {Child mortality and socioeconomic status: An examination
          of differentials by migration status in {South Africa}},
  journal   = {International Migration Review},
  volume    = {41},
  number    = {1},
  pages     = {40--74}
}

@InCollection{thomas:stra:1995,
  author    = "D. Thomas and J. Strauss",
  year      = 1995,
  title     = "Human Resources: Empirical Modeling of Household and
          Family Decisions",
  chapter   = 34,
  booktitle = "Handbook of Development Economics",
  volume    = "3A",
  publisher = "Elsevier"
}

@Book{thompson:1992,
  author    = "Steven K. Thompson",
  year      = 1992,
  title     = "Sampling",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Book{thompson:1997,
  author    = "Mary E. Thompson",
  year      = 1997,
  title     = "Theory of Sample Surveys",
  publisher = "Chapman \& Hall/CRC",
  address   = "New York",
  series    = "Monographs on Statistics and Applied Probability",
  volume    = 74
}

@article{thompson:2010,
  title = "Simple formulas for standard errors that cluster by both firm and time",
  journal = "Journal of Financial Economics",
  volume = "in press",
  number = "",
  pages = "",
  year = "2010",
  doi = "10.1016/j.jfineco.2010.08.016",
  author = "Samuel B. Thompson",
}

@article{thom:fern:mold:2012,
    author = {Thompson, David M. and Fernald, Douglas H. and Mold, James W.},
    doi = {10.1370/afm.1347},
    journal = {The Annals of Family Medicine},
    number = {3},
    pages = {235--240},
    pmid = {22585888},
    title = {Intraclass Correlation Coefficients Typical of Cluster-Randomized Studies:
        Estimates From the {R}obert {W}ood {J}ohnson Prescription for Health Projects},
    volume = {10},
    year = {2012}
}

@article{tibshirani:1996,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}

@Article{tibs:walt:hast:2001,
  author    = "Tibshirani, R. and Walther, G. and Hastie, T.",
  year      = 2001,
  title     = "Estimating the number of clusters in a data set via the
          gap statistic",
  journal   = "Journal of the Royal Statistical Society",
  series    = "B (Statistical Methodology)",
  volume    = 63,
  number    = 2,
  pages     = "411--423"
}

@Article{tierney:1994,
  author    = {Tierney, L.},
  year      = {1994},
  title     = {{Markov} Chains for Exploring Posterior Distributions (with
          discussion)},
  journal   = {Annals of Statistics},
  volume    = {22},
  pages     = {1701--1762},
}

@article{tille:1996,
    author = {Till\'{e}, Yves},
    doi = {10.1093/biomet/83.1.238},
    journal = {Biometrika},
    number = {1},
    pages = {238--241},
    title = {An elimination procedure for unequal probability sampling without replacement},
    volume = {83},
    year = {1996},
    abstract =
         {An exact without replacement sampling procedure with fixed sample
         size and unequal inclusion probabilities is presented. The proposed
         method eliminates a unit of the population at each of the N ? n
         steps of the algorithm in order finally to retain n units. A simple
         expression is given for the joint inclusion probabilities.},
}

@Book{tille:2006,
  author    = {Yves Till{\'e}},
  year      = {2006},
  title     = {Sampling Algorithms},
  publisher = {Springer},
  address   = {New York},
  series    = {Springer Series in Statistics},
  isbn      = {0387308148}
}


@BOOK{tinto:1987,
  AUTHOR =       {Tinto, V.},
  TITLE =        {Leaving college : rethinking the causes and cures of student attrition.},
  PUBLISHER =    {University of Chicago Press},
  YEAR =         {1987},
  address =      {Chicago},
  source =       {Isabella Zaniletti},
}


@Article{toulo:pococ:babik:darby:2002,
  author    = {Touloumi, Giota and Pocock, Stuart J. and Babiker, Abdel
          G. and Darbyshire, Janet H.},
  year      = {2002},
  title     = {Impact of Missing Data Due to Selective Dropouts in Cohort
          Studies and Clinical Trials},
  doi       = {10.2307/3703407},
  issn      = {10443983},
  journal   = {Epidemiology},
  number    = {3},
  pages     = {347--355},
  publisher = {Lippincott Williams \& Wilkins},
  volume    = {13}
}

@Book{tour:rips:rasi:2000,
  author    = {Tourangeau, Roger and Rips, Lance J. and Rasinski, Kenneth
          },
  year      = {2000},
  title     = {The Psychology of Survey Response},
  isbn      = {0521576296},
  publisher = {{Cambridge University Press}}
}

@book{tour:conr:coup:2013,
    author = {Tourangeau, Roger and Conrad, Frederick G. and Couper, Mick P.},
    isbn = {0199747040},
    publisher = {Oxford University Press},
    title = {The Science of Web Surveys},
    year = {2013}
}

@article{tour:smith:1996,
    abstract =
        {This study compared three methods of collecting survey data about
        sexual behaviors and other sensitive topics: computer-assisted
        personal interviewing ({CAPI}), computer-assisted self-administered
        interviewing ({CASI}), and audio computer-assisted self-administered
        interviewing ({ACASI}). Interviews were conducted with an area
        probability sample of more than 300 adults in Cook County, Illinois.
        The experiment also compared open and closed questions about the
        number of sex partners and varied the context in which the sex
        partner items were embedded. The three mode groups did not differ in
        response rates, but the mode of data collection did affect the level
        of reporting of sensitive behaviors: both forms of
        self-administration tended to reduce the disparity between men and
        women in the number of sex partners reported. Self-admimstration,
        especially via {ACASI}, also increased the proportion of respondents
        admitting that they had used illicit drugs. In addition, when the
        closed answer options emphasized the low end of the distribution,
        fewer sex partners were reported than when the options emphasized the
        high end of the distribution; responses to the open-ended versions of
        the sex partner items generally fell between responses to the two
        closed versions.},
    author = {Tourangeau, Roger and Smith, Tom W.},
    doi = {10.1086/297751},
    journal = {Public Opinion Quarterly},
    number = {2},
    pages = {275--304},
    title = {Asking Sensitive Questions: The Impact of {Data-Collection} Mode, Question Format, and Question Context},
    volume = {60},
    year = {1996}
}

@Book{train2003discrete,
  author    = {Train, Kenneth E.},
  year      = {2003},
  title     = {Discrete Choice Methods with Simulation},
  abstract  = {{Focusing on the many advances that are made possible by
          simulation, this book describes the new generation of
          discrete choice methods. Researchers use these statistical
          methods to examine the choices that consumers, households,
          firms, and other agents make. Each of the major models is
          covered: logit, generalized extreme value, or GEV
          (including nested and cross-nested logits), probit, and
          mixed logit, plus a variety of specifications that build on
          these basics. The procedures are applicable in many fields,
          including energy, transportation, environmental studies,
          health, labor, and marketing.} {This book describes the new
          generation of discrete choice methods, focusing on the many
          advances that are made possible by simulation. Researchers
          use these statistical methods to examine the choices that
          consumers, households, firms, and other agents make. Each
          of the major models is covered: logit, generalized extreme
          value, or GEV (including nested and cross-nested logits),
          probit, and mixed logit, plus a variety of specifications
          that build on these basics. Simulation-assisted estimation
          procedures are investigated and compared, including maximum
          simulated likelihood, method of simulated moments, and
          method of simulated scores. Procedures for drawing from
          densities are described, including variance reduction
          techniques such as anithetics and Halton draws. Recent
          advances in Bayesian procedures are explored, including the
          use of the Metropolis-Hastings algorithm and its variant
          Gibbs sampling. No other book incorporates all these
          fields, which have arisen in the past 20 years. The
          procedures are applicable in many fields, including energy,
          transportation, environmental studies, health, labor, and
          marketing.}},
  howpublished  = {Paperback},
  isbn      = {0521017157},
  publisher = {{Cambridge University Press}}
}

@TechReport{train:2001,
  author    = {Kenneth Train},
  year      = 2001,
  title     = {Halton Sequences for Mixed Logit},
  institution   = {EconWPA},
  type      = {Econometrics Working Paper},
  note      = {available at
          http://ideas.repec.org/p/wpa/wuwpem/0012002.html},
  number    = {0012002}
}

@Book{train:2003,
  author    = {Train, Kenneth E. },
  year      = {2003},
  title     = {Discrete Choice Methods with Simulation},
  isbn      = {0521017157},
  publisher = {{Cambridge University Press}}
}

@article{train:wilson:2008,
  title = "Estimation on stated-preference experiments constructed from revealed-preference choices",
  journal = "Transportation Research Part B: Methodological",
  volume = "42",
  number = "3",
  pages = "191--203",
  year = "2008",
  doi = "DOI: 10.1016/j.trb.2007.04.012",
  author = "Kenneth Train and Wesley W. Wilson",
}


@Article{tucker:lewis:1973,
  author    = {Tucker, Ledyard and Lewis, Charles },
  year      = {1973},
  title     = {A reliability coefficient for maximum likelihood factor
          analysis},
  doi       = {http://dx.doi.org/10.1007/BF02291170},
  journal   = {Psychometrika},
  number    = {1},
  pages     = {1--10},
  volume    = {38},
  abstract  = {Maximum likelihood factor analysis provides an effective
          method for estimation of factor matrices and a useful test
          statistic in the likelihood ratio for rejection of overly
          simple factor models. A reliability coefficient is proposed
          to indicate quality of representation of interrelations
          among attributes in a battery by a maximum likelihood
          factor analysis. Usually, for a large sample of individuals
          or objects, the likelihood ratio statistic could indicate
          that an otherwise acceptable factor model does not exactly
          represent the interrelations among the attributes for a
          population. The reliability coefficient could indicate a
          very close representation in this case and be a better
          indication as to whether to accept or reject the factor
          solution.}
}

@Article{tukey:1949,
  author    = {John W Tukey},
  year      = {1949},
  title     = {One degree of freedom for non-addditivity},
  journal   = {Biometrics},
  volume    = {5},
  pages     = {232--242}
}

@Book{turabian:1996,
  author    = "Kate Turabian",
  year      = 1996,
  title     = "A Manual for Writers of Term Papers, Theses, and
          Dissertations",
  publisher = "University of Chicago Press",
  address   = "Chicago",
  edition   = "6th"
}

@Article{tzala:best:2009,
  author    = {Tzala, Evangelia and Best, Nicky},
  year      = {2008},
  title     = {Bayesian latent variable modelling of multivariate
          spatio-temporal variation in cancer mortality},
  abstract  = {In this article, three alternative Bayesian hierarchical
          latent factor models are described for spatially and
          temporally correlated multivariate health data. The
          fundamentals of factor analysis with ideas of space-- time
          disease mapping to provide a flexible framework for the
          joint analysis of multiple-related diseases in space and
          time with a view to estimating common and disease-specific
          trends in cancer risk are combined. The models are applied
          to area-level mortality data on six diet-related cancers
          for Greece over the 20-year period from 1980 to 1999. The
          aim of this study is to uncover the spatial and temporal
          patterns of any latent factor(s) underlying the cancer data
          that could be interpreted as reflecting some aspects of the
          habitual diet of the Greek population.
          10.1177/0962280207081243},
  doi       = {10.1177/0962280207081243},
  journal   = {Stat Methods Med Res},
  number    = {1},
  pages     = {97--118},
  volume    = {17}
}

@MANUAL{census:2014:acs,
  title =        {American Community Survey: Design and Methodology},
  author =       {{U.S. Census Bureau}},
  address =      {Washington, DC},
  year =         {2014},
  note =         {Available at: http://www2.census.gov/programs-surveys/{\allowbreak}acs/{\allowbreak}methodology/design{\_}and{\_}methodology/{\allowbreak}acs{\_}design{\_}methodology{\_}report{\_}2014.pdf},
}

@MANUAL{nsduh:2010,
  TITLE =        {National Survey on Drug Use and Health, 2010},
  author =       {{{United States Department of Health and Human Services.} {Substance
                  Abuse and Mental Health Services Administration.} {Center for Behavioral
                  Health Statistics and Quality}}},
  organization = {Inter-university Consortium for Political and Social Research [distributor]},
  address =      {Ann Arbor, MI},
  doi =          {10.3886/ICPSR32722.v1},
  year =         {2011},
  note =         {Computer file ICPSR32722-v1},
}


@Book{vaart:asymptotic,
  author    = "A. W. {van der Vaart}",
  year      = 1998,
  title     = "Asymptotic statistics",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@Article{vale:maurelli:1983,
  author    = {Vale, C. and Maurelli, Vincent},
  year      = {1983},
  title     = {Simulating multivariate nonnormal distributions},
  abstract  = {Abstract\&nbsp;\&nbsp;A method for generating multivariate
          nonnormal distributions with specified intercorrelations
          and marginal means, variances, skews, and kurtoses is
          proposed. As an example, the method is applied to the
          generation of simulated scores on three psychological tests
          administered to a single group of individuals.},
  doi       = {10.1007/BF02293687},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {465--471},
  volume    = {48}
}

@Article{valliant:1993,
  author    = {Valliant, Richard },
  year      = {1993},
  title     = {Poststratification and Conditional Variance Estimation},
  journal   = {Journal of the American Statistical Association},
  number    = {421},
  pages     = {89--96},
  volume    = {88},
  abstract  = {Poststratification estimation is a technique used in
          sample surveys to improve efficiency of estimators. Survey
          weights are adjusted to force the estimated numbers of
          units in each of a set of estimation cells to be equal to
          known population totals. The resulting weights are then
          used in forming estimates of means or totals of variables
          collected in the survey. For example, in a household survey
          the estimation cells may be based on age/race/sex
          categories of individuals, and the known totals may come
          from the most recent population census. Although the
          variance of a poststratified estimator can be computed over
          all possible sample configurations, inferences made
          conditionally on the achieved sample configuration are
          desirable. Theory and a simulation study using data from
          the U.S. Current Population Survey are presented to study
          both the conditional bias and variance of the
          poststratified estimator of a total. The linearization,
          balanced repeated replication, and jackknife variance
          estimators are also examined to determine whether they
          appropriately estimate the conditional variance.}
}

@Article{valliant:1996,
  author    = {Richard Valliant},
  year      = {1996},
  title     = {Discussion of ``{R}esampling {M}ethods in {S}ample
          {S}urveys'' by {J. Shao}},
  journal   = {Statistics},
  volume    = {27},
  pages     = {247--251}
}

@Article{valliant:brick:dever:2008,
  author    = {Valliant, Richard and Brick, J. Michael and Dever, Jill
          A.},
  year      = {2008},
  title     = {Weight Adjustments for the Grouped Jackknife Variance
          Estimator},
  journal   = {Journal of Official Statistics},
  number    = {3},
  pages     = {469--488},
  volume    = {24},
  abstract  = {The jackknife variance estimator is often implemented by
          dropping groups of units rather than a single unit at a
          time. This has the practical advantages of economizing on
          computation time and file size because a separate weight is
          appended to the analysis file for each jackknife replicate.
          If the replicate weight adjustments and the grouped
          jackknife itself are not appropriately constructed, the
          variance estimates can have some extremely pathological
          behavior when estimating totals. When the dropout groups do
          not all have exactly the same number of first-stage units,
          the standard version of the grouped jackknife may be a
          severe overestimate. This problem is most likely to arise
          in single-stage samples with a large number of first-stage
          units in many of the strata. The standard grouped jackknife
          variance estimator and two alternatives are examined for
          the situation of unequally sized groups through a
          simulation study of school districts in the 50 United
          States and the District of Columbia.}
}

@BOOK{valliant:dorfman:royall:2000,
  AUTHOR =       {Richard Valliant and Alan H. Dorfman and Richard M. Royall},
  TITLE =        {Finite Population Sampling and Inference: A Prediction Approach},
  PUBLISHER =    {John Wiley and Sons},
  YEAR =         {2000},
  address =      {New York},
}

@article{valliant:rust:2010,
    author = {Valliant, Richard and Rust, Keith F.},
    journal = {Journal of Official Statistics},
    number = {4},
    pages = {585--602},
    title = {Degrees of Freedom Approximations and Rules-of-Thumb},
    volume = {26},
    year = {2010}
}

@book{valliant:dever:kreuter:2013,
    author = {Valliant, Richard and Dever, Jill A. and Kreuter, Frauke},
    isbn = {146146448X},
    publisher = {Springer},
    title = {Practical Tools for Designing and Weighting Survey Samples},
    seroes = {Statistics for Social and Behavioral Sciences},
    year = {2013}
}

@BOOK{valliant:dever:2017,
  author =       {Richard Valliant and Jill Dever},
  title =        {Survey Weights: A Step-by-step Guide to Calculation},
  publisher =    {Stata Press},
  year =         {2017},
  address =      {College Station, TX},
}


@book{vanbuuren:2012,
    address = {Boca Raton, FL},
    author = {{van} Buuren, Stef},
    isbn = {1439868247},
    publisher = {Chapman and Hall/CRC},
    title = {Flexible Imputation of Missing Data},
    series = {Interdisciplinary Statistics},
    year = {2012}
}

@book{vanbuuren:2018,
    address = {Boca Raton, FL},
    author = {{van} Buuren, Stef},
    isbn = {1439868247},
    publisher = {Chapman and Hall/CRC},
    title = {Flexible Imputation of Missing Data},
    series = {Interdisciplinary Statistics},
    edition = {2nd},
    year = {2018}
}

@Book{vandervaart2000asymptotic,
  author    = {van der Vaart, A. W.},
  year      = {2000},
  title     = {Asymptotic Statistics (Cambridge Series in Statistical and
          Probabilistic Mathematics)},
  abstract  = {{Here is a practical and mathematically rigorous
          introduction to the field of asymptotic statistics. In
          addition to most of the standard topics of an asymptotics
          course--likelihood inference, M-estimation, the theory of
          asymptotic efficiency, U-statistics, and rank
          procedures--the book also presents recent research topics
          such as semiparametric models, the bootstrap, and empirical
          processes and their applications. The topics are organized
          from the central idea of approximation by limit
          experiments, one of the book's unifying themes that mainly
          entails the local approximation of the classical i.i.d. set
          up with smooth parameters by location experiments involving
          a single, normally distributed observation.}},
  comment   = {This is probably the most modern treatment of asymptotic
          theory. Books like Serfling derive everything from the
          first principles, while van der Vaart tries to have a
          unifying methodology of estimating equations to go through
          the whole book. My personal concern with it was that the
          results I needed on M-estimators were not given in the most
          general form, but then it has stuff of say empirical
          processes or Hajek projections that I would never be able
          to digest properly!},
  howpublished  = {Paperback},
  isbn      = {0521784506},
  publisher = {Cambridge University Press}
}

@Book{vandervaart:1998,
  author    = "{van der Vaart}, A. W.",
  year      = 1998,
  title     = "Asymptotic Statistics",
  publisher = "Cambridge University Press",
  address   = "Cambridge"
}

@article{vandewoestyne:cools:2006,
    title = "Good permutations for deterministic scrambled Halton sequences in terms of $L_2$-discrepancy",
  journal = "Journal of Computational and Applied Mathematics",
   volume = "189",
   number = "1-2",
    pages = "341--361",
     year = "2006",
  comment = "Proceedings of The 11th International Congress on Computational and Applied Mathematics",
      doi = "10.1016/j.cam.2005.05.022",
   author = "Bart Vandewoestyne and Ronald Cools",
}



@ARTICLE{van:kerm:jenkins:2001,
  AUTHOR =       {Philippe Van Kerm and Stephen P. Jenkins},
  TITLE =        {Generalized Lorenz curves and related graphs: an update for Stata 7},
  JOURNAL =      {Stata Journal},
  YEAR =         {2001},
  volume =       {1},
  number =       {1},
  pages =        {107--112},
  comment =      {http://www.stata-journal.com/article.html?article=gr0001}
}


@Article{vardi:zhang:2000,
  author    = {Vardi, Yehuda and Zhang, Cun-Hui},
  year      = {2000},
  title     = {The Multivariate \$L\_1\$-Median and Associated Data Depth},
  journal   = {Proceedings of the National Academy of Sciences of the
          USA},
  number    = {4},
  pages     = {1423--1426},
  volume    = {97}
}

@Article{vdberg:linde:1998,
  author    = {van den Berg, Gerard J. and Lindeboom, Maarten},
  year      = {1998},
  title     = {Attrition in Panel Survey Data and the Estimation of
          Multi-State Labor Market Models},
  abstract  = {In the analysis of labor market transitions, spells that
          are incomplete due to panel survey attrition are treated as
          spells that are subject to independent right-censoring. It
          is likely, however, that the unobserved characteristics
          affecting individual transitions on the labor market are
          related to those affecting the attitude toward survey
          participation. In that case, the transition rate estimates
          are inconsistent. In this paper we analyze the relation
          between durations spent in labor market states and the
          duration of panel survey participation, by explicitly
          modeling and estimating both stochastic processes. We use
          multi-state multi-spell models which allow for
          stochastically related unobserved determinants.},
  doi       = {10.2307/146437},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {458--478},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Book{vdlinden:hambleton:1996,
  editor    = {van der Linden, Wim J. and Hambleton, Ronald K.},
  year      = {1996},
  title     = {Handbook of Modern Item Response Theory},
  abstract  = {Item response theory has become an essential component in
          the toolkit of every researcher in the behavioral sciences.
          It provides a powerful means to study individual responses
          to a variety of stimuli, and the methodology has been
          extended and developed to cover many different models of
          interaction. This volume presents a wide-ranging handbook
          to item response theory - and its applications to
          educational and psychological testing. It will serve as
          both an introduction to the subject and also as a
          comprehensive reference volume for practitioners and
          researchers. It is organized into six major sections: the
          nominal categories model, models for response time or
          multiple attempts on items, models for multiple abilities
          or cognitive components, nonparametric models, models for
          nonmonotone items, and models with special assumptions.
          Each chapter in the book has been written by an expert of
          that particular topic, and the chapters have been carefully
          edited to ensure that a uniform style of notation and
          presentation is used throughout. As a result, all
          researchers whose work uses item response theory will find
          this an indispensable companion to their work and it will
          be the subject's reference volume for many years to come.},
  edition   = {1},
  howpublished  = {Hardcover},
  isbn      = {0387946616},
  publisher = {Springer}
}

@Article{verb:mole:thij:lesa:kenw:2001,
  author    = {Verbeke, Geert and Molenberghs, Geert and Thijs, Herbert
          and Lesaffre, Emmanuel and Kenward, Michael G.},
  year      = {2001},
  title     = {Sensitivity Analysis for Nonrandom Dropout: A Local
          Influence Approach},
  abstract  = {Diggle and Kenward (1994, Applied Statistics 43, 49-93)
          proposed a selection model for continuous longitudinal data
          subject to nonrandom dropout. It has provoked a large
          debate about the role for such models. The original
          enthusiasm was followed by skepticism about the strong but
          untestable assumptions on which this type of model
          invariably rests. Since then, the view has emerged that
          these models should ideally be made part of a sensitivity
          analysis. This paper presents a formal and flexible
          approach to such a sensitivity assessment based on local
          influence (Cook, 1986, Journal of the Royal Statistical
          Society, Series B 48, 133-169). The influence of perturbing
          a missing-at-random dropout model in the direction of
          nonrandom dropout is explored. The method is applied to
          data from a randomized experiment on the inhibition of
          testosterone production in rats.},
  doi       = {10.2307/2676836},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {1},
  pages     = {7--14},
  publisher = {International Biometric Society},
  volume    = {57}
}

@Article{verbeke:molen:2007,
  author    = {Verbeke, Geert and Molenberghs, Geert },
  year      = {2007},
  title     = {What Can Go Wrong With the Score Test?},
  journal   = {The American Statistician},
  number    = {4},
  pages     = {289--290},
  volume    = {61},
  abstract  = {The score test presents a number of issues. There are four
          dimensions of concern: (1) Is the parameter space
          constrained? (2) Is observed information used, or some form
          of expected information? (3) Are size and power computed
          under the null hypothesis, a correctly specified
          alternative, or a misspecified alternative? (4) Are
          computations asymptotic or small sample?}
}

@Article{vermunt:2009,
  author    = {Vermunt, Jeroen K.},
  year      = {2008},
  title     = {Latent class and finite mixture models for multilevel data
          sets},
  abstract  = {An extension of latent class (LC) and finite mixture
          models is described for the analysis of hierarchical data
          sets. As is typical in multilevel analysis, the dependence
          between lower-level units within higher-level units is
          dealt with by assuming that certain model parameters differ
          randomly across higher-level observations. One of the
          special cases is an LC model in which group-level
          differences in the logit of belonging to a particular LC
          are captured with continuous random effects. Other variants
          are obtained by including random effects in the model for
          the response variables rather than for the LCs. The variant
          that receives most attention in this article is an LC model
          with discrete random effects: higher-level units are
          clustered based on the likelihood of their members
          belonging to the various LCs. This yields a model with
          mixture distributions at two levels, namely at the group
          and the subject level. This model is illustrated with three
          rather different empirical examples. The appendix describes
          an adapted version of the expectation--maximization
          algorithm that can be used for maximum likelihood
          estimation, as well as providing setups for estimating the
          multilevel LC model with generally available software.
          10.1177/0962280207081238},
  doi       = {10.1177/0962280207081238},
  journal   = {Stat Methods Med Res},
  number    = {1},
  pages     = {33--51},
  volume    = {17}
}

@Article{villa2006using,
  author    = {Villa, Enrique R. and Escobar, Luis A.},
  year      = {2006},
  title     = {Using Moment Generating Functions to Derive Mixture
          Distributions},
  comment   = {Moment generating function arithmetics: may be a few
          A-level exercises?},
  doi       = {10.1198/000313006X90819},
  issn      = {0003-1305},
  journal   = {The American Statistician},
  number    = {1},
  pages     = {75--80},
  publisher = {American Statistical Association},
  volume    = {60}
}

@Article{viol:heck:2007,
  author    = {Violato, Claudio and Hecker, Kent G.},
  year      = {2007},
  title     = {How to use structural equation modeling in medical
          education research: a brief guide.},
  abstract  = {BACKGROUND: Structural equationmodeling (SEM) is a family
          of statistical techniques used for the analysis of
          multivariate data to measure latent variables and their
          interrelationships. SEM has potential to advance theory and
          research in medical education. PURPOSE: The purpose of this
          article is to introduce SEM to medical education
          researchers and provide procedural information for applying
          SEM. METHODS: We outline the basic tenets of SEM,
          principles of model creation, identification, estimation,
          and model fit to data, and the use of SEM in medical
          education research. RESULTS: Although it is a powerful
          statistical research tool, SEM has had only limited use in
          medical education research. We explicate a five-step
          procedure for applying SEM to research problems and
          summarize an example of SEM to test a hypothetical model.
          CONCLUSIONS: Notwithstanding some pitfalls, SEM does
          provide promise for testing complex, integrated theoretical
          models and advance research in medical education.},
  doi       = {10.1080/10401330701542685},
  issn      = {1040-1334},
  journal   = {Teaching and learning in medicine},
  number    = {4},
  pages     = {362--371},
  publisher = {Routledge},
  volume    = {19}
}

@Article{vyas:kuma:2006,
  author    = {Vyas, S. and Kumaranayake, L.},
  year      = {2006},
  title     = {Constructing socio-economic status indices: how to use
          principal components analysis},
  journal   = {Health Policy and Planning},
  volume    = {21},
  number    = {6},
  pages     = {459--468}
}

@Book{wainer:2000,
  author    = {Howard Wainer},
  year      = {2000},
  title     = {Computerized Adaptive Testing: A Primer},
  publisher = {Lawrence Erlbaum Associates},
  address   = {Mahwah, New Jersey},
  edition   = {2nd}
}

@article{waksberg:1978,
    author = {Waksberg, Joseph},
    journal = {Journal of the American Statistical Association},
    number = {361},
    pages = {40--46},
    title = {Sampling Methods for Random Digit Dialing},
    volume = {73},
    year = {1978}
}

@Article{wald:1948,
  author    = {Wald, Abraham },
  year      = {1948},
  title     = {Asymptotic Properties of the Maximum Likelihood Estimate
          of an Unknown Parameter of a Discrete Stochastic Process},
  doi       = {10.2307/2236054},
  journal   = {The Annals of Mathematical Statistics},
  number    = {1},
  pages     = {40--46},
  volume    = {19},
  abstract  = {Asymptotic properties of maximum likelihood estimates have
          been studied so far mainly in the case of independent
          observations. In this paper the case of stochastically
          dependent observations is considered. It is shown that
          under certain restrictions on the joint probability
          distribution of the observations the maximum likelihood
          equation has at least one root which is a consistent
          estimate of the parameter ? to be estimated. Furthermore,
          any root of the maximum likelihood equation which is a
          consistent estimate of ? is shown to be asymptotically
          efficient. Since the maximum likelihood estimate is always
          a root of the maximum likelihood equation, consistency of
          the maximum likelihood estimate implies its asymptotic
          efficiency.}
}

@Article{wall:amemiya:2003,
  author    = {Wall, M. M. and Amemiya, Y. },
  year      = {2003},
  title     = {A method of moments technique for fitting interaction
          effects in structural equation models.},
  doi       = {http://dx.doi.org/10.1348/000711003321645331},
  journal   = {The British journal of mathematical and statistical
          psychology},
  number    = {1},
  pages     = {47--63},
  volume    = {56},
  abstract  = {The desire to fit structural equation models containing an
          interaction term has received much methodological attention
          in the social science literature. This paper presents a
          technique for the cross-product structural model that
          utilizes factor score estimates and results in closed-form
          moments-type estimators. The technique, which does not
          require normality for the underlying factors, was
          originally introduced in a very general form by Wall and
          Amemiya (2000) for any polynomial structural model. In this
          paper, the practical implementation of this method,
          including standard error estimation, is presented
          specifically for the cross-product model. The procedure is
          applied to an example from social/behavioural epidemiology
          where the flexibility of the cross-product model provides a
          useful description of the underlying theory. A simulation
          study is also presented comparing the method of moments for
          the cross-product model with three other procedures.}
}

@article{wang:fuller:qu:2008,
  title={Small area estimation under a restriction},
  author={Wang, Junyuan and Fuller, Wayne A and Qu, Yongming},
  journal={Survey methodology},
  volume={34},
  number={1},
  pages={29},
  year={2008}
}

@article{wang:he:2007,
    author = {Wang, Huixia and He, Xuming},
    doi = {10.1198/016214506000001220},
    journal = {Journal of the American Statistical Association},
    month = {March},
    number = {477},
    pages = {104--112},
    title = {Detecting Differential Expressions in GeneChip Microarray Studies:
             A Quantile Approach},
    volume = {102},
    year = {2007},
    abstract =
       {In this article we consider testing for differentially
       expressed genes in GeneChip studies by modeling and analyzing
       the quantiles of gene expression through probe level
       measurements. By developing a robust rank score test for
       linear quantile models with a random effect, we propose a
       reliable test for detecting differences in certain quantiles
       of the intensity distributions. By using a genomewide
       adjustment to the test statistic to account for within-array
       correlation, we demonstrate that the proposed rank score test
       is highly effective even when the number of arrays is small.
       Our empirical studies with real experimental data show that
       detecting differences in the quartiles for the probe level
       data is a valuable complement to the usual mixed model
       analysis based on Gaussian likelihood. The methodology
       proposed in this article is a first attempt to develop
       inferential tools for quantile regression in mixed models.},
}

@article{wang:fygenson:2009,
    author = {Wang, Huixia J. and Fygenson, Mendel},
    doi = {10.1214/07-AOS564},
    journal = {Annals of Statistics},
    number = {2},
    pages = {756--481},
    title = {Inference for censored quantile regression models in longitudinal studies},
    volume = {37},
    year = {2009},
    abstract =
       {We develop inference procedures for longitudinal data where
       some of themeasurements are censored by fixed constants. We
       consider a semi-parametricquantile regression model that
       makes no distributional assumptions. Ourresearch is motivated
       by the lack of proper inference procedures for data
       frombiomedical studies where measurements are censored due to
       a fixedquantification limit. In such studies the focus is
       often on testing hypothesesabout treatment equality. To this
       end, we propose a rank score test for largesample inference
       on a subset of the covariates. We demonstrate the
       importanceof accounting for both censoring and intra-subject
       dependency and evaluate theperformance of our proposed
       methodology in a simulation study. We then applythe proposed
       inference procedures to data from an AIDS-related clinical
       trial.We conclude that our framework and proposed methodology
       is very valuable fordifferentiating the influences of
       predictors at different locations in theconditional
       distribution of a response variable.},
}


@ARTICLE{wang:opsomer:2010,
  AUTHOR =       {Jianqiang C Wang and Jean Opsomer},
  TITLE =        {On the asymptotic normality and variance estimation
                  of non-differentiable survey estimators},
  JOURNAL =      {Biometrika},
  YEAR =         {forthcoming},
}


@Article{wang:1997,
  author    = {Wang, Yuchung},
  year      = {1997},
  title     = {Multivariate normal integrals and contingency tables with
          ordered categories},
  abstract  = {Abstract\&nbsp;\&nbsp;Ak-dimensional multivariate normal
          distribution is made discrete by partitioning
          thek-dimensional Euclidean space with rectangular grids.
          The collection of probability integrals over the
          partitioned cubes is ak-dimensional contingency table with
          ordered categories. It is shown that loglinear model with
          main effects plus two-way interactions provides an accurate
          approximation for thek-dimensional table. The complete
          multivariate normal integral table is computed via the
          iterative proportional fitting algorithm from bivariate
          normal integral tables. This approach imposes no
          restriction on the correlation matrix. Comparisons with
          other numerical integration algorithms are reported. The
          approximation suggests association models for discretized
          multivariate normal distributions and contingency tables
          with ordered categories.},
  doi       = {10.1007/BF02295280},
  journal   = {Psychometrika},
  number    = {2},
  pages     = {267--284},
  volume    = {62}
}

@Article{wang:hick:2000:randomized,
  author    = "X. Wang and F. Hickernell",
  year      = "2000",
  title     = "Randomized {H}alton sequences",
  journal   = "Math. Comput. Modelling",
  volume    = 32,
  pages     = "887--899",
  comment   = "http://citeseer.ist.psu.edu/wang00randomized.html"
}

@Article{wang:pute:1998,
  author    = {Wang, Peiming and Puterman, Martin L.},
  year      = 1998,
  title     = {Mixed Logistic Regression Models},
  journal   = {Journal of Agricultural, Biological, and Environmental
          Statistics},
  volume    = 3,
  pages     = {175--200}
}

@Article{wang:zivot:1998,
  author    = {Wang, Jiahui and Zivot, Eric },
  year      = {1998},
  title     = {Inference on Structural Parameters in Instrumental
          Variables Regression with Weak Instruments},
  journal   = {Econometrica},
  number    = {6},
  pages     = {1389--1404},
  volume    = {66},
  abstract  = {We consider the problem of making asymptotically valid
          inference on structural parameters in instrumental
          variables regression with weak instruments. Using the
          local-to-zero asymptotics of Staiger and Stock (1997), we
          derive the asymptotic distributions of LR and LM type
          statistics for testing simple hypotheses on structural
          parameters based on maximum likelihood and generalized
          method of moments estimation methods. In contrast to the
          nonstandard limiting behavior of Wald statistics, the
          limiting distributions of certain LM and LR statistics are
          bounded by a chi-square distribution with degrees of
          freedom given by the number of instruments. Further, we
          show how to construct asymptotically valid confidence sets
          for structural parameters by inverting these statistics.}
}

@Book{wansbeek:meijer:latent,
  author    = "Tom Wansbeek and Erik Meijer",
  year      = 2000,
  title     = "Measurement Error and Latent Variables in Econometrics",
  publisher = "North-Holland",
  address   = "Amsterdam"
}

@Book{wass:1994,
  author    = {Stanley Wasserman},
  year      = {1994},
  title     = {Social Network Analysis: Methods and Applications},
  publisher = {Cambridge University Press}
}

@Book{wass:gala:1994,
  editor    = {Stanley Wasserman and Joseph Galaskiewicz},
  year      = {1994},
  title     = {Advances in Social Network Analysis: Research in the
          Social and Behavioral Sciences},
  publisher = {SAGE},
  address   = {Thousand Oaks, CA}
}

@Book{wasserman:2003,
  author    = {Wasserman, Larry},
  year      = {2003},
  title     = {All of Statistics: A Concise Course in Statistical
          Inference (Springer Texts in Statistics)},
  abstract  = {{This book is for people who want to learn probability and
          statistics quickly. It brings together many of the main
          ideas in modern statistics in one place. The book is
          suitable for students and researchers in statistics,
          computer science, data mining and machine learning. This
          book covers a much wider range of topics than a typical
          introductory text on mathematical statistics. It includes
          modern topics like nonparametric curve estimation,
          bootstrapping and classification, topics that are usually
          relegated to follow-up courses. The reader is assumed to
          know calculus and a little linear algebra. No previous
          knowledge of probability and statistics is required. The
          text can be used at the advanced undergraduate and graduate
          level.}},
  comment   = {This is a really neat intro book at upper
          undergraduate/nonmajor graduate level that cruises through
          the main topics in probability theory and mathematical
          statistics deliberately and meticulously avoiding proofs --
          those are essentially lecture notes from a statistics class
          offered to computer science students, with an idea to
          present as much material as possible in one semester. In a
          rigorous class, this would probably take around three
          semesters to cover it proving at least the main theorems.
          It has a well-thought-of structure and nice layout. It
          makes relevant links between mathematical statistics and
          data mining/learning literatures, and uses resampling
          inference from time to time, so it is way more modern than
          many other standard books like Hogg \& Craig.},
  howpublished  = {Hardcover},
  isbn      = {0387402721},
  publisher = {Springer}
}

@BOOK{wata:yama:2004,
  editor =       {Michiko Watanabe and Kazunori Yamaguchi},
  TITLE =        {The {EM} Algorithm and Related Statistical Models},
  PUBLISHER =    {Marcel Dekker},
  YEAR =         {2004},
  address =      {New York},
  isbn =         {0824747011},
}


@TechReport{wb:2007:series,
  author    = {Gwatkin, Davidson R. and Rutstein, Shea and Johnson,
          Kiersten and Suliman, Eldaw and Wagstaff, Adam and Amouzou,
          Agbessi},
  year      = {2007},
  title     = {Socio-economic differences in health, nutrition, and
          population},
  institution   = {The World Bank},
  type      = {Working Papers},
  number    = {Cameroon: WP \# 39467; Bangladesh: WP \#39465; Burkina
          Faso: WP \# 39466; Colobmia: WP \# 39468; Ghana: WP \#
          39469; Guatemala: WP \# 39445; Haiti: WP \# 39446; India:
          WP \# 39447; Indonesia: WP \# 39448; Kazakhstan: WP \#
          39449; Kenya: WP \# 39470; Malawi: WP \# 39450; Mali: WP \#
          39451; Morocco: WP \# 39452; Mozambique: WP \# 39453;
          Namibia: WP \# 39454; Nepal: WP \# 39455; Nicaragua: WP \#
          39456; Nigeria: WP \# 39457; Peru: WP \# 39458;
          Philippines: WP \# 39459; Tanzania: WP \# 39471; Turkey: WP
          \# 39460; Uganda: WP \# 39461; Zambia: WP \# 39463; Zimbabwe: WP \# 39464}
}

@Article{webster1996hispanic,
  author    = {Webster, Cynthia},
  year      = {1996},
  title     = {Hispanic and Anglo Interviewer and Respondent Ethnicity
          and Gender: The Impact on Survey Response Quality},
  abstract  = {The author applies a hierarchical regression model to
          analyze the simultaneous effects of Hispanic and Anglo
          interviewer and respondent ethnicity and gender on response
          quality in survey research. Although few significant main
          effects were found, response quality was affected
          significantly by interaction effects of respondent and
          interviewer ethnicity and gender. For the most part,
          ethnically homophilous and gender-heterophilous interviews
          generate the highest response quality. Both Hispanic and
          Anglo respondents deferred to an interviewer of a different
          ethnic background when queried about the interviewer's
          culture, but not when asked noncultural, albeit sensitive,
          questions.},
  journal   = {Journal of Marketing Research},
  number    = {1},
  pages     = {62--72},
  volume    = {33}
}

@Article{webster:1996,
  author    = {Webster, Cynthia },
  year      = {1996},
  title     = {Hispanic and Anglo Interviewer and Respondent Ethnicity
          and Gender: The Impact on Survey Response Quality},
  journal   = {Journal of Marketing Research},
  number    = {1},
  pages     = {62--72},
  volume    = {33},
  abstract  = {The author applies a hierarchical regression model to
          analyze the simultaneous effects of Hispanic and Anglo
          interviewer and respondent ethnicity and gender on response
          quality in survey research. Although few significant main
          effects were found, response quality was affected
          significantly by interaction effects of respondent and
          interviewer ethnicity and gender. For the most part,
          ethnically homophilous and gender-heterophilous interviews
          generate the highest response quality. Both Hispanic and
          Anglo respondents deferred to an interviewer of a different
          ethnic background when queried about the interviewer's
          culture, but not when asked noncultural, albeit sensitive,
          questions.}
}

@Article{webster:2001,
  author    = "Thomas J. Webster",
  year      = 2001,
  title     = "A Principal Component Analysis of the {U}.{S}.{N}ews \&
          {W}orld {R}eport Tier Rankings of Colleges and
          Universities",
  journal   = "Economics of Education Review",
  volume    = 20,
  pages     = "235-244"
}

@Misc{weiss:mathworld:eigenvalue,
  author    = "Eric W. Weisstein",
  year      = "2004",
  title     = "Eigenvalue",
  note      = "From MathWorld--A Wolfram Web Resource.
          http://mathworld.wolfram.com/Eigenvalue.html"
}

@article{west:berglund:heeringa:2008,
    author = "West, B. T. and Berglund, P. and Heeringa, S. G.",
    title = "A closer examination of subpopulation analysis
             of complex-sample survey data",
    journal = "Stata Journal",
    publisher = "Stata Press",
    address = "College Station, TX",
    volume = "8",
    number = "4",
    year = "2008",
    pages = "520-531(12)",
    comment = "http://www.stata-journal.com/article.html?article=st0153"
}

@article{west:olson:2010,
    abstract =
        {Kish's (1962) classical intra-interviewer correlation (roh int)
        provides survey researchers with an estimate of the effect of
        interviewers on variation in measurements of a survey variable of
        interest. This correlation is an undesirable product of the data
        collection process that can arise when answers from respondents
        interviewed by the same interviewer are more similar to each other
        than answers from other respondents, decreasing the precision of
        survey estimates. Estimation of this parameter, however, uses only
        respondent data. The potential contribution of variance in
        nonresponse errors between interviewers to the estimation of roh int has
        been largely ignored. Responses within interviewers may appear
        correlated because the interviewers successfully obtain cooperation
        from different pools of respondents, not because of systematic
        response deviations. This study takes a first step in filling this
        gap in the literature on interviewer effects by analyzing a unique
        survey data set, collected using computer-assisted telephone
        interviewing (CATI) from a sample of divorce records. This data set,
        which includes both true values and reported values for respondents
        and a CATI sample assignment that approximates interpenetrated
        assignment of subsamples to interviewers, enables the decomposition
        of interviewer variance in means of respondent reports into
        nonresponse error variance and measurement error variance across
        interviewers. We show that in cases where there is substantial
        interviewer variance in reported values, the interviewer variance may
        arise from nonresponse error variance across interviewers.},
    author = {West, Brady T. and Olson, Kristen},
    doi = {10.1093/poq/nfq061},
    issn = {1537-5331},
    journal = {Public Opinion Quarterly},
    number = {5},
    pages = {1004--1026},
    title = {How Much of Interviewer Variance is Really Nonresponse Error Variance?},
    volume = {74},
    year = {2010}
}

@article{west:sakshaug:aure:2016,
  title={How big of a problem is analytic error in secondary analyses of survey data?},
  author={West, Brady T and Sakshaug, Joseph W and Aurelien, Guy Alain S},
  journal={{PloS} {One}},
  volume={11},
  number={6},
  pages={e0158120},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

@Article{white:1978,
  author    = {White, D.},
  year      = {1978},
  title     = {Computer generation of balanced arrays},
  journal   = {Journal of Statistical Planning and Inference},
  pages     = {253--275},
  volume    = {2}
}

@Article{white:1980,
  author    = "Halbert White",
  year      = 1980,
  title     = "A Heteroskedasticity-Consistent Covariance-Matrix
          Estimator and a Direct Test for Heteroskedasticity",
  journal   = "Econometrica",
  volume    = 48,
  number    = 4,
  pages     = {817--838}
}

@Article{white:1981,
  author    = {Halbert White},
  year      = {1981},
  title     = {Consequences and Detection of Misspecified Nonlinear
          Regression Models},
  journal   = {Journal of the American Statistical Association},
  volume    = {76},
  pages     = {419--433}
}

@Article{white:1982,
  author    = "Halbert White",
  year      = 1982,
  title     = "Maximum Likelihood Estimation of Misspecified Models",
  journal   = "Econometrica",
  volume    = 50,
  number    = 1,
  pages     = "1--26"
}

@Book{white:1984,
  author    = {Halbert White},
  year      = {1984},
  title     = {Asymptotic Theory for Econometricians},
  publisher = {Academic Press},
  address   = {Orlando, FL}
}

@Book{white:1996,
  author    = {Halbert White},
  year      = {1996},
  title     = {Estimation, Inference and Specification Analysis},
  publisher = {Cambridge University Press},
  volume    = {22},
  series    = {Econometric Society Monographs},
  address   = {New York}
}

@article{white:royston:wood:2011,
    abstract =
        {Multiple imputation by chained equations is a flexible and practical
        approach to handling missing data. We describe the principles of the
        method and show how to impute categorical and quantitative variables,
        including skewed variables. We give guidance on how to specify the
        imputation model and how many imputations are needed. We describe the
        practical analysis of multiply imputed data, including model building
        and model checking. We stress the limitations of the method and
        discuss the possible pitfalls. We illustrate the ideas using a data
        set in mental health, giving Stata code fragments. Copyright
        {\copyright} 2010 John Wiley \& Sons, Ltd.},
    author = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
    doi = {10.1002/sim.4067},
    journal = {Statistics in Medicine},
    number = {4},
    pages = {377--399},
    pmid = {21225900},
    title = {Multiple imputation using chained equations: Issues and guidance for practice},
    volume = {30},
    year = {2011}
}

@Misc{wider:wiid:2008,
  author    = "WIDER",
  year      = 2000,
  month     = "May",
  title     = "World Income Inequality Database ({WIID})",
  note      = "{UNU/WIDER-UNDP W}orld {I}ncome {I}nequality {D}atabase,
          Version 2.0c,
          http://www.wider.unu.edu/wiid/wiid.htm",
  institution   = "World {I}nstitute for {D}evelopment {E}conomics
          {R}esearch",
  address   = "Helsinki, Finland"
}

@Article{wikle:2003,
  author    = "Christopher K. Wikle",
  year      = 2003,
  title     = "Hierarchical {B}ayesian models for predicting the spread
          of ecological processes",
  journal   = "Ecology",
  volume    = 83,
  pages     = "1382--1394"
}

@Article{wikle:mill:nych:berl:2001,
  author    = "Christopher K. Wikle and Ralph E. Milliff and Douglas
          Nychka and and L. Mark Berliner",
  year      = 2001,
  title     = "Spatiotemporal Hierarchical {B}ayesian Modeling: Tropical
          Ocean Surface Winds",
  journal   = "Journal of the American Statistical Association",
  volume    = 96,
  issue     = 454,
  pages     = "382--398"
}

@Article{wilkinson:2008,
  author    = {Leland Wilkinson},
  year      = {2008},
  title     = {The Future of Statistical Computing},
  journal   = {Technometrics},
  volume    = {50},
  number    = {4},
  pages     = {418--435},
  doi       = {10.1198/004017008000000460}
}

@Article{wilks:1938,
  author    = "S. S. Wilks",
  year      = 1938,
  title     = "The Large Sample Distribution of the Likelihood Ratio for
          Testing Composite Hypotheses",
  journal   = "The Annals of Mathematical Statistics",
  volume    = 9,
  number    = 1,
  pages     = "60--62"
}

@Book{wilks:1962,
  author    = "S. S. Wilks",
  year      = 1962,
  title     = "Mathematical Statistics",
  publisher = "John Wiley and Sons",
  address   = "New York"
}

@ARTICLE{will:birk:rees:1998,
  AUTHOR =       {P. Williamson and M. Birkin and P. Rees},
  TITLE =        {The estimation of population microdata by using data from small area statistics and sample of anonymised records},
  JOURNAL =      {Environment and Planning Analysis},
  YEAR =         {1998},
  volume =       {30},
  pages =        {785--816},
}


@Article{windmeijer:2005,
  author    = {Windmeijer, Frank},
  year      = {2005},
  title     = {A finite sample correction for the variance of linear
          efficient two-step GMM estimators},
  abstract  = {Monte Carlo studies have shown that estimated asymptotic
          standard errors of the efficient two-step generalized
          method of moments (GMM) estimator can be severely downward
          biased in small samples. The weight matrix used in the
          calculation of the efficient two-step GMM estimator is
          based on initial consistent parameter estimates. In this
          paper it is shown that the extra variation due to the
          presence of these estimated parameters in the weight matrix
          accounts for much of the difference between the finite
          sample and the usual asymptotic variance of the two-step
          GMM estimator, when the moment conditions used are linear
          in the parameters. This difference can be estimated,
          resulting in a finite sample corrected estimate of the
          variance. In a Monte Carlo study of a panel data model it
          is shown that the corrected variance estimate approximates
          the finite sample variance well, leading to more accurate
          inference.},
  doi       = {10.1016/j.jeconom.2004.02.005},
  journal   = {Journal of Econometrics},
  number    = {1},
  pages     = {25--51},
  volume    = {126}
}

@Article{winship1994sampling,
  author    = {Winship, Christopher and Radbill, Larry},
  year      = {1994},
  title     = {Sampling Weights and Regression Analysis},
  abstract  = {Most major population surveys used by social scientists
          are based on complex sampling designs where sampling units
          have different probabilities of being selected. Although
          sampling weights must generally be used to derive unbiased
          estimates of univariate population characteristics, the
          decision about their use in regression analysis is more
          complicated. Where sampling weights are solely a function
          of independent variables included in the model, unweighted
          OLS estimates are preferred because they are unbiased,
          consistent, and have smaller standard errors than weighted
          OLS estimates. Where sampling weights are a function of the
          dependent variable (and thus of the error term), we
          recommend first attempting to respecify the model so that
          they are solely a function of the independent variables. If
          this can be accomplished, then unweighted OLS is again
          preferred. If the model cannot be respecified, then
          estimation of the model using sampling weights may be
          appropriate. In this case, however, the formula used by
          most computer programs for calculating standard errors will
          be incorrect. We recommend using the White heteroskedastic
          consistent estimator for the standard errors.
          10.1177/0049124194023002004},
  doi       = {10.1177/0049124194023002004},
  journal   = {Sociological Methods Research},
  number    = {2},
  pages     = {230--257},
  volume    = {23}
}

@Misc{winter:2002,
  author={Nick Winter},
  title={SURVWGT: Stata module to create and manipulate survey weights},
  year=2002,
  howpublished={Statistical Software Components, Boston College Department of Economics},
  abstract=
    {survwgt creates sets of weights for replication-based variance
    estimation techniques for survey data. These include balanced repeated
    replication (BRR) and several version of the survey jackknife (JK*).
    These replication methods are alternates to the Taylor series
    linearization methods used by Stata's svy-based commands.},
  note = {RePEc:boc:bocode:s427503}
}


@INPROCEEDINGS{witt:2010,
  AUTHOR =       {Michael B. Witt},
  TITLE =        {Estimating the {R}-indicator, Its Standard Error and Other Related Statistics with {SAS} and {SUDAAN}},
  BOOKTITLE =    {Proceedings of the Survey Research Methods Section},
  YEAR =         {2010},
  publisher =    {The American Statistical Association},
  address =      {Alexandria, VA},
}



@ARTICLE{wittenberg:2010,
    title = {An introduction to maximum entropy and minimum cross-entropy estimation using Stata},
    author = {Wittenberg, Martin},
    year = {2010},
    journal = {Stata Journal},
    volume = {10},
    number = {3},
    pages = {315-330},
}


@Article{wolter:1979,
  author    = "K. M. Wolter",
  year      = "1979",
  title     = "Composite Estimation in Finite Population",
  journal   = "Journal of the American Statistical Association",
  volume    = "74",
  pages     = "604--613"
}

@Book{wolter:1985,
  author    = {K. M. Wolter},
  year      = {1985},
  title     = {Introduction to Variance Estimation},
  publisher = {Springer},
  address   = {New York}
}

@Book{wolter:2007,
  author    = {K. M. Wolter},
  year      = {2007},
  title     = {Introduction to Variance Estimation},
  publisher = {Springer},
  address   = {New York},
  edition   = {2nd}
}

@article{wolter:smith:blumberg:2010,
    author = {Wolter, Kirk and Smith, Phil and Blumberg, Stephen J.},
    journal = {Survey Methodology},
    number = {2},
    pages = {203--215},
    title = {Statistical foundations of cell-phone surveys},
    volume = {36},
    year = {2010}
}

@Article{wood:booth:butler:1993,
  author    = {Wood, Andrew T. A. and Booth, James G. and Butler, Ronald
          W.},
  year      = {1993},
  title     = {Saddlepoint Approximations to the CDF of Some Statistics
          with Nonnormal Limit Distributions},
  abstract  = {In standard saddlepoint approximations to the cumulative
          distribution function of a random variable, the normal
          distribution has appeared to play a special role. In this
          article we consider what happens when the normal "base"
          distribution is replaced by an arbitrary base distribution.
          Generalized versions of several standard formulas, are
          presented. The choice of a chi-squared base or an inverse
          Gaussian base is then considered in detail. The generalized
          approximations are compared in two examples: a linear
          combination of chi-squared variables and the first passage
          time distribution for a random walk. The former example
          considers approximations using the chi-squared base that
          are slightly more accurate than their normal-based
          counterparts. In the latter example, approximations based
          on the inverse Gaussian are considerably more accurate than
          their normal-based counterparts.},
  doi       = {10.2307/2290351},
  issn      = {01621459},
  journal   = {Journal of the American Statistical Association},
  number    = {422},
  pages     = {680--686},
  publisher = {American Statistical Association},
  volume    = {88}
}

@article{woodruff:1952,
    author = {Woodruff, Ralph S.},
    doi = {10.2307/2280781},
    issn = {01621459},
    journal = {Journal of the American Statistical Association},
    number = {260},
    pages = {635--646},
    publisher = {American Statistical Association},
    title = {Confidence Intervals for Medians and Other Position Measures},
    volume = {47},
    year = {1952}
}

@InCollection{wooldridge:1994,
  author    = {Jeffrey M. Wooldridge},
  editor    = {R. Engle and D. Mc{F}adden},
  year      = {1994},
  title     = {Estimation and Inference for Dependent Processes},
  booktitle = {Handbook of econometrics},
  chapter   = {45},
  publisher = {North Holland},
  volume    = {{IV}},
  address   = {New York}
}



@Book{woold:2002,
  author    = "Jeffrey M. Wooldridge",
  year      = 2002,
  title     = "Econometric Analysis of Cross Section and Panel Data",
  publisher = "MIT Press",
  address   = "Cambridge, MA"
}

@Book{wooldridge:2002,
  author    = {Wooldridge, Jeffrey M. },
  year      = {2002},
  title     = {Econometric Analysis of Cross Section and Panel Data},
  isbn      = {0262232197},
  publisher = {The MIT Press},
  address   = {Cambridge, MA}
}

@article{wooldridge:2003,
     title = {Cluster-Sample Methods in Applied Econometrics},
     author = {Wooldridge, Jeffrey M.},
     journal = {The American Economic Review},
     volume = {93},
     number = {2},
     pages = {133--138},
     year = {2003},
     publisher = {American Economic Association},
}

@InBook{wooldridge:2005,
  author    = {Wooldridge, Jeffrey M.},
  editor    = {Andrews, Donald W. K. and Stock, James H.},
  year      = {2005},
  title     = {Unobserved Heterogeneity and Estimation of Average Partial
          Effects},
  abstract  = {I study the problem of identifying average partial effects
          (APEs), which are partial effects averaged across the
          population distribution of unobserved heterogeneity, under
          different assumptions. One possibility is that the
          unobserved heterogeneity is conditionally independent of
          the observed covariates. When the unobserved heterogeneity
          is independent of the original covariates, or conditional
          mean independent but heteroskedastic, the derivations of
          APEs provide a new view of traditional specification
          problems in widely used models such as probit and Tobit. In
          addition, the focus on average partial effects resolves
          scaling issues that arise in estimating the parameters of
          probit and Tobit models with endogenous explanatory
          variables.},
  booktitle = {Identification and Inference for Econometric Models:
          Essays in Honor of Thomas Rothenberg},
  chapter   = {3},
  comment   = {While unobserved heterogeneity (such as omitted variables)
          may be detrimental for point estimates in nonlinear models
          (Yatchew and Griliches 1985), the partial effects
          E[difference in response|two given values of x, all other
          covariates] may be more robust to those types of
          misspecification under weak assumptions of conditional
          independence of heterogeneity and covariates given the set
          of additional controls, similar to ignorability assumptions
          in treatment effect literature.},
  publisher = {Cambridge University Press}
}

@INCOLLECTION{wothke:1993,
  AUTHOR =       {Werner Wothke},
  TITLE =        {Nonpositive Definite Matrices in Structural Modeling},
  BOOKTITLE =    {Testing Structural Equation Models},
  PUBLISHER =    {SAGE},
  YEAR =         {1993},
  editor =       {Kenneth A. Bollen and J. Scott Long},
  chapter =      {11},
}


@Article{wright:1918,
  author    = {Sewall Wright},
  year      = {1918},
  title     = {On the nature of size factors},
  journal   = {Genetics},
  volume    = {3},
  pages     = {367--374}
}

@ARTICLE{wright:1934,
  AUTHOR =       {Sewall Wright},
  TITLE =        {The Method of Path Coefficients},
  JOURNAL =      {Annals of Mathematical Statistics},
  YEAR =         {1934},
  volume =       {5},
  pages =        {161--215},
}

@article{wright:2012,
    abstract =
        {We present a surprising though obvious result that seems to have
        been unnoticed until now. In particular, we demonstrate the
        equivalence of two well-known problems?the optimal allocation of the
        fixed overall sample size n among L strata under stratified random
        sampling and the optimal allocation of the H = 435 seats among the 50
        states for apportionment of the {U.S}. House of Representatives
        following each decennial census. In spite of the strong similarity
        manifest in the statements of the two problems, they have not been
        linked and they have well-known but different solutions; one solution
        is not explicitly exact (Neyman allocation), and the other (equal
        proportions) is exact. We give explicit exact solutions for both and
        note that the solutions are equivalent. In fact, we conclude by
        showing that both problems are special cases of a general problem.
        The result is significant for stratified random sampling in that it
        explicitly shows how to minimize sampling error when estimating a
        total {TY} while keeping the final overall sample size fixed at n;
        this is usually not the case in practice with Neyman allocation where
        the resulting final overall sample size might be near n + L after
        rounding. An example reveals that controlled rounding with Neyman
        allocation does not always lead to the optimum allocation, that is,
        an allocation that minimizes variance.},
    author = {Wright, Tommy},
    doi = {10.1080/00031305.2012.733679},
    journal = {The American Statistician},
    number = {4},
    pages = {217--224},
    title = {The Equivalence of {Neyman} Optimum Allocation for Sampling
        and Equal Proportions for Apportioning the {U.S}. {H}ouse of {R}epresentatives},
    volume = {66},
    year = {2012}
}

@article{wu:1983,
    author = {Wu, C. F. Jeff},
    doi = {10.2307/2240463},
    journal = {The Annals of Statistics},
    number = {1},
    pages = {95--103},
    title = {On the Convergence Properties of the {EM} Algorithm},
    volume = {11},
    year = {1983},
    abstract =
       {Two convergence aspects of the {EM} algorithm are studied: (i) does
       the {EM} algorithm find a local maximum or a stationary value of the
       (incomplete-data) likelihood function? (ii) does the sequence of
       parameter estimates generated by {EM} converge? Several convergence
       results are obtained under conditions that are applicable to many
       practical situations. Two useful special cases are: (a) if the
       unobserved complete-data specification can be described by a curved
       exponential family with compact parameter space, all the limit points
       of any {EM} sequence are stationary points of the likelihood function;
       (b) if the likelihood function is unimodal and a certain
       differentiability condition is satisfied, then any {EM} sequence
       converges to the unique maximum likelihood estimate. A list of key
       properties of the algorithm is included.},
}


@Article{wu:1986,
  author    = {Wu, C. F. J. },
  year      = {1986},
  title     = {Jackknife, Bootstrap and Other Resampling Methods in
          Regression Analysis},
  journal   = {The Annals of Statistics},
  number    = {4},
  pages     = {1261--1295},
  comment   = {http://links.jstor.org/sici?sici=0090-5364%28198612%2914%3A4%3C1261%3AJBAORM%3E2.0.CO%3B2-F}
          ,
  volume    = {14},
  doi       = {10.2307/2241454},
  abstract  = {Motivated by a representation for the least squares
          estimator, we propose a class of weighted jackknife
          variance estimators for the least squares estimator by
          deleting any fixed number of observations at a time. They
          are unbiased for homoscedastic errors and a special case,
          the delete-one jackknife, is almost unbiased for
          heteroscedastic errors. The method is extended to cover
          nonlinear parameters, regression M-estimators, nonlinear
          regression and generalized linear models. Interval
          estimators can be constructed from the jackknife histogram.
          Three bootstrap methods are considered. Two are shown to
          give biased variance estimators and one does not have the
          bias-robustness property enjoyed by the weighted delete-one
          jackknife. A general method for resampling residuals is
          proposed. It gives variance estimators that are
          bias-robust. Several bias-reducing estimators are proposed.
          Some simulation results are reported.}
}

@Article{wu:1990,
  author    = {Wu, C. F. J.},
  year      = {1990},
  title     = {On the Asymptotic Properties of the Jackknife Histogram},
  abstract  = {We study the asymptotic normality of the jackknife
          histogram. For one sample mean, it holds if and only if r,
          the number of observations retained, and d (= n - r), the
          number of observations deleted, both diverge to infinity.
          The best convergence rate n-1/2 is obtained when r = O(n)
          and d = O(n). For U statistics of degree 2 and nonlinear
          statistics admitting the expansion (3.1), similar results
          are obtained under conditions on r and d. A second order
          approximation based on the Edgeworth expansion is discussed
          briefly.},
  doi       = {10.2307/2242062},
  issn      = {00905364},
  journal   = {The Annals of Statistics},
  number    = {3},
  pages     = {1438--1452},
  publisher = {Institute of Mathematical Statistics},
  volume    = {18}
}

@Article{wu:1991,
  author    = {Wu, C. F. J. },
  year      = {1991},
  title     = {Balanced repeated replications based on mixed orthogonal
          arrays},
  doi       = {10.1093/biomet/78.1.181},
  journal   = {Biometrika},
  number    = {1},
  pages     = {181--188},
  volume    = {78},
  abstract  = {For stratified samples with arbitrary numbers of primary
          sampling units per stratum, one can divide the units in the
          stratum into two groups and then apply the balanced
          half-samples method to the two groups thus formed. The
          efficiency loss of the grouped balanced half-samples
          variance estimator can be substantial. Use of a mixed
          orthogonal array, whose numbers of symbols correspond to
          the numbers of units per stratum, gives a set of balanced
          subsamples. Unlike that of Gupta \& Nigam (1987), the
          proposed method of analysis gives a consistent variance
          estimator and bias reduction for nonlinear statistics. To
          further reduce the number of replicates, use of nearly
          orthogonal arrays is considered. 10.1093/biomet/78.1.181}
}

@article{wu:sitter:2001,
    author = {Wu, Changbao and Sitter, Randy R.},
    doi = {10.2307/2670358},
    journal = {Journal of the American Statistical Association},
    number = {453},
    title = {A Model-Calibration Approach to Using Complete Auxiliary Information from Survey Data},
    pages = {185--193},
    volume = {96},
    year = {2001},
    abstract =
       {Suppose that the finite population consists of N
       identifiable units. Associated with the ith unit are the
       study variable, yi, and a vector of auxiliary variables, xi.
       The values x1, x2,..., xN are known for the entire population
       (i.e., complete) but yi is known only if the ith unit is
       selected in the sample. One of the fundamental questions is
       how to effectively use the complete auxiliary information at
       the estimation stage. In this article, a unified
       model-assisted framework has been attempted using a proposed
       model-calibration technique. The proposed model-calibration
       estimators can handle any linear or nonlinear working models
       and reduce to the conventional calibration estimators of
       Deville and Sarndal and/or the generalized regression
       estimators in the linear model case. The pseudoempirical
       maximum likelihood estimator of Chen and Sitter, when used in
       this setting, gives an estimator that is asymptotically
       equivalent to the model-calibration estimator but with
       positive weights. Some existing estimators using auxiliary
       information are reexamined under this framework. The
       estimation of the finite population distribution function,
       using complete auxiliary information, is also considered, and
       estimators based on a general model are presented. Results of
       a limited simulation study on the performance of the proposed
       estimators are reported.},
}

@article{wu:2003:bmka,
    author = {Wu, Changbao},
    doi = {10.1093/biomet/90.4.937},
    journal = {Biometrika},
    number = {4},
    pages = {937--951},
    title = {Optimal calibration estimators in survey sampling},
    volume = {90},
    year = {2003},
    abstract =
       {We show that the model?calibration estimator for the
       finite population mean, which was proposed by Wu \& Sitter
       (2001) through an intuitive argument, is optimal among a
       class of calibration estimators. We also present optimal
       calibration estimators for the finite population distribution
       function, the population variance, the variance of a linear
       estimator and other quadratic finite population functions
       under a unified framework. The proposed calibration
       estimators are optimal under the true model but remain design
       consistent even if the working model is misspecified. A
       limited simulation study shows that the improvement of these
       optimal estimators over the conventional ones can be
       substantial. The question of when and how auxiliary
       information can be used for both the estimation of the
       population mean using a generalised regression estimator and
       the estimation of its variance through calibration is
       addressed clearly under the proposed general methodology.
       Some fundamental issues in using auxiliary information from
       survey data are also addressed in the context of optimal
       estimation.},
}

@article{wu:2003:spl,
    author = {Wu, C.},
    doi = {10.1016/j.spl.2003.10.007},
    journal = {Statistics \& Probability Letters},
    number = {1},
    pages = {67--79},
    title = {Weighted empirical likelihood inference},
    volume = {66},
    year = {2004},
    abstract =
        {A weighted empirical likelihood approach is proposed to
        take account of the heteroscedastic structure of the data.
        The resulting weighted empirical likelihood ratio statistic
        is shown to have a limiting chisquare distribution. A
        limited simulation study shows that the associated
        confidence intervals for a population mean or a regression
        coefficient have more accurate coverage probabilities and
        more balanced two-sided tail errors when the sample size is
        small or moderate. The proposed weighted empirical
        likelihood method also provides more efficient point
        estimators for a population mean in the presence of side
        information. Large sample resemblances between the weighted
        and the unweighted empirical likelihood estimators are
        characterized through high-order asymptotics and small
        sample discrepancies of these estimators are investigated
        through simulation. The proposed weighted approach reduces
        to the usual unweighted empirical likelihood method under a
        homogeneous variance structure.},
}


@Article{wu:2004:statsinica,
  author    = {Wu, Changbao },
  year      = {2004},
  title     = {Some Algorithmic Aspects of the Empirical Likelihood
          Method in Survey Sampling},
  journal   = {Statistica Sinica},
  pages     = {1057--1067},
  volume    = {14},
  abstract  = {Recent development of the empirical likelihood method in
          survey sampling has attracted attention from survey
          statisticians. Practical considerations for using the
          method in real surveys depend largely on the availability
          of simple and efficient algorithms for computing the
          related weights for the maximum empirical likelihood
          estimators. In this article we briefly describe the
          modified Newton-Raphson procedure of Chen, Sitter and Wu
          (2002) for non-stratified sampling designs and show that
          under suitable reformulation the algorithm can also be used
          to handle stratified sampling, the most commonly used
          design in survey practice. The idea of the q-weighted
          empirical likelihood approach is briefly introduced and the
          related algorithms are discussed. The proposed algorithms
          are tested in a limited simulation study and are shown to
          perform well.}
}

@article{wu:2008,
    author = {Wu, Changbao},
    doi = {10.2307/3315996},
    journal = {Can J Statistics},
    number = {1},
    pages = {15--26},
    title = {Combining information from multiple surveys through the empirical likelihood method},
    volume = {32},
    year = {2004},
    abstract =
        {It is often desirable to combine information collected in
        compatible multiple surveys in order to improve estimation
        and meet consistency requirements. Zieschang (1990) and
        Renssen \& Nieuwenbroek (1997) suggested to this end the use
        of the generalized regression estimator with enlarged number
        of auxiliary variables. Unfortunately, adjusted weights
        associated with their approach can be negative. The author
        uses the notion of pseudo empirical likelihood to construct
        new estimators that are consistent, efficient and possess
        other attractive properties. The proposed approach is
        asymptotically equivalent to the earlier one, but it has
        clear maximum likelihood interpretations and its adjusted
        weights are always positive. The author also provides
        efficient algorithms for computing his estimators.},
}

@article{wu:rao:2009,
    author = {Wu, Changbao and Rao, J. N. K.},
    doi = {10.1002/cjs.5550340301},
    journal = {Canadian Journal of Statistics},
    number = {3},
    pages = {359--375},
    title = {Pseudo-empirical likelihood ratio confidence intervals for complex surveys},
    volume = {34},
    year = {2006},
    abstract =
       {The authors show how an adjusted pseudo-empirical likelihood
       ratio statistic that is asymptotically distributed as a
       chi-square random variable can be used to construct
       confidence intervals for a finite population mean or a finite
       population distribution function from complex survey samples.
       They consider both non-stratified and stratified sampling
       designs, with or without auxiliary information. They examine
       the behaviour of estimates of the mean and the distribution
       function at specific points using simulations calling on the
       Rao-Sampford method of unequal probability sampling without
       replacement. They conclude that the pseudo-empirical
       likelihood ratio confidence intervals are superior to those
       based on the normal approximation, whether in terms of
       coverage probability, tail error rates or average length of
       the intervals.},
}

@article{wu:2022:nonprob,
  author = {Changbao Wu},
  journal = {Survey Methodology},
  issue = 48,
  number = 2,
  year = 2022,
  pages = {283--374},
  title = {Statistical inference with non-probability survey samples (with discussion)}
}

@Article{wu:carroll:1988,
  author    = {Wu, Margaret C. and Carroll, Raymond J.},
  year      = {1988},
  title     = {Estimation and Comparison of Changes in the Presence of
          Informative Right Censoring by Modeling the Censoring
          Process},
  abstract  = {In the estimation and comparison of the rates of change of
          a continuous variable between two groups, the unweighted
          averages of individual simple least squares estimates from
          each group are often used. Under a linear random effects
          model, when all individuals have complete observations at
          identical time points, these statistics are maximum
          likelihood estimates for the expected rates of change.
          However, with censored or missing data, these estimates are
          no longer efficient when compared to generalized least
          squares estimates. When, in addition, the right-censoring
          process is dependent on the individual rates of change
          (i.e., informative right censoring), the generalized least
          squares estimates will be biased. Likelihood-ratio tests
          for informativeness of the censoring process and maximum
          likelihood estimates for the expected rates of change and
          the parameters of the right-censoring process are developed
          under a linear random effects model with a probit model for
          the right-censoring process. In realistic situations, we
          illustrate that the bias in estimating group rate of change
          and the reduction of power in comparing group differences
          could be substantial when strong dependency of the
          right-censoring process on individual rates of change is
          ignored.},
  doi       = {10.2307/2531905},
  issn      = {0006341X},
  journal   = {Biometrics},
  number    = {1},
  pages     = {175--188},
  publisher = {International Biometric Society},
  volume    = {44}
}

@Article{wu:rao:2006,
  author    = {Wu, Changbao and Rao, J. N. K. },
  year      = {2006},
  title     = {Pseudo-empirical likelihood ratio confidence intervals for
          complex surveys},
  journal   = {The Canadian Journal of Statistics},
  number    = {3},
  pages     = {359--376},
  volume    = {34}
}

@book{wu:thompson:2020,
  author    = {Wu, Changbao and Thompson, Mary E.},
  title     = {Sampling Theory and Practice},
  publisher = {Springer},
  address   = {New York},
  year      = {2020}
}

@INCOLLECTION{yamaguchi:2004,
  AUTHOR =       {Kazunori Yamaguchi},
  TITLE =        {Robust Model and the {EM} Algorithm},
  BOOKTITLE =    {The {EM} Algorithm and Related Statistical Models},
  PUBLISHER =    {Marcel Dekker},
  YEAR =         {2004},
  editor =       {Michiko Watanabe and Kazunori Yamaguchi},
  address =      {New York},
}

@article{yang:kim:2020,
  title={Statistical data integration in survey sampling: A review},
  author={Yang, Shu and Kim, Jae Kwang},
  journal={Japanese Journal of Statistics and Data Science},
  volume={3},
  number={2},
  pages={625--650},
  year={2020},
  publisher={Springer}
}

@Book{yates:1949,
  author    = "F. Yates",
  year      = "1949",
  title     = "Sampling Methods for Censuses and Surveys",
  publisher = "Charles Griffin",
  address   = "London"
}

@Article{yau:litt:2001,
  author    = {Yau, Linda H Y and Little, Roderick J},
  year      = 2001,
  title     = {Inference for the Complier-average Causal Effect From
          Longitudinal Data Subject to Noncompliance and Missing
          Data, With Application to a Job Training Assessment for the
          Unemployed},
  journal   = {Journal of the American Statistical Association},
  volume    = 96,
  number    = 456,
  pages     = {1232--1244}
}

@Article{ye:1998,
  author    = {Ye, Jianming},
  year      = {1998},
  title     = {On Measuring and Correcting the Effects of Data Mining and
          Model Selection},
  abstract  = {In the theory of linear models, the concept of degrees of
          freedom plays an important role. This concept is often used
          for measurement of model complexity, for obtaining an
          unbiased estimate of the error variance, and for comparison
          of different models. I have developed a concept of
          generalized degrees of freedom (GDF) that is applicable to
          complex modeling procedures. The definition is based on the
          sum of the sensitivity of each fitted value to perturbation
          in the corresponding observed value. The concept is
          nonasymptotic in nature and does not require analytic
          knowledge of the modeling procedures. The concept of GDF
          offers a unified framework under which complex and highly
          irregular modeling procedures can be analyzed in the same
          way as classical linear models. By using this framework,
          many difficult problems can be solved easily. For example,
          one can now measure the number of observations used in a
          variable selection process. Different modeling procedures,
          such as a tree-based regression and a projection pursuit
          regression, can be compared on the basis of their residual
          sums of squares and the GDF that they cost. I apply the
          proposed framework to measure the effect of variable
          selection in linear models, leading to corrections of
          selection bias in various goodness-of-fit statistics. The
          theory also has interesting implications for the effect of
          general model searching by a human modeler.},
  comment   = {Ye (1988) introduces the concept of generalized degrees of
          freedom by referring to a well known result in linear
          regression that d.f. = Cov[\hat Y\_i, Y\_i]/\sigma^2 = \sum
          \partial \hat Y\_i / \partial Y\_i. The extension to
          nonlinear models and model selection is by simulating Y\_i
          with some noise and refitting the complex model. },
  journal   = {Journal of the American Statistical Association},
  number    = {441},
  pages     = {120--131},
  volume    = {93}
}

@InProceedings{yeo:mantel:liu:1999,
  author    = {Yeo, Douglas and Mantel, Harold and Liu, Tzen-Ping },
  year      = {1999},
  title     = {Bootstrap variance estimation for the {N}ational
          {P}opulation {H}ealth {S}urvey},
  booktitle = {Proceedings of Survey Research Methods Section},
  organization  = {The American Statistical Association},
  pages     = {778--785}
}

@article{yin:cai:2005,
    author = {Yin, Guosheng and Cai, Jianwen},
    doi = {10.1111/j.0006-341X.2005.030815.x},
    journal = {Biometrics},
    number = {1},
    pages = {151--161},
    title = {Quantile Regression Models with Multivariate Failure Time Data},
    volume = {61},
    year = {2005},
    abstract =
       {As an alternative to the mean regression model, the quantile
       regression model has been studied extensively with
       independent failure time data. However, due to natural or
       artificial clustering, it is common to encounter multivariate
       failure time data in biomedical research where the
       intracluster correlation needs to be accounted for
       appropriately. For right-censored correlated survival data,
       we investigate the quantile regression model and adapt an
       estimating equation approach for parameter estimation under
       the working independence assumption, as well as a weighted
       version for enhancing the efficiency. We show that the
       parameter estimates are consistent and asymptotically follow
       normal distributions. The variance estimation using
       asymptotic approximation involves nonparametric functional
       density estimation. We employ the bootstrap and perturbation
       resampling methods for the estimation of the
       variance2013covariance matrix. We examine the proposed method
       for finite sample sizes through simulation studies, and
       illustrate it with data from a clinical trial on otitis
       media.},
}


@Article{yitzhaki:1983,
  author    = {Yitzhaki, Shlomo},
  year      = {1983},
  title     = {On an Extension of the {G}ini Inequality Index},
  doi       = {10.2307/2648789},
  issn      = {00206598},
  journal   = {International Economic Review},
  number    = {3},
  pages     = {617--628},
  publisher = {Blackwell Publishing for the Economics Department of the
          University of Pennsylvania and Institute of Social and
          Economic Research -- Osaka University},
  volume    = {24}
}

@Article{yitzhaki:1991,
  author    = {Yitzhaki, Shlomo},
  year      = {1991},
  title     = {Calculating Jackknife Variance Estimators for Parameters
          of the {G}ini Method},
  abstract  = {This article calculates jackknife variance estimators for
          parameters of the Gini method. For nonstratified samples,
          the calculation of the jackknife variance estimators
          requires only two runs over the data. Therefore, it enables
          users of the Gini method to calculate standard errors of
          the estimators using a standard personal computer.},
  doi       = {10.2307/1391792},
  issn      = {07350015},
  journal   = {Journal of Business \& Economic Statistics},
  number    = {2},
  pages     = {235--239},
  publisher = {American Statistical Association},
  volume    = {9}
}

@article{yitzhaki:1996,
    author = {Yitzhaki, Shlomo},
    doi = {10.2307/1392256},
    journal = {Journal of Business \& Economic Statistics},
    number = {4},
    pages = {478--486},
    publisher = {American Statistical Association},
    title = {On Using Linear Regressions in Welfare Economics},
    volume = {14},
    year = {1996},
    abstract =
       {This article consists of two parts. The first part shows
       that the ordinary least squares regression coefficient is a
       weighted average of slopes between adjacent sample points.
       When applied to a linear regression, with income as the
       independent variable, the regression coefficient depends
       heavily on the slopes of high-income groups. The weight of
       the highest income decile may well exceed that of the other
       nine deciles. This may be undesirable, especially if the
       regression is used for welfare analysis, because the marginal
       propensities to consume attributed to the commodities are
       determined by the high-income groups. The second part of the
       article proposes alternative estimators, the extended Gini
       estimators, that enable investigators to control the
       weighting scheme and to incorporate their social views into
       the weighting scheme of the estimators.},
}

@Article{you:chapman:2006,
  author    = {Yong You and Beatrice Chapman},
  year      = {2006},
  title     = {Small Area Estimation Using Area Level Models and
          Estimated Sampling Variances},
  journal   = {Survey Methodology},
  publisher = {Statistics Canada},
  volume    = {32},
  number    = {1},
  pages     = {97--103},
  abstract  = {In small area estimation, area level models such as the
          Fay?Herriot model (Fay and Herriot 1979) are widely used to
          obtain efficient modelbased estimators for small areas. The
          sampling error variances are customarily assumed to be
          known in the model. In this paper we consider the situation
          where the sampling error variances are estimated
          individually by direct estimators. A full hierarchical
          Bayes (HB) model is constructed for the direct survey
          estimators and the sampling error variances estimators. The
          Gibbs sampling method is employed to obtain the small area
          HB estimators. The proposed HB approach automatically takes
          account of the extra uncertainty of estimating the sampling
          error variances, especially when the areaspecific sample
          sizes are small. We compare the proposed HB model with the
          Fay?Herriot model through analysis of two survey data sets.
          Our results have shown that the proposed HB estimators
          perform quite well compared to the direct estimates. We
          also discussed the problem of priors on the variance
          components.}
}

@article{you:rao:2002,
    author = {You, Yong and Rao, J. N. K.},
    doi = {10.2307/3316146},
    journal = {Canadian Journal of Statistics},
    number = {3},
    pages = {431--439},
    title = {A pseudo-empirical best linear unbiased prediction approach to small area estimation using survey weights},
    volume = {30},
    year = {2002}
}

@Book{young:smith:2005,
  author    = {G. A. Young and R. L. Smith},
  year      = {2005},
  title     = {Essentials of Statistical Inference},
  publisher = {Cambridge University Press},
  series    = {Cambridge Series in Statistical and Probabilistic
          Mathematics},
  address   = {New York}
}

@article{yuan:chan:bentler:2000,
    author = {Yuan, Ke-Hai and Chan, Wai and Bentler, Peter M.},
    doi = {10.1348/000711000159169},
    journal = {British Journal of Mathematical and Statistical Psychology},
    number = {1},
    pages = {31--50},
    title = {Robust transformation with applications to structural equation modelling},
    volume = {53},
    year = {2000},
    abstract =
       {Data sets in social and behavioural sciences are seldom
       normal. Influential cases or outliers can lead to
       inappropriate solutions and problematic conclusions in
       structural equation modelling. By giving a proper weight to
       each case, the influence of outliers on a robust procedure
       can be minimized. We propose using a robust procedure as a
       transformation technique, generating a new data matrix that
       can be analysed by a variety of multivariate methods.
       Mardia's multivariate skewness and kurtosis statistics are
       used to measure the effect of the transformation in achieving
       approximate normality. Since the transformation makes the
       data approximately normal, applying a classical normal theory
       based procedure to the transformed data gives more efficient
       parameter estimates. Three procedures for parameter
       evaluation and model testing are discussed. Six examples
       illustrate the various aspects with the robust
       transformation.},
}

@Article{yuan:hayashi:bentler:2007,
  author    = {Yuan, Ke-Hai and Hayashi, Kentaro and Bentler, Peter M.},
  year      = {2007},
  title     = {Normal theory likelihood ratio statistic for mean and
          covariance structure analysis under alternative
          hypotheses},
  abstract  = {The normal distribution based likelihood ratio (LR)
          statistic is widely used in structural equation modeling.
          Under a sequence of local alternative hypotheses, this
          statistic has been shown to asymptotically follow a
          noncentral chi-square distribution. In practice, the
          population mean vector and covariance matrix as well as the
          model and sample size are always fixed. It is hard to
          justify the validity of the noncentral chi-square
          distribution for the resulting LR statistic even when data
          are normally distributed and sample size is large. By
          extending results in the literature, this paper develops
          normal distributions to describe the behavior of the LR
          statistic for mean and covariance structure analysis. A
          sequence of local alternative hypotheses is not necessary
          for the proposed distributions to be asymptotically valid.
          When the effect size is medium and above or when the model
          is not trivially misspecified, empirical results indicate
          that a refined normal distribution describes the behavior
          of the LR statistic better than the commonly used
          noncentral chi-square distribution, as measured by the
          Kolmogorov-Smirnov distance. Quantile-quantile plots are
          also provided to better understand the different
          distributions.},
  journal   = {Journal of Multivariate Analysis},
  number    = {6},
  pages     = {1262--1282},
  volume    = {98}
}

@Article{yuan:2009,
  author    = {Yuan, Ke-Hai},
  year      = {2009},
  title     = {Normal distribution based pseudo ML for missing data: With
          applications to mean and covariance structure analysis?},
  doi       = {10.1016/j.jmva.2009.05.001},
  issn      = {0047259X},
  journal   = {Journal of Multivariate Analysis},
  number    = {9},
  pages     = {1900--1918},
  volume    = {100},
  abstract  = {When missing data are either missing completely at random
          (MCAR) or missing at random (MAR), the maximum likelihood
          (ML) estimation procedure preserves many of its properties.
          However, in any statistical modeling, the distribution
          specification for the likelihood function is at best only
          an approximation to the real world. In particular, since
          the normal-distribution-based ML is typically applied to
          data with heterogeneous marginal skewness and kurtosis, it
          is necessary to know whether such a practice still
          generates consistent parameter estimates. When the manifest
          variables are linear combinations of independent random
          components and missing data are MAR, this paper shows that
          the normal-distribution-based MLE is consistent regardless
          of the distribution of the sample. Examples also show that
          the consistency of the MLE is not guaranteed for all
          nonnormally distributed samples. When the population
          follows a confirmatory factor model, and data are missing
          due to the magnitude of the factors, the MLE may not be
          consistent even when data are normally distributed. When
          data are missing due to the magnitude of measurement
          errors/uniqueness, MLEs for many of the covariance
          parameters related to the missing variables are still
          consistent. This paper also identifies and discusses the
          factors that affect the asymptotic biases of the MLE when
          data are not missing at random. In addition, the paper also
          shows that, under certain data models and MAR mechanism,
          the MLE is asymptotically normally distributed and the
          asymptotic covariance matrix is consistently estimated by
          the commonly used sandwich-type covariance matrix. The
          results indicate that certain formulas and/or conclusions
          in the existing literature may not be entirely correct.}
}

@Article{yuan:bent:1998,
  author    = {K.H. Yuan and P.M. Bentler},
  year      = {1998},
  title     = {Normal theory based test statistics in structural equation
          modeling},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {51},
  pages     = {289--309}
}

@article{yuan:bentler:1998:robust,
    author = {Yuan, Ke-Hai and Bentler, Peter M.},
    doi = {10.1111/0081-1750.00052},
    journal = {Sociological Methodology},
    number = {1},
    pages = {363--396},
    title = {Structural Equation Modeling With Robust Covariances},
    volume = {28},
    year = {1998},
    abstract =
       {Existing methods for structural equation modeling involve
       fitting the ordinary sample covariance matrix by a proposed
       structural model. Since a sample covariance is easily
       influenced by a few outlying cases, the standard practice of
       modeling sample covariances can lead to inefficient estimates
       as well as inflated fit indices. By giving a proper weight to
       each individual case, a robust covariance will have a bounded
       influence function as well as a nonzero breakdown point.
       These robust properties of the covariance estimators will be
       carried over to the parameter estimators in the structural
       model if a technically appropriate procedure is used. We
       study such a procedure in which robust covariances replace
       ordinary sample covariances in the context of the Wishart
       likelihood function. This procedure is easy to implement in
       practice. Statistical properties of this procedure are
       investigated. A fit index is given based on sampling from an
       elliptical distribution. An estimating equation approach is
       used to develop a variety of robust covariances, and
       consistent covariances of these robust estimators, needed for
       standard errors and test statistics, follow from this
       approach. Examples illustrate the inflated statistics and
       distorted parameter estimates obtained by using sample
       covariances when compared with those obtained by using robust
       covariances. The merits of each method and its relevance to
       specific types of data are discussed.},
}


@Article{yuan:bentler:1997,
  author    = {Yuan, Ke-Hai and Bentler, Peter M. },
  year      = {1997},
  title     = {Mean and Covariance Structure Analysis: Theoretical and
          Practical Improvements},
  abstract  = {The most widely used multivariate statistical models in
          the social and behavioral sciences involve linear
          structural relations among observed and latent variables.
          In practice, these variables are generally nonnormally
          distributed, and hence classical multivariate analysis,
          based on multinormal error-free variables having no
          simultaneous interrelations, is not adequate to deal with
          such data. Since structural relations among variables imply
          a structure for the multivariate product moments of...},
  journal   = {Journal of the American Statistical Association},
  number    = {438},
  pages     = {767--774},
  volume    = {92}
}

@Article{yuan:bentler:1998,
  author    = {Yuan, Ke-Hai and Peter M. Bentler},
  year      = {1998},
  title     = {Normal theory based test statistics in structural equation
          modeling},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {51},
  pages     = {289--309}
}

@Article{yuan:bentler:1999,
  author    = {Yuan, Ke-Hai and Bentler, Peter M.},
  year      = {1999},
  title     = {$F$-Tests for Mean and Covariance Structure Analysis},
  doi       = {10.2307/1165323},
  journal   = {Journal of Educational and Behavioral Statistics},
  number    = {3},
  pages     = {225--243},
  volume    = {24},
  abstract  = {Covariance structure analysis is used for inference and
          for dimension reduction with multivariate data. When data
          are not normally distributed, the asymptotic distribution
          free (ADF) method is often used to fit a proposed model.
          The ADF test statistic is asymptotically distributed as a
          chi-square variate. Experience with real data indicates
          that the ADF statistic tends to reject theoretically
          meaningful models. Empirical simulation shows that the ADF
          statistic rejects correct models too often for all but
          impractically large sample sizes. By comparing mean and
          covariance structure analysis with its analogue in the
          multivariate linear model, we propose some modified ADF
          test statistics whose distributions are approximated by F
          distributions. Empirical studies show that the
          distributions of the new statistics are more closely
          approximated by F distributions than are the original ADF
          statistics when referred to chi-square distributions.
          Detailed analysis indicates why the ADF statistic fails on
          large models and why F tests and corrections give better
          results. Implications for power analysis and model tests in
          other areas are discussed.}
}

@Article{yuan:bentler:2000,
  author    = {Yuan, Ke H. and Bentler, Peter M. },
  year      = {2000},
  title     = {Three Likelihood-Based Methods for Mean and Covariance
          Structure Analysis with Nonnormal Missing Data},
  doi       = {10.2307/271133},
  journal   = {Sociological Methodology},
  pages     = {165--200},
  publisher = {American Sociological Association},
  volume    = {30},
  abstract  = {Survey and longitudinal studies in the social and
          behavioral sciences generally contain missing data. Mean
          and covariance structure models play an important role in
          analyzing such data. Two promising methods for dealing with
          missing data are a direct maximum-likelihood and a
          two-stage approach based on the unstructured mean and
          covariance estimates obtained by the EM-algorithm. Typical
          assumptions under these two methods are ignorable
          nonresponse and normality of data. However, data sets in
          social and behavioral sciences are seldom normal, and
          experience with these procedures indicates that normal
          theory based methods for nonnormal data very often lead to
          incorrect model evaluations. By dropping the normal
          distribution assumption, we develop more accurate
          procedures for model inference. Based on the theory of
          generalized estimating equations, a way to obtain
          consistent standard errors of the two-stage estimates is
          given. The asymptotic efficiencies of different estimators
          are compared under various assumptions. We also propose a
          minimum chi-square approach and show that the estimator
          obtained by this approach is asymptotically at least as
          efficient as the two likelihood-based estimators for either
          normal or nonnormal data. The major contribution of this
          paper is that for each estimator, we give a test statistic
          whose asymptotic distribution is chi-square as long as the
          underlying sampling distribution enjoys finite fourth-order
          moments. We also give a characterization for each of the
          two likelihood ratio test statistics when the underlying
          distribution is nonnormal. Modifications to the likelihood
          ratio statistics are also given. Our working assumption is
          that the missing data mechanism is missing completely at
          random. Examples and Monte Carlo studies indicate that, for
          commonly encountered nonnormal distributions, the
          procedures developed in this paper are quite reliable even
          for samples with missing data that are missing at random.}
}

@Article{yuan:bentler:2001,
  author    = {Yuan, K. H. and Bentler, P. M.},
  year      = {2001},
  title     = {Effect of outliers on estimators and tests in covariance
          structure analysis},
  abstract  = {A small proportion of outliers can distort the results
          based on classical procedures in covariance structure
          analysis. We look at the quantitative effect of outliers on
          estimators and test statistics based on normal theory
          maximum likelihood and the asymptotically distribution-free
          procedures. Even if a proposed structure is correct for the
          majority of the data in a sample, a small proportion of
          outliers leads to biased estimators and significant test
          statistics. An especially unfortunate consequence is that
          the power to reject a model can be made arbitrarily - but
          misleadingly - large by inclusion of outliers in an
          analysis.},
  issn      = {0007-1102},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  pages     = {161--175},
  publisher = {British Psychological Society}
}

@Article{yuan:bentler:2003,
  author    = {Yuan, Ke-Hai and Bentler, Peter M. },
  year      = {2003},
  title     = {Eight test statistics for multilevel structural equation
          models},
  booktitle = {Special Issue in Honour of Stan Azen: a Birthday
          Celebration},
  journal   = {Computational Statistics \& Data Analysis},
  number    = {1-2},
  pages     = {89--107},
  volume    = {44},
  abstract  = {Data in social and behavioral sciences are often
          hierarchically organized though seldom normal. They
          typically contain heterogeneous marginal skewnesses and
          kurtoses. With such data, the normal theory based
          likelihood ratio statistic is not reliable when evaluating
          a multilevel structural equation model. Statistics that are
          not sensitive to sampling distributions are desirable. Six
          statistics for evaluating a structural equation model are
          extended from the conventional context to the multilevel
          context. These statistics are asymptotically distribution
          free, that is, their distributions do not depend on the
          sampling distribution when sample size at the highest level
          is large enough. The performance of these statistics in
          practical data analysis is evaluated with a Monte Carlo
          study simulating conditions encountered with real data.
          Results indicate that each of the statistics is very
          insensitive to the underlying sampling distributions even
          with finite sample sizes. However, the six statistics
          perform quite differently at smaller sample sizes; some
          over-reject the correct model and some under-reject the
          correct model. Comparing the six statistics with two
          existing ones in the multilevel context, two of the six new
          statistics are recommended for model evaluation in
          practice.}
}

@Article{yuan:bentler:2004,
  author    = {Yuan, Ke-Hai and Bentler, Peter M.},
  year      = {2004},
  title     = {On Chi-Square Difference and $z$ Tests in Mean and
          Covariance Structure Analysis when the Base Model is
          Misspecified},
  abstract  = {In mean and covariance structure analysis, the chi-square
          difference test is often applied to evaluate the number of
          factors, cross-group constraints, and other nested model
          comparisons. Let model Ma be the base model within which
          model Mb is nested. In practice, this test is commonly used
          to justify Mb even when Ma is misspecified. The authors
          study the behavior of the chi-square difference test in
          such a circumstance. Monte Carlo results indicate that a
          nonsignificant chi-square difference cannot be used to
          justify the constraints in Mb. They also show that when the
          base model is misspecified, the z test for the statistical
          significance of a parameter estimate can also be
          misleading. For specific models, the analysis further shows
          that the intercept and slope parameters in growth curve
          models can be estimated consistently even when the
          covariance structure is misspecified, but only in linear
          growth models. Similarly, with misspecified covariance
          structures, the mean parameters in multiple group models
          can be estimated consistently under null conditions.
          10.1177/0013164404264853},
  doi       = {10.1177/0013164404264853},
  journal   = {Educational and Psychological Measurement},
  number    = {5},
  pages     = {737--757},
  volume    = {64}
}

@InCollection{yuan:bentler:2007,
  author    = {Ke-Hai Yuan and Peter M Bentler},
  editor    = {C. Rao and S. Sinharay},
  year      = {2007},
  title     = {Structural Equation Modeling},
  booktitle = {Handbook of Statistics: Psychometrics},
  publisher = {Elsevier},
  volume    = {26},
  isbn      = {0-444-52103-8},
  series    = {Handbook of Statistics},
  chapter   = {10}
}

@Article{yuan:bentler:chan:2004,
  author    = {Yuan, Ke-Hai and Bentler, Peter and Chan, Wai},
  year      = {2004},
  title     = {Structural equation modeling with heavy tailed
          distributions},
  doi       = {10.1007/BF02295644},
  journal   = {Psychometrika},
  number    = {3},
  pages     = {421--436},
  volume    = {69},
  abstract  = {Data in social and behavioral sciences typically possess
          heavy tails. Structural equation modeling is commonly used
          in analyzing interrelations among variables of such data.
          Classical methods for structural equation modeling fit a
          proposed model to the sample covariance matrix, which can
          lead to very inefficient parameter estimates. By fitting a
          structural model to a robust covariance matrix for data
          with heavy tails, one generally gets more efficient
          parameter estimates. Because many robust procedures are
          available, we propose using the empirical efficiency of a
          set of invariant parameter estimates in identifying an
          optimal robust procedure. Within the class of elliptical
          distributions, analytical results show that the robust
          procedure leading to the most efficient parameter estimates
          also yields a most powerful test statistic. Examples
          illustrate the merit of the proposed procedure. The
          relevance of this procedure to data analysis in a broader
          context is noted.}
}

@Article{yuan:chan:2005,
  author    = {Yuan, Ke-Hai and Chan, Wai},
  year      = {2005},
  title     = {On Nonequivalence of Several Procedures of Structural
          Equation Modeling},
  doi       = {10.1007/s11336-001-0930-9},
  journal   = {Psychometrika},
  number    = {4},
  pages     = {791--798},
  volume    = {70},
  abstract  = {The normal theory based maximum likelihood procedure is
          widely used in structural equation modeling. Three
          alternatives are: the normal theory based generalized least
          squares, the normal theory based iteratively reweighted
          least squares, and the asymptotically distribution-free
          procedure. When data are normally distributed and the model
          structure is correctly specified, the four procedures are
          asymptotically equivalent. However, this equivalence is
          often used when models are not correctly specified. This
          short paper clarifies conditions under which these
          procedures are not asymptotically equivalent. Analytical
          results indicate that, when a model is not correct, two
          factors contribute to the nonequivalence of the different
          procedures. One is that the estimated covariance matrices
          by different procedures are different, the other is that
          they use different scales to measure the distance between
          the sample covariance matrix and the estimated covariance
          matrix. The results are illustrated using real as well as
          simulated data. Implication of the results to model fit
          indices is also discussed using the comparative fit index
          as an example.}
}

@Article{yuan:hayashi:2006,
  author    = {Ke-Hai Yuan and K. Hayashi},
  year      = {2006},
  title     = {Standard Errors in Covariance Structure Models:
          Asymptotics versus Bootstrap},
  journal   = {British Journal of Mathematical and Statistical
          Psychology},
  volume    = {59},
  pages     = {397--417},
  source    = {Ken Bollen}
}

@Article{yuan:jennrich:1998,
  author    = {Yuan, Ke-Hai and Jennrich, Robert I.},
  year      = 1998,
  title     = {Asymptotics of Estimating Equations under Natural
          Conditions},
  journal   = {Journal of Multivariate Analysis},
  volume    = {65},
  number    = {2},
  pages     = {245--260}
}

@Unpublished{yuan:kolenikov:2008,
  author    = {Stanislav Kolenikov and Yiyong Yuan},
  year      = {2009},
  title     = {Empirical likelihood estimation and testing in structural
          equation models},
  note      = {R\&R in \textit{British Journal of Mathematical and Statistical Psychology}}
}

@Article{yuan:marshall:2004,
  author    = {Yuan, Ke-Hai and Marshall, Linda L.},
  year      = {2004},
  title     = {A New Measure of Misfit for Covariance Structure Models},
  abstract  = {Various fit indices exist in structural equation models.
          Most of these indices are related to the noncentrality
          parameter (NCP) of the chi-square distribution that the
          involved test statistic is implicitly assumed to follow.
          Existing literature suggests that few statistics can be
          well approximated by chi-square distributions. The meaning
          of the NCP is not clear when the behavior of the statistic
          cannot be described by a chi-square distribution. In this
          paper we define a new measure of model misfit (MMM) as the
          difference between the expected values of a statistic under
          the alternative and null hypotheses. This definition does
          not need to assume that the population covariance matrix is
          in the vicinity of the proposed model, nor does it need for
          the test statistic to follow any distribution of a known
          form. The MMM does not necessarily equal the discrepancy
          between the model and the population covariance matrix as
          has been assumed in existing literature. Bootstrap
          approaches to estimating the MMM and a related quantity are
          developed. An algorithm for obtaining bootstrap confidence
          intervals of the MMM is constructed. Examples with
          practical data sets contrast several measures of model
          misfit. The quantile-quantile plot is used to illustrate
          the unrealistic nature of chi-square distribution
          assumptions under either the null or an alternative
          hypothesis in practice.},
  doi       = {doi:10.2333/bhmk.31.67},
  journal   = {Behaviormetrika},
  number    = {1},
  pages     = {67--90},
  volume    = {31}
}

@Article{yuan:marshall:bentler:2003,
  author    = {Yuan, Ke-Hai and Marshall, Linda L. and Bentler, Peter M.
          },
  year      = {2003},
  title     = {Assessing the Effect of Model Misspecifications on
          Parameter Estimates in Structural Equation Models},
  comment   = {10.1111/j.0081-1750.2003.00132.x},
  journal   = {Sociological Methodology},
  number    = {1},
  pages     = {241--265},
  volume    = {33},
  abstract  = {Model misspecifications may have a systematic effect on
          parameters, causing biases in their estimates. In the
          application of structural equation models, every
          interesting model is fallible. When simultaneously
          evaluating a model, it is of interest to study whether all
          parameters are affected by a misspecification. This paper
          provides three procedures for evaluating such an effect:
          (1) analyzing the path, (2) using a functional
          relationship, and (3) using a significance test. Analyzing
          the path is illustrated through a confirmatory factor
          model. This method is ad hoc but intuitive. A more rigorous
          approach is built upon the concept of orthogonality of two
          sets of parameters. When parameter a is orthogonal to
          parameter b, omitting parameter b will not affect the
          estimation of parameter a. The functional relationship of
          two sets of parameters is used to check their
          orthogonality. The distribution of the difference between
          estimates based on different models is obtained, which
          provides a Hausman-like way to check significant parameter
          differences that are due to biases. Examples illustrate
          that these procedures can provide valuable information on
          identifying parameter estimates that are systematically
          affected by a model misspecification.}
}

@article{yuan:zhong:2008,
    address = {University of Notre Dame},
    author = {Yuan, Ke-Hai and Zhong, Xiaoling},
    doi = {10.1111/j.1467-9531.2008.00198.x},
    journal = {Sociological Methodology},
    number = {1},
    pages = {329--368},
    title = {Outliers, Leverage Observations, and Influential Cases
             in Factor Analysis: Using Robust Procedures to Minimize Their Effect},
    volume = {38},
    year = {2008},
    abstract =
       {Parallel to the development in regression diagnosis, this
       paper defines good and bad leverage observations in factor
       analysis. Outliers are observations that deviate from the
       factor model, not from the center of the data cloud. The
       effects of each kind of outlying observations on the normal
       distribution-based maximum likelihood estimator and the
       associated likelihood ratio statistic are studied through
       analysis. The distinction between outliers and leverage
       observations also clarifies the roles of three robust
       procedures based on different Mahalanobis distances. All the
       robust procedures are designed to minimize the effect of
       certain outlying observations. Only the robust procedure with
       a residual-based distance properly controls the effect of
       outliers. Empirical results illustrate the strength or
       weakness of each procedure and support those obtained in
       analysis. The relevance of the results to general structural
       equation models is discussed and formulas are provided.},
}

@ARTICLE{yung:rao:1996,
  AUTHOR =       {Wesley Yung and Jon N K Rao},
  TITLE =        {Jackknife linearization variance estimators under stratified multistage sampling},
  JOURNAL =      {Survey Methodology},
  YEAR =         {1996},
  volume =       {22},
  number =       {1},
  pages =        {23--31},
}


@InProceedings{yung:1997,
  author    = {Wesley Yung},
  year      = {1997},
  title     = {Variance Estimation for Public Use Files under
          Confidentiality Constraints},
  booktitle = {Proceedings of Statistics Canada Symposium},
  pages     = {434--439},
  organization  = {Statistics Canada}
}

@Article{yung:bentler:1994,
  author    = {Yung, Y. F. and Bentler, P. M.},
  year      = {1994},
  title     = {Bootstrap-corrected {ADF} test statistics in covariance
          structure analysis.},
  journal   = {The British journal of mathematical and statistical
          psychology},
  pages     = {63--84},
  volume    = {47},
  number    = {1},
  abstract  = {The asymptotically distribution-free (ADF) test statistic
          for covariance structure analysis (CSA) has been reported
          to perform very poorly in simulation studies, i.e. it leads
          to inaccurate decisions regarding the adequacy of models of
          psychological processes. It is shown in the present study
          that the poor performance of the ADF test statistic is due
          to inadequate estimation of the weight matrix (W = gamma
          -1), which is a critical quantity in the ADF theory.
          Bootstrap procedures based on Hall's bias reduction
          perspective are proposed to correct the ADF test statistic.
          It is shown that the bootstrap correction of additive bias
          on the ADF test statistic yields the desired tail behaviour
          as the sample size reaches 500 for a 15-variable-3-factor
          confirmatory factor-analytic model, even if the
          distribution of the observed variables is not multivariate
          normal and the latent factors are dependent. These results
          help to revive the ADF theory in CSA.}
}

@Article{zabel:1998,
  author    = {Zabel, Jeffrey E.},
  year      = {1998},
  title     = {An Analysis of Attrition in the Panel Study of Income
          Dynamics and the Survey of Income and Program Participation
          with an Application to a Model of Labor Market Behavior},
  abstract  = {This paper analyzes attrition behavior in two major
          longitudinal surveys, the Panel Study of Income Dynamics
          (PSID) and the Survey of Income and Program Participation
          (SIPP). Significant indicators in a model of attrition
          include measures of mobility and variables that correspond
          to the interviewer and the interview process. There is
          evidence that surveys with more waves or higher frequency
          interviews experience higher attrition rates. The
          estimation results for a model of attrition and labor
          market behavior show little indication of bias due to
          attrition but there is evidence that the labor market
          behavior of attritors and nonattritors is different.},
  doi       = {10.2307/146438},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {479--506},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@ARTICLE{zasl:zheng:adams:2008,
  AUTHOR =       {Alan M. Zaslavsky and Hui Zheng and John Adams},
  TITLE =        {Optimal sample allocation for design-consistent regression
                  in a cancer services survey when design variables are
                  known for aggregates},
  JOURNAL =      {Survey Methodology},
  YEAR =         {2008},
  volume =       {34},
  number =       {1},
  pages =        {65--78},
}


@article{zhong:yuan:2011,
    author = {Zhong, Xiaoling and Yuan, Ke-Hai},
    doi = {10.1080/00273171.2011.558736},
    journal = {Multivariate Behavioral Research},
    number = {2},
    pages = {229--265},
    title = {Bias and Efficiency in Structural Equation Modeling: Maximum Likelihood Versus Robust Methods},
    volume = {46},
    year = {2011},
    abstract =
       {In the structural equation modeling literature, the
       normal-distribution-based maximum likelihood ({ML}) method is
       most widely used, partly because the resulting estimator is
       claimed to be asymptotically unbiased and most efficient.
       However, this may not hold when data deviate from normal
       distribution. Outlying cases or nonnormally distributed data,
       in practice, can make the {ML} estimator ({MLE}) biased and
       inefficient. In addition to {ML}, robust methods have also
       been developed, which are designed to minimize the effects of
       outlying cases. But the properties of robust estimates and
       their standard errors ({SEs}) have never been systematically
       studied. This article studies two robust methods and compares
       them against the {ML} method with respect to bias and
       efficiency using a confirmatory factor model. Simulation
       results show that robust methods lead to results comparable
       with {ML} when data are normally distributed. When data have
       heavy tails or outlying cases, robust methods lead to less
       biased and more efficient estimators than {MLEs}. A formula
       to obtain consistent {SEs} for one of the robust methods is
       also developed. The formula-based {SEs} for both robust
       estimators match the empirical {SEs} very well with
       medium-size samples. A sample of the Cross Racial Identity
       Scale with a 6-factor model is used for illustration. Results
       also confirm conclusions of the simulation study. In the
       structural equation modeling literature, the
       normal-distribution-based maximum likelihood ({ML}) method is
       most widely used, partly because the resulting estimator is
       claimed to be asymptotically unbiased and most efficient.
       However, this may not hold when data deviate from normal
       distribution. Outlying cases or nonnormally distributed data,
       in practice, can make the {ML} estimator ({MLE}) biased and
       inefficient. In addition to {ML}, robust methods have also
       been developed, which are designed to minimize the effects of
       outlying cases. But the properties of robust estimates and
       their standard errors ({SEs}) have never been systematically
       studied. This article studies two robust methods and compares
       them against the {ML} method with respect to bias and
       efficiency using a confirmatory factor model. Simulation
       results show that robust methods lead to results comparable
       with {ML} when data are normally distributed. When data have
       heavy tails or outlying cases, robust methods lead to less
       biased and more efficient estimators than {MLEs}. A formula
       to obtain consistent {SEs} for one of the robust methods is
       also developed. The formula-based {SEs} for both robust
       estimators match the empirical {SEs} very well with
       medium-size samples. A sample of the Cross Racial Identity
       Scale with a 6-factor model is used for illustration. Results
       also confirm conclusions of the simulation study.},
}

@Article{zhu:lee:2001,
  author    = {Zhu, Hong-Tu and Lee, Sik-Yum},
  year      = 2001,
  title     = {Local Influence for Incomplete-data Models},
  journal   = {Journal of the Royal Statistical Society, Series B,
          Methodological},
  volume    = 63,
  number    = 1,
  pages     = {111--126}
}

@Article{zientek:thompson:2007,
  author    = {Zientek, Linda and Thompson, Bruce},
  year      = {2007},
  title     = {Applying the bootstrap to the multivariate case: Bootstrap
          component/factor analysis},
  abstract  = {The bootstrap method, which empirically estimates the
          sampling distribution for either inferential or descriptive
          statistical purposes, can be applied to the multivariate
          case. When conducting bootstrap component, or factor,
          analysis, resampling results must be located in a common
          factor space before summary statistics for each estimated
          parameter can be computed. The present article describes a
          strategy for applying the bootstrap method to conduct
          either a bootstrap component or a factor analysis with a
          program syntax for SPSS. The Holzinger-Swineford data set
          is employed to make the discussion more concrete.},
  address   = {Department of Mathematics, Sam Houston State University,
          Huntsville, Texas 77341, USA. lrzientek@yahoo.com},
  issn      = {1554-351X},
  journal   = {Behavior Research Methods},
  number    = {2},
  pages     = {318--325},
  volume    = {39}
}

@Article{ziliak:kniesner:1998,
  author    = {Ziliak, James P. and Kniesner, Thomas J.},
  year      = {1998},
  title     = {The Importance of Sample Attrition in Life Cycle Labor
          Supply Estimation},
  abstract  = {We examine the importance of possible nonrandom attrition
          to an econometric model of life cycle labor supply using
          both a Wald test comparing attriters to nonattriters and
          variable addition tests based on formal models of
          attrition. Estimates using the Panel Study of Income
          Dynamics show that nonrandom attrition is of little concern
          when estimating prime-age male labor supply because the
          effect of attrition is absorbed into fixed effects in labor
          supply. The wage measure and instrument set have much
          larger effects on the estimated labor supply function of
          prime-age men than how one adjusts for panel attrition.},
  doi       = {10.2307/146439},
  issn      = {0022166X},
  journal   = {The Journal of Human Resources},
  number    = {2},
  pages     = {507--530},
  publisher = {University of Wisconsin Press},
  volume    = {33}
}

@Article{zimm:1989:reml,
  author    = "Zimmerman, D.L.",
  year      = 1989,
  title     = "Computationally Efficient Restricted Maximum Likelihood
          Estimation of Generalized Covariance Functions",
  journal   = "Mathematical Geology",
  volume    = 21,
  number    = "655--672"
}

@InBook{zivot:2006,
  author    = {Zivot, Eric and Startz, Richard and Nelson, Charles R.},
  editor    = {Corbade, Dean and Durlauf, Steven N. and Hancen, Bruce
          E.},
  year      = {2006},
  title     = {Improved inference in weakly identified instrumental
          variables regression},
  abstract  = {The inference problem on individual coefficients in the IV
          regression with multiple endogenous right-hand-side
          variables and weak instruments is analyzed. The previous
          work of Choi and Phillips (1992) is extended to allow for
          weak instruments. The existing techniques for performing
          inference on individual coefficients using Staiger and
          Stock's (1997) weak instruments asymptotics is evaluated.
          The finite sample properties are analyzed using Monte Carlo
          simulations. The only asymptotically valid tests for
          individual coefficients are shown to be based on
          projections of asymptotically valid tests for all
          coefficients. Kleibergen's (2002) concentrated K statistic
          was found to have quite good size and power behavior
          relative to other methods},
  address   = {New York},
  booktitle = {Econometric Theory and Practice},
  chapter   = {5},
  publisher = {Cambridge University Press}
}

@TechReport{zoho:blan:popk:2004,
  author    = "N. Zohoori and D. Blanchette and B.M. Popkin",
  year      = "2004",
  title     = "Monitoring Health Conditions in the {R}ussian
          {F}ederation: The {R}ussia {L}ongitudinal {M}onitoring
          {S}urvey 1992-2003",
  type      = "Report submitted to the {U.S. A}gency for {I}nternational
          {D}evelopment",
  institution   = "{C}arolina {P}opulation {C}enter, {U}niversity of {N}orth
          {C}arolina at {C}hapel {H}ill"
}


@article{verbeke:molenberghs:2007,
    abstract = {{The score test presents a number of issues. There are four dimensions of concern: (1) Is the parameter space constrained? (2) Is observed information used, or some form of expected information? (3) Are size and power computed under the null hypothesis, a correctly specified alternative, or a misspecified alternative? (4) Are computations asymptotic or small sample?}},
    author = {Verbeke, Geert and Molenberghs, Geert},
    doi = {10.1198/000313007x243089},
    issn = {0003-1305},
    journal = {The American Statistician},
    keywords = {information\_matrix},
    month = nov,
    number = {4},
    pages = {289--290},
    posted-at = {2015-04-29 20:34:13},
    priority = {2},
    publisher = {Taylor \& Francis},
    title = {{What Can Go Wrong With the Score Test?}},
    url = {http://dx.doi.org/10.1198/000313007x243089},
    volume = {61},
    year = {2007}
}

